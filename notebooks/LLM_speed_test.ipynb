{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This test is to measure the speed of the model on a large dataset.\n",
    "from src.networks.network import Network\n",
    "from src.games.lean_game import LeanGameState\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import Optional, Tuple\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "import modal\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ProverLLM(Network):\n",
    "    def __init__(self,\n",
    "                 base_model: Optional[AutoModelForCausalLM] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        A ProverLLM model that uses a transformer-based language model for proof generation\n",
    "        and MCTS decision-making. It implements:\n",
    "        1. Given a game state, generate a prompt (either for the V/P-heads or for completion).\n",
    "        2. Given this prompt, tokenize it.\n",
    "        3. Given a tokenized prompt, run it through the base model to get a completion.\n",
    "        4. Given a tokenized prompt, run it through the base model to get an intermediate state.\n",
    "        5. Use the intermediate state to get a value estimate and a policy output.\n",
    "\n",
    "        The worker/MCTS immediately outside of ProverLLM should be agnostic to the LLM-related\n",
    "        principles including the specific prompting methods etc. To this end,\n",
    "        all worker/MCTS interations with this class (\"public methods\") should\n",
    "        involve LeanGameStates only.\n",
    "\n",
    "        The controller and training loop necessarily need to know about the internal\n",
    "        details of the model; the five steps above can be short-circuited in different\n",
    "        ways by the controller.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        base_model: Optional[AutoModelForCausalLM]\n",
    "            An optional pre-trained base model. If None, a default model is loaded.\n",
    "        \"\"\"\n",
    "        super(ProverLLM, self).__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            'deepseek-ai/DeepSeek-Prover-V1.5-RL',\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        if base_model is None:\n",
    "            self.base_model: AutoModelForCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "                'deepseek-ai/DeepSeek-Prover-V1.5-RL',\n",
    "                trust_remote_code=True,\n",
    "                device_map='auto'\n",
    "            )\n",
    "        else:\n",
    "            self.base_model = base_model\n",
    "\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Linear(4096, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        self.policy_head = nn.Sequential(\n",
    "            nn.Linear(4096, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 100)\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "########################## Utilities ##########################\n",
    "\n",
    "    def policy_value_state_dict(self) -> dict:\n",
    "        \"\"\"\n",
    "        Usually, we do not want to be saving/loading the entire model,\n",
    "        just the policy and value heads.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        policy_value_state_dict: dict\n",
    "            The state dictionaries for the policy and value heads.\n",
    "        \"\"\"\n",
    "        if self.llm_only:\n",
    "            raise ValueError(\"Cannot get state dicts in llm_only mode.\")\n",
    "        return {\n",
    "            'policy_head': self.policy_head.state_dict(),\n",
    "            'value_head': self.value_head.state_dict()\n",
    "        }\n",
    "\n",
    "    def load_policy_value_state_dict(self, policy_value_state_dict: dict):\n",
    "        \"\"\"\n",
    "        Set the state dictionaries for the policy and value heads.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        policy_value_state_dict: dict\n",
    "            The state dictionaries for the policy and value heads.\n",
    "        \"\"\"\n",
    "        if self.llm_only:\n",
    "            raise ValueError(\"Cannot set state dicts in llm_only mode.\")\n",
    "        self.policy_head.load_state_dict(\n",
    "            policy_value_state_dict['policy_head'])\n",
    "        self.value_head.load_state_dict(policy_value_state_dict['value_head'])\n",
    "\n",
    "########################## Public Interface ##########################\n",
    "\n",
    "    def mcts_forward(self, prompt: str) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Perform a forward pass through the model using MCTS.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        state: LeanGameState\n",
    "            The current game state.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        policy_output: torch.Tensor\n",
    "            The policy output from the policy head.\n",
    "        value_output: torch.Tensor\n",
    "            The value estimate from the value head.\n",
    "        \"\"\"\n",
    "\n",
    "        input_ids, attention_mask = self.tokenize(prompt)\n",
    "        intermediate_output = self.get_intermediate_state(\n",
    "            input_ids, attention_mask)\n",
    "        policy_output, value_output = self.policy_and_value(\n",
    "            intermediate_output)\n",
    "        return policy_output, value_output\n",
    "\n",
    "    def forward(self, state: LeanGameState) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Runs the forward pass of the network. Returns a policy and a value.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        state: LeanGameState\n",
    "            The current game state.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        policy_output: torch.Tensor\n",
    "            The policy output from the policy head.\n",
    "        value_output: torch.Tensor\n",
    "            The value estimate from the value head.\n",
    "        \"\"\"\n",
    "        return self.mcts_forward(state)\n",
    "\n",
    "\n",
    "########################## Part 2: Given this prompt, tokenize it ##########################\n",
    "\n",
    "    def tokenize(self, prompt: str) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Tokenizes the given prompt and returns the input IDs and attention mask.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        prompt: str\n",
    "            The prompt to tokenize.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        Tuple[torch.Tensor, torch.Tensor]\n",
    "            The input IDs and attention mask.\n",
    "        \"\"\"\n",
    "        tokens = self.tokenizer(\n",
    "            prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
    "        return tokens['input_ids'], tokens['attention_mask']\n",
    "\n",
    "########################## Part 3: Given a tokenized prompt, run it through the base model to get a completion ##########################\n",
    "\n",
    "    def complete(self, input_ids: torch.Tensor, attention_mask=None, max_length=1000) -> str:\n",
    "        \"\"\"\n",
    "        Returns most likely completed proof (max'ed with 1000 tokens)\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        input_ids: torch.Tensor\n",
    "            The input token IDs.\n",
    "        attention_mask: torch.Tensor\n",
    "            The attention mask for the input.\n",
    "        max_length: int\n",
    "            The maximum length of the generated text.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        generated_text: str\n",
    "            The generated proof text.\n",
    "        \"\"\"\n",
    "\n",
    "        base_output = self.base_model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=input_ids.shape[1] + max_length\n",
    "        )\n",
    "        generated_text = self.tokenizer.decode(\n",
    "            base_output[0], skip_special_tokens=True)\n",
    "        return generated_text\n",
    "\n",
    "########################## Part 4: Given a tokenized prompt, run it through the base model to get an intermediate state ##########################\n",
    "\n",
    "    def get_intermediate_state(self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns the intermediate hidden state output from the base model.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        input_ids: torch.Tensor\n",
    "            The input token IDs.\n",
    "        attention_mask: torch.Tensor\n",
    "            The attention mask for the input.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        intermediate_output: torch.Tensor\n",
    "            The hidden state output from the base model.\n",
    "        \"\"\"\n",
    "\n",
    "        base_output = self.base_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "        hidden_states = base_output.hidden_states\n",
    "\n",
    "        intermediate_output = hidden_states[25][0][-1]\n",
    "        # debug\n",
    "        print(\"Intermediate output shape:\", intermediate_output.shape)\n",
    "        return intermediate_output\n",
    "\n",
    "########################## Part 5: Use the intermediate state to get a value estimate and a policy output ##########################\n",
    "\n",
    "    def policy_and_value(self, intermediate_output: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Returns policy and value outputs for the given intermediate output.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        intermediate_output: torch.Tensor\n",
    "            The intermediate output from the base model.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        policy_output: torch.Tensor\n",
    "            The policy output from the policy head.\n",
    "        value_output: torch.Tensor\n",
    "            The value estimate from the value head.\n",
    "        \"\"\"\n",
    "\n",
    "        policy_output = self.policy_head(intermediate_output)\n",
    "        value_output = self.value_head(intermediate_output)\n",
    "\n",
    "        return policy_output, value_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ProverLLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = []\n",
    "with open(os.path.join('datasets', 'minif2f.jsonl'), 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "# convert each data point to a string for inference.\n",
    "# Example:\n",
    "# {\n",
    "#     \"name\": \"amc12a_2019_p21\",\n",
    "#     \"split\": \"valid\",\n",
    "#     \"informal_prefix\": \"/-- Let $z=\\\\frac{1+i}{\\\\sqrt{2}}.$What is $\\\\left(z^{1^2}+z^{2^2}+z^{3^2}+\\\\dots+z^{{12}^2}\\\\right) \\\\cdot \\\\left(\\\\frac{1}{z^{1^2}}+\\\\frac{1}{z^{2^2}}+\\\\frac{1}{z^{3^2}}+\\\\dots+\\\\frac{1}{z^{{12}^2}}\\\\right)?$\\n\\n$\\\\textbf{(A) } 18 \\\\qquad \\\\textbf{(B) } 72-36\\\\sqrt2 \\\\qquad \\\\textbf{(C) } 36 \\\\qquad \\\\textbf{(D) } 72 \\\\qquad \\\\textbf{(E) } 72+36\\\\sqrt2$ Show that it is \\\\textbf{(C) }36.-/\\n\",\n",
    "#     \"formal_statement\": \"theorem amc12a_2019_p21 (z : ℂ) (h₀ : z = (1 + Complex.I) / Real.sqrt 2) :\\n  ((∑ k : ℤ in Finset.Icc 1 12, z ^ k ^ 2) * (∑ k : ℤ in Finset.Icc 1 12, 1 / z ^ k ^ 2)) = 36 := by\\n\",\n",
    "#     \"goal\": \"z : ℂ\\nh₀ : z = (1 + Complex.I) / ↑√2\\n⊢ (∑ k ∈ Finset.Icc 1 12, z ^ k ^ 2) * ∑ k ∈ Finset.Icc 1 12, 1 / z ^ k ^ 2 = 36\",\n",
    "#     \"header\": \"import Mathlib\\nimport Aesop\\n\\nset_option maxHeartbeats 0\\n\\nopen BigOperators Real Nat Topology Rat\\n\\n\"\n",
    "# }\n",
    "\n",
    "# We should concatenate the informal prefix, header, formal statement, and goal together.\n",
    "\n",
    "input_data = []\n",
    "for d in data:\n",
    "    input_data.append(d['goal'] + \"\\n\" + d['informal_prefix'] + '```lean4\\n' +\n",
    "                      d['header'] + d['formal_statement'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_size in [1, 10, 12, 14, 16, 18, 20]:\n",
    "    print(f\"Testing with {test_size} data points.\")\n",
    "    input_subset = []\n",
    "    while len(input_subset) < test_size:\n",
    "        input_subset.extend(input_data)\n",
    "\n",
    "    input_subset = input_subset[:test_size]\n",
    "\n",
    "    # Inference on the input data, timing it.\n",
    "    start = time.time()\n",
    "    policy, value = model.mcts_forward(input_subset)\n",
    "    end = time.time()\n",
    "    print(f\"Inference took {end-start} seconds.\")\n",
    "    print(\"output shape: \", policy.shape, value.shape)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
