{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict, Optional\n",
    "from collections import defaultdict\n",
    "\n",
    "from vllm import LLM, SamplingParams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 09-30 01:51:49 config.py:1651] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 09-30 01:51:49 config.py:890] Defaulting to use mp for distributed inference\n",
      "INFO 09-30 01:51:49 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='deepseek-ai/DeepSeek-Prover-V1.5-RL', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-Prover-V1.5-RL', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=deepseek-ai/DeepSeek-Prover-V1.5-RL, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)\n",
      "WARNING 09-30 01:51:50 multiproc_gpu_executor.py:56] Reducing Torch parallelism from 16 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-30 01:51:50 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "INFO 09-30 01:51:50 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 09-30 01:51:50 selector.py:116] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8157)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=8156)\u001b[0;0m INFO 09-30 01:51:50 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 09-30 01:51:50 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8157)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=8156)\u001b[0;0m INFO 09-30 01:51:50 selector.py:116] Using XFormers backend.\n",
      "INFO 09-30 01:51:50 selector.py:116] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8155)\u001b[0;0m INFO 09-30 01:51:50 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8155)\u001b[0;0m INFO 09-30 01:51:50 selector.py:116] Using XFormers backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=8155)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=8157)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=8156)\u001b[0;0m /opt/conda/envs/pytorch/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8155)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=8156)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=8157)\u001b[0;0m   @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8155)\u001b[0;0m /opt/conda/envs/pytorch/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8157)\u001b[0;0m /opt/conda/envs/pytorch/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8156)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=8155)\u001b[0;0m /opt/conda/envs/pytorch/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8156)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=8157)\u001b[0;0m   @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=8157)\u001b[0;0m INFO 09-30 01:51:50 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8156)\u001b[0;0m INFO 09-30 01:51:50 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8155)\u001b[0;0m INFO 09-30 01:51:50 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 09-30 01:51:51 utils.py:977] Found nccl from library libnccl.so.2\n",
      "INFO 09-30 01:51:51 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8155)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=8157)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=8156)\u001b[0;0m INFO 09-30 01:51:51 utils.py:977] Found nccl from library libnccl.so.2\n",
      "INFO 09-30 01:51:51 utils.py:977] Found nccl from library libnccl.so.2\n",
      "INFO 09-30 01:51:51 utils.py:977] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8155)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=8157)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=8156)\u001b[0;0m INFO 09-30 01:51:51 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 09-30 01:51:51 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 09-30 01:51:51 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 09-30 01:51:52 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8155)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=8156)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=8157)\u001b[0;0m INFO 09-30 01:51:52 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "INFO 09-30 01:51:52 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "INFO 09-30 01:51:52 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "INFO 09-30 01:51:52 shm_broadcast.py:235] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f42b1d449a0>, local_subscribe_port=49167, remote_subscribe_port=None)\n",
      "INFO 09-30 01:51:52 model_runner.py:915] Starting to load model deepseek-ai/DeepSeek-Prover-V1.5-RL...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8155)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=8156)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=8157)\u001b[0;0m INFO 09-30 01:51:52 model_runner.py:915] Starting to load model deepseek-ai/DeepSeek-Prover-V1.5-RL...\n",
      "INFO 09-30 01:51:52 model_runner.py:915] Starting to load model deepseek-ai/DeepSeek-Prover-V1.5-RL...\n",
      "INFO 09-30 01:51:52 model_runner.py:915] Starting to load model deepseek-ai/DeepSeek-Prover-V1.5-RL...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8157)\u001b[0;0m INFO 09-30 01:51:52 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8157)\u001b[0;0m INFO 09-30 01:51:52 selector.py:116] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8155)\u001b[0;0m INFO 09-30 01:51:52 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 09-30 01:51:52 selector.py:116] Using XFormers backend.\n",
      "INFO 09-30 01:51:52 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8155)\u001b[0;0m INFO 09-30 01:51:52 selector.py:116] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8156)\u001b[0;0m INFO 09-30 01:51:52 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8156)\u001b[0;0m INFO 09-30 01:51:52 selector.py:116] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8155)\u001b[0;0m INFO 09-30 01:51:52 weight_utils.py:236] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8157)\u001b[0;0m INFO 09-30 01:51:52 weight_utils.py:236] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8156)\u001b[0;0m INFO 09-30 01:51:52 weight_utils.py:236] Using model weights format ['*.safetensors']\n",
      "INFO 09-30 01:51:52 weight_utils.py:236] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15d1dfea4dad41baa2aeba3ab71f9d9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=8155)\u001b[0;0m INFO 09-30 01:51:55 model_runner.py:926] Loading model weights took 3.2632 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8157)\u001b[0;0m INFO 09-30 01:51:56 model_runner.py:926] Loading model weights took 3.2632 GB\n",
      "INFO 09-30 01:51:56 model_runner.py:926] Loading model weights took 3.2632 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8156)\u001b[0;0m INFO 09-30 01:51:56 model_runner.py:926] Loading model weights took 3.2632 GB\n",
      "INFO 09-30 01:51:57 distributed_gpu_executor.py:57] # GPU blocks: 4848, # CPU blocks: 2184\n",
      "INFO 09-30 01:52:03 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8157)\u001b[0;0m INFO 09-30 01:52:03 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8156)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=8155)\u001b[0;0m INFO 09-30 01:52:03 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 09-30 01:52:03 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 09-30 01:52:03 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8157)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=8155)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=8156)\u001b[0;0m INFO 09-30 01:52:03 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 09-30 01:52:03 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 09-30 01:52:03 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 09-30 01:52:20 custom_all_reduce.py:223] Registering 2135 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8157)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=8155)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=8156)\u001b[0;0m INFO 09-30 01:52:20 custom_all_reduce.py:223] Registering 2135 cuda graph addresses\n",
      "INFO 09-30 01:52:20 custom_all_reduce.py:223] Registering 2135 cuda graph addresses\n",
      "INFO 09-30 01:52:20 custom_all_reduce.py:223] Registering 2135 cuda graph addresses\n",
      "INFO 09-30 01:52:20 model_runner.py:1335] Graph capturing finished in 17 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8155)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=8156)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=8157)\u001b[0;0m INFO 09-30 01:52:20 model_runner.py:1335] Graph capturing finished in 17 secs.\n",
      "INFO 09-30 01:52:20 model_runner.py:1335] Graph capturing finished in 17 secs.\n",
      "INFO 09-30 01:52:20 model_runner.py:1335] Graph capturing finished in 17 secs.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Two models do not fit on 4 V100s, so you should run this script and paste the output into vllm_PV_final.ipynb.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# V100 settings.\n",
    "llm = LLM(model=\"deepseek-ai/DeepSeek-Prover-V1.5-RL\",\n",
    "          max_num_batched_tokens=8192,\n",
    "          trust_remote_code=True,\n",
    "          dtype=\"float16\",\n",
    "          tensor_parallel_size=4)\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    max_tokens=4096,\n",
    "    temperature=0.0,\n",
    "    top_k=1,\n",
    "    top_p=1.0,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lean_game_dict = {\n",
    "    'header': 'import Mathlib\\nimport Aesop\\n\\nset_option maxHeartbeats 0\\n\\nopen BigOperators Real Nat Topology Rat\\n\\n',\n",
    "    'problem': '/-- The second and fourth terms of a geometric sequence are $2$ and $6$. Which of the following is a possible first term?\\nShow that it is $\\\\frac{2\\\\sqrt{3}}{3}$.-/\\ntheorem amc12b_2003_p6 (a r : \\u211d) (u : \\u2115 \\u2192 \\u211d) (h\\u2080 : \\u2200 k, u k = a * r ^ k) (h\\u2081 : u 1 = 2)\\n    (h\\u2082 : u 3 = 6) : u 0 = 2 / Real.sqrt 3 \\u2228 u 0 = -(2 / Real.sqrt 3) := by\\n',\n",
    "    'old_code' : '  simp_all only [h\\u2080, Nat.cast_one, Nat.cast_zero, Nat.cast_succ, Nat.cast_zero]\\n  have h\\u2083 : a * r = 2 := by linarith\\n  have h\\u2084 : a * r ^ 3 = 6 := by linarith\\n  have h\\u2085 : r ^ 2 = 3 := by\\n    nlinarith\\n',\n",
    "    'tactic_state' : 'a r : ℝ\\nu : ℕ → ℝ\\nh₀ : ∀ (k : ℕ), u k = a * r ^ k\\nh₁ : a * r ^ 1 = 2\\nh₂ : a * r ^ 3 = 6\\nh₃ : a * r = 2\\nh₄ : a * r ^ 3 = 6\\nh₅ : r ^ 2 = 3\\n⊢ a * r ^ 0 = 2 / √3 ∨ a * r ^ 0 = -(2 / √3)\\n'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def construct_context(lean_game_dict: Dict) -> str:\n",
    "    \"\"\"\n",
    "    Generate a prompt for the policy-value worker to suggest comments.\n",
    "    \"\"\"\n",
    "    res = \"\"\"This is a partial Lean 4 proof.\n",
    "```lean4\n",
    "\"\"\"\n",
    "    res += lean_game_dict['header'] + \\\n",
    "        lean_game_dict['problem'] + lean_game_dict['old_code']\n",
    "    res += \"\"\"\n",
    "```\n",
    "Here is the tactic state at this point:\n",
    "```lean4\n",
    "\"\"\"\n",
    "    res += lean_game_dict['tactic_state']\n",
    "    res += f\"\"\"\n",
    "```\n",
    "Please summarize what we have proven so far. \n",
    "Please summarize the current tactic state of the proof.\n",
    "Then, please discuss whether or not the proof is on the right track. Are we proving useful lemmas? Are we using the right tactics? Are we missing any key insights?\n",
    "\"\"\"\n",
    "    return res\n",
    "\n",
    "def policy_suggest_comments(lean_game_dict: Dict, discussion_context : str, num: int = 5) -> str:\n",
    "    # We should call the LLM now.\n",
    "\n",
    "    res = discussion_context + f\"\"\"Here is the tactic state at this point:\n",
    "```lean4\n",
    "\"\"\"\n",
    "    res += lean_game_dict['tactic_state']\n",
    "    res += f\"\"\"\n",
    "```\n",
    "Please suggest 5 ideas to complete the proof.\n",
    "Please delimit each idea with <IDEA></IDEA> tags.\n",
    "\n",
    "<IDEA>\"\"\"\n",
    "    return res\n",
    "\n",
    "def value_suggest_comments(lean_game_dict: Dict, discussion_context : str, num: int = 5) -> str:\n",
    "    # We should call the LLM now.\n",
    "\n",
    "    res = discussion_context + f\"\"\"Here is the tactic state at this point:\n",
    "```lean4\n",
    "\"\"\"\n",
    "    res += lean_game_dict['tactic_state']\n",
    "    res += f\"\"\"\n",
    "```\n",
    "Rate the likelihood that this proof will succeed on a scale of 1 (very unlikely) to 10 (very likely).\n",
    "Please delimit this rating with <RATING></RATING> tags.\n",
    "\n",
    "<RATING>\"\"\"\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a partial Lean 4 proof.\n",
      "```lean4\n",
      "import Mathlib\n",
      "import Aesop\n",
      "\n",
      "set_option maxHeartbeats 0\n",
      "\n",
      "open BigOperators Real Nat Topology Rat\n",
      "\n",
      "/-- The second and fourth terms of a geometric sequence are $2$ and $6$. Which of the following is a possible first term?\n",
      "Show that it is $\\frac{2\\sqrt{3}}{3}$.-/\n",
      "theorem amc12b_2003_p6 (a r : ℝ) (u : ℕ → ℝ) (h₀ : ∀ k, u k = a * r ^ k) (h₁ : u 1 = 2)\n",
      "    (h₂ : u 3 = 6) : u 0 = 2 / Real.sqrt 3 ∨ u 0 = -(2 / Real.sqrt 3) := by\n",
      "  simp_all only [h₀, Nat.cast_one, Nat.cast_zero, Nat.cast_succ, Nat.cast_zero]\n",
      "  have h₃ : a * r = 2 := by linarith\n",
      "  have h₄ : a * r ^ 3 = 6 := by linarith\n",
      "  have h₅ : r ^ 2 = 3 := by\n",
      "    nlinarith\n",
      "\n",
      "```\n",
      "Here is the tactic state at this point:\n",
      "```lean4\n",
      "a r : ℝ\n",
      "u : ℕ → ℝ\n",
      "h₀ : ∀ (k : ℕ), u k = a * r ^ k\n",
      "h₁ : a * r ^ 1 = 2\n",
      "h₂ : a * r ^ 3 = 6\n",
      "h₃ : a * r = 2\n",
      "h₄ : a * r ^ 3 = 6\n",
      "h₅ : r ^ 2 = 3\n",
      "⊢ a * r ^ 0 = 2 / √3 ∨ a * r ^ 0 = -(2 / √3)\n",
      "\n",
      "```\n",
      "Please summarize what we have proven so far. \n",
      "Please summarize the current tactic state of the proof.\n",
      "Then, please discuss whether or not the proof is on the right track. Are we proving useful lemmas? Are we using the right tactics? Are we missing any key insights?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_1 = construct_context(lean_game_dict)\n",
    "print(prompt_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.31s/it, est. speed input: 221.16 toks/s, output: 109.06 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize the proof state:\n",
      "\n",
      "We have a geometric sequence defined by \\( u_k = a \\cdot r^k \\). We know that the second term \\( u_1 = 2 \\) and the fourth term \\( u_3 = 6 \\). We have derived the following equations:\n",
      "1. \\( a \\cdot r = 2 \\)\n",
      "2. \\( a \\cdot r^3 = 6 \\)\n",
      "3. \\( r^2 = 3 \\)\n",
      "\n",
      "Our goal is to show that the first term \\( u_0 \\) is either \\( \\frac{2}{\\sqrt{3}} \\) or \\( -\\frac{2}{\\sqrt{3}} \\).\n",
      "\n",
      "Discussion:\n",
      "\n",
      "The proof state is consistent with the problem statement and the given conditions. We have derived the necessary equations to relate the terms of the geometric sequence. The next steps would involve solving for \\( a \\) and \\( r \\) using the derived equations and then verifying the possible values for \\( u_0 \\).\n",
      "\n",
      "The proof is on the right track as it uses the given conditions to derive useful equations. However, the proof is incomplete as it does not yet solve for \\( a \\) and \\( r \\) explicitly.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "outputs_1 = llm.generate(\n",
    "    prompt_1,\n",
    "    sampling_params=sampling_params\n",
    ")\n",
    "outputs_1 = outputs_1[0].outputs[0].text + \"\\n\"\n",
    "print(outputs_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize the proof state:\n",
      "\n",
      "We have a geometric sequence defined by \\( u_k = a \\cdot r^k \\). We know that the second term \\( u_1 = 2 \\) and the fourth term \\( u_3 = 6 \\). We have derived the following equations:\n",
      "1. \\( a \\cdot r = 2 \\)\n",
      "2. \\( a \\cdot r^3 = 6 \\)\n",
      "3. \\( r^2 = 3 \\)\n",
      "\n",
      "Our goal is to show that the first term \\( u_0 \\) is either \\( \\frac{2}{\\sqrt{3}} \\) or \\( -\\frac{2}{\\sqrt{3}} \\).\n",
      "\n",
      "Discussion:\n",
      "\n",
      "The proof state is consistent with the problem statement and the given conditions. We have derived the necessary equations to relate the terms of the geometric sequence. The next steps would involve solving for \\( a \\) and \\( r \\) using the derived equations and then verifying the possible values for \\( u_0 \\).\n",
      "\n",
      "The proof is on the right track as it uses the given conditions to derive useful equations. However, the proof is incomplete as it does not yet solve for \\( a \\) and \\( r \\) explicitly.\n",
      "Here is the tactic state at this point:\n",
      "```lean4\n",
      "a r : ℝ\n",
      "u : ℕ → ℝ\n",
      "h₀ : ∀ (k : ℕ), u k = a * r ^ k\n",
      "h₁ : a * r ^ 1 = 2\n",
      "h₂ : a * r ^ 3 = 6\n",
      "h₃ : a * r = 2\n",
      "h₄ : a * r ^ 3 = 6\n",
      "h₅ : r ^ 2 = 3\n",
      "⊢ a * r ^ 0 = 2 / √3 ∨ a * r ^ 0 = -(2 / √3)\n",
      "\n",
      "```\n",
      "Please suggest 5 ideas to complete the proof.\n",
      "Please delimit each idea with <IDEA></IDEA> tags.\n",
      "\n",
      "<IDEA>\n"
     ]
    }
   ],
   "source": [
    "prompt_2 = policy_suggest_comments(lean_game_dict, outputs_1)\n",
    "print(prompt_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.31s/it, est. speed input: 101.37 toks/s, output: 110.18 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Solve for \\( r \\) using \\( r^2 = 3 \\).\n",
      "2. Substitute \\( r \\) back into the equation \\( a \\cdot r = 2 \\) to solve for \\( a \\).\n",
      "3. Verify the possible values for \\( u_0 \\) using the derived values of \\( a \\) and \\( r \\).\n",
      "\n",
      "</IDEA>\n",
      "\n",
      "Let's complete the proof step by step.\n",
      "\n",
      "1. **Solve for \\( r \\) using \\( r^2 = 3 \\):**\n",
      "   \\[\n",
      "   r = \\sqrt{3} \\quad \\text{or} \\quad r = -\\sqrt{3}\n",
      "   \\]\n",
      "\n",
      "2. **Substitute \\( r \\) back into the equation \\( a \\cdot r = 2 \\) to solve for \\( a \\):**\n",
      "   - If \\( r = \\sqrt{3} \\):\n",
      "     \\[\n",
      "     a \\cdot \\sqrt{3} = 2 \\implies a = \\frac{2}{\\sqrt{3}}\n",
      "     \\]\n",
      "   - If \\( r = -\\sqrt{3} \\):\n",
      "     \\[\n",
      "     a \\cdot (-\\sqrt{3}) = 2 \\implies a = -\\frac{2}{\\sqrt{3}}\n",
      "     \\]\n",
      "\n",
      "3. **Verify the possible values for \\( u_0 \\) using the derived values of \\( a \\) and \\( r \\):**\n",
      "   - For \\( r = \\sqrt{3} \\) and \\( a = \\frac{2}{\\sqrt{3}} \\):\n",
      "     \\[\n",
      "     u_0 = a \\cdot r^0 = \\frac{2}{\\sqrt{3}}\n",
      "     \\]\n",
      "   - For \\( r = -\\sqrt{3} \\) and \\( a = -\\frac{2}{\\sqrt{3}} \\):\n",
      "     \\[\n",
      "     u_0 = a \\cdot r^0 = -\\frac{2}{\\sqrt{3}}\n",
      "     \\]\n",
      "\n",
      "Thus, the first term \\( u_0 \\) is either \\( \\frac{2}{\\sqrt{3}} \\) or \\( -\\frac{2}{\\sqrt{3}} \\).\n",
      "\n",
      "Therefore, the proof is complete, and the final answer is:\n",
      "\\[\n",
      "\\boxed{\\text{True}}\n",
      "\\]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "outputs_2 = llm.generate(\n",
    "    prompt_2,\n",
    "    sampling_params=sampling_params\n",
    ")\n",
    "outputs_2 = outputs_2[0].outputs[0].text + \"\\n\"\n",
    "print(outputs_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
