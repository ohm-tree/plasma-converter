{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict, Optional\n",
    "from collections import defaultdict\n",
    "\n",
    "from vllm import LLM, SamplingParams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 09-30 01:37:55 config.py:1651] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 09-30 01:37:55 config.py:890] Defaulting to use mp for distributed inference\n",
      "INFO 09-30 01:37:55 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='deepseek-ai/deepseek-math-7b-instruct', speculative_config=None, tokenizer='deepseek-ai/deepseek-math-7b-instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=deepseek-ai/deepseek-math-7b-instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=7541)\u001b[0;0m INFO 09-30 01:37:56 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=7544)\u001b[0;0m INFO 09-30 01:37:56 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=7549)\u001b[0;0m INFO 09-30 01:37:56 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=7544)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=7541)\u001b[0;0m ERROR 09-30 01:37:56 multiproc_worker_utils.py:226] Exception in worker VllmWorkerProcess while processing method init_device: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method, Traceback (most recent call last):\n",
      "ERROR 09-30 01:37:56 multiproc_worker_utils.py:226] Exception in worker VllmWorkerProcess while processing method init_device: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method, Traceback (most recent call last):\n",
      "\u001b[1;36m(VllmWorkerProcess pid=7541)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=7544)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=7549)\u001b[0;0m ERROR 09-30 01:37:56 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/vllm/executor/multiproc_worker_utils.py\", line 223, in _run_worker_process\n",
      "ERROR 09-30 01:37:56 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/vllm/executor/multiproc_worker_utils.py\", line 223, in _run_worker_process\n",
      "ERROR 09-30 01:37:56 multiproc_worker_utils.py:226] Exception in worker VllmWorkerProcess while processing method init_device: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method, Traceback (most recent call last):\n",
      "\u001b[1;36m(VllmWorkerProcess pid=7544)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=7541)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=7549)\u001b[0;0m ERROR 09-30 01:37:56 multiproc_worker_utils.py:226]     output = executor(*args, **kwargs)\n",
      "ERROR 09-30 01:37:56 multiproc_worker_utils.py:226]     output = executor(*args, **kwargs)\n",
      "ERROR 09-30 01:37:56 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/vllm/executor/multiproc_worker_utils.py\", line 223, in _run_worker_process\n",
      "\u001b[1;36m(VllmWorkerProcess pid=7544)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=7541)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=7549)\u001b[0;0m ERROR 09-30 01:37:56 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/vllm/worker/worker.py\", line 166, in init_device\n",
      "ERROR 09-30 01:37:56 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/vllm/worker/worker.py\", line 166, in init_device\n",
      "\u001b[1;36m(VllmWorkerProcess pid=7544)\u001b[0;0m ERROR 09-30 01:37:56 multiproc_worker_utils.py:226]     output = executor(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=7541)\u001b[0;0m ERROR 09-30 01:37:56 multiproc_worker_utils.py:226]     torch.cuda.set_device(self.device)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=7549)\u001b[0;0m ERROR 09-30 01:37:56 multiproc_worker_utils.py:226]     torch.cuda.set_device(self.device)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=7544)\u001b[0;0m ERROR 09-30 01:37:56 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/vllm/worker/worker.py\", line 166, in init_device\n",
      "\u001b[1;36m(VllmWorkerProcess pid=7541)\u001b[0;0m ERROR 09-30 01:37:56 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 420, in set_device\n",
      "\u001b[1;36m(VllmWorkerProcess pid=7549)\u001b[0;0m ERROR 09-30 01:37:56 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 420, in set_device\n",
      "\u001b[1;36m(VllmWorkerProcess pid=7544)\u001b[0;0m ERROR 09-30 01:37:56 multiproc_worker_utils.py:226]     torch.cuda.set_device(self.device)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=7541)\u001b[0;0m ERROR 09-30 01:37:56 multiproc_worker_utils.py:226]     torch._C._cuda_setDevice(device)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=7549)\u001b[0;0m ERROR 09-30 01:37:56 multiproc_worker_utils.py:226]     torch._C._cuda_setDevice(device)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=7544)\u001b[0;0m ERROR 09-30 01:37:56 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 420, in set_device\n",
      "\u001b[1;36m(VllmWorkerProcess pid=7541)\u001b[0;0m ERROR 09-30 01:37:56 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 300, in _lazy_init\n",
      "\u001b[1;36m(VllmWorkerProcess pid=7549)\u001b[0;0m ERROR 09-30 01:37:56 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 300, in _lazy_init\n",
      "\u001b[1;36m(VllmWorkerProcess pid=7544)\u001b[0;0m ERROR 09-30 01:37:56 multiproc_worker_utils.py:226]     torch._C._cuda_setDevice(device)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=7541)\u001b[0;0m ERROR 09-30 01:37:56 multiproc_worker_utils.py:226]     raise RuntimeError(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=7549)\u001b[0;0m ERROR 09-30 01:37:56 multiproc_worker_utils.py:226]     raise RuntimeError(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=7544)\u001b[0;0m ERROR 09-30 01:37:56 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 300, in _lazy_init\n",
      "\u001b[1;36m(VllmWorkerProcess pid=7541)\u001b[0;0m ERROR 09-30 01:37:56 multiproc_worker_utils.py:226] RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n",
      "\u001b[1;36m(VllmWorkerProcess pid=7549)\u001b[0;0m ERROR 09-30 01:37:56 multiproc_worker_utils.py:226] RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n",
      "\u001b[1;36m(VllmWorkerProcess pid=7544)\u001b[0;0m ERROR 09-30 01:37:56 multiproc_worker_utils.py:226]     raise RuntimeError(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=7541)\u001b[0;0m ERROR 09-30 01:37:56 multiproc_worker_utils.py:226] \n",
      "\u001b[1;36m(VllmWorkerProcess pid=7549)\u001b[0;0m ERROR 09-30 01:37:56 multiproc_worker_utils.py:226] \n",
      "ERROR 09-30 01:37:56 multiproc_worker_utils.py:226] RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n",
      "\u001b[1;36m(VllmWorkerProcess pid=7549)\u001b[0;0m ERROR 09-30 01:37:56 multiproc_worker_utils.py:226] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mTwo models do not fit on 4 V100s, so you should run this script and paste the output into vllm_PV_final.ipynb.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# V100 settings.\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdeepseek-ai/deepseek-math-7b-instruct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m          \u001b[49m\u001b[43mmax_num_batched_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8192\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m          \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m          \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfloat16\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m          \u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m sampling_params \u001b[38;5;241m=\u001b[39m SamplingParams(\n\u001b[1;32m     13\u001b[0m     max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4096\u001b[39m,\n\u001b[1;32m     14\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m,\n\u001b[1;32m     15\u001b[0m     top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     16\u001b[0m     top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m,\n\u001b[1;32m     17\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/vllm/entrypoints/llm.py:177\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_context_len_to_capture, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere is no need to pass vision-related arguments anymore.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    155\u001b[0m engine_args \u001b[38;5;241m=\u001b[39m EngineArgs(\n\u001b[1;32m    156\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    157\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    176\u001b[0m )\n\u001b[0;32m--> 177\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m \u001b[43mLLMEngine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUsageContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/vllm/engine/llm_engine.py:538\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    536\u001b[0m executor_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_executor_cls(engine_config)\n\u001b[1;32m    537\u001b[0m \u001b[38;5;66;03m# Create the LLM engine.\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mengine_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m engine\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/vllm/engine/llm_engine.py:305\u001b[0m, in \u001b[0;36mLLMEngine.__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, device_config, load_config, lora_config, speculative_config, decoding_config, observability_config, prompt_adapter_config, executor_class, log_stats, usage_context, stat_loggers, input_registry, step_return_finished_only)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_registry \u001b[38;5;241m=\u001b[39m input_registry\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_processor \u001b[38;5;241m=\u001b[39m input_registry\u001b[38;5;241m.\u001b[39mcreate_input_processor(\n\u001b[1;32m    303\u001b[0m     model_config)\n\u001b[0;32m--> 305\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_executor \u001b[38;5;241m=\u001b[39m \u001b[43mexecutor_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspeculative_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspeculative_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_adapter_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_adapter_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobservability_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservability_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39membedding_mode:\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_kv_caches()\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/vllm/executor/distributed_gpu_executor.py:26\u001b[0m, in \u001b[0;36mDistributedGPUExecutor.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Updated by implementations that require additional args to be passed\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# to the _run_workers execute_model call\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextra_execute_model_run_workers_kwargs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/vllm/executor/executor_base.py:47\u001b[0m, in \u001b[0;36mExecutorBase.__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, device_config, load_config, lora_config, speculative_config, prompt_adapter_config, observability_config)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt_adapter_config \u001b[38;5;241m=\u001b[39m prompt_adapter_config\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservability_config \u001b[38;5;241m=\u001b[39m observability_config\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_executor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/vllm/executor/multiproc_gpu_executor.py:124\u001b[0m, in \u001b[0;36mMultiprocessingGPUExecutor._init_executor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    120\u001b[0m     signal\u001b[38;5;241m.\u001b[39msignal(signal\u001b[38;5;241m.\u001b[39mSIGTERM, shutdown)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver_worker \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_worker(\n\u001b[1;32m    123\u001b[0m     distributed_init_method\u001b[38;5;241m=\u001b[39mdistributed_init_method)\n\u001b[0;32m--> 124\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minit_device\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_workers(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_model\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    126\u001b[0m                   max_concurrent_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_config\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    127\u001b[0m                   max_parallel_loading_workers)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/vllm/executor/multiproc_gpu_executor.py:203\u001b[0m, in \u001b[0;36mMultiprocessingGPUExecutor._run_workers\u001b[0;34m(self, method, async_run_tensor_parallel_workers_only, max_concurrent_workers, *args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m driver_worker_output \u001b[38;5;241m=\u001b[39m driver_worker_method(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# Get the results of the workers.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [driver_worker_output\n\u001b[0;32m--> 203\u001b[0m         ] \u001b[38;5;241m+\u001b[39m [output\u001b[38;5;241m.\u001b[39mget() \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m worker_outputs]\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/vllm/executor/multiproc_gpu_executor.py:203\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    199\u001b[0m driver_worker_output \u001b[38;5;241m=\u001b[39m driver_worker_method(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# Get the results of the workers.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [driver_worker_output\n\u001b[0;32m--> 203\u001b[0m         ] \u001b[38;5;241m+\u001b[39m [\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m worker_outputs]\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/vllm/executor/multiproc_worker_utils.py:58\u001b[0m, in \u001b[0;36mResultFuture.get\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult\u001b[38;5;241m.\u001b[39mexception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult\u001b[38;5;241m.\u001b[39mexception\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult\u001b[38;5;241m.\u001b[39mvalue\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Two models do not fit on 4 V100s, so you should run this script and paste the output into vllm_PV_final.ipynb.\n",
    "\"\"\"\n",
    "\n",
    "# V100 settings.\n",
    "llm = LLM(model=\"deepseek-ai/deepseek-math-7b-instruct\",\n",
    "          max_num_batched_tokens=8192,\n",
    "          trust_remote_code=True,\n",
    "          dtype=\"float16\",\n",
    "          tensor_parallel_size=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sampling_params = SamplingParams(\n",
    "    max_tokens=4096,\n",
    "    temperature=1.0,\n",
    "    top_k=1,\n",
    "    top_p=1.0,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lean_game_dict = {\n",
    "    'header': 'import Mathlib\\nimport Aesop\\n\\nset_option maxHeartbeats 0\\n\\nopen BigOperators Real Nat Topology Rat\\n\\n',\n",
    "    'problem': '/-- The second and fourth terms of a geometric sequence are $2$ and $6$. Which of the following is a possible first term?\\nShow that it is $\\\\frac{2\\\\sqrt{3}}{3}$.-/\\ntheorem amc12b_2003_p6 (a r : \\u211d) (u : \\u2115 \\u2192 \\u211d) (h\\u2080 : \\u2200 k, u k = a * r ^ k) (h\\u2081 : u 1 = 2)\\n    (h\\u2082 : u 3 = 6) : u 0 = 2 / Real.sqrt 3 \\u2228 u 0 = -(2 / Real.sqrt 3) := by\\n',\n",
    "    'old_code' : '  simp_all only [h\\u2080, Nat.cast_one, Nat.cast_zero, Nat.cast_succ, Nat.cast_zero]\\n  have h\\u2083 : a * r = 2 := by linarith\\n  have h\\u2084 : a * r ^ 3 = 6 := by linarith\\n  have h\\u2085 : r ^ 2 = 3 := by\\n    nlinarith\\n',\n",
    "    'tactic_state' : 'a r : ℝ\\nu : ℕ → ℝ\\nh₀ : ∀ (k : ℕ), u k = a * r ^ k\\nh₁ : a * r ^ 1 = 2\\nh₂ : a * r ^ 3 = 6\\nh₃ : a * r = 2\\nh₄ : a * r ^ 3 = 6\\nh₅ : r ^ 2 = 3\\n⊢ a * r ^ 0 = 2 / √3 ∨ a * r ^ 0 = -(2 / √3)\\n'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_1 = r\"\"\"Summarize the proof state:\n",
    "\n",
    "We have a geometric sequence defined by \\( u_k = a \\cdot r^k \\). We know that the second term \\( u_1 = 2 \\) and the fourth term \\( u_3 = 6 \\). We have derived the following equations:\n",
    "1. \\( a \\cdot r = 2 \\)\n",
    "2. \\( a \\cdot r^3 = 6 \\)\n",
    "3. \\( r^2 = 3 \\)\n",
    "\n",
    "Our goal is to show that the first term \\( u_0 \\) is either \\( \\frac{2}{\\sqrt{3}} \\) or \\( -\\frac{2}{\\sqrt{3}} \\).\n",
    "\n",
    "Discussion:\n",
    "\n",
    "The proof state is consistent with the problem statement and the given conditions. We have derived the necessary equations to relate the terms of the geometric sequence. The next steps would involve solving for \\( a \\) and \\( r \\) using the derived equations and then verifying the possible values for \\( u_0 \\).\n",
    "\n",
    "The proof is on the right track as it uses the given conditions to derive useful equations. However, the proof is incomplete as it does not yet solve for \\( a \\) and \\( r \\) explicitly.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_value_suggest_comments(lean_game_dict: Dict, discussion_context : str, num: int = 5) -> str:\n",
    "    # We should call the LLM now.\n",
    "\n",
    "    res = discussion_context + f\"\"\"Here is the tactic state at this point:\n",
    "```lean4\n",
    "\"\"\"\n",
    "    res += lean_game_dict['tactic_state']\n",
    "    res += f\"\"\"\n",
    "```\n",
    "Please suggest 5 ideas to complete the proof.\n",
    "Please delimit each idea with <IDEA></IDEA> tags.\n",
    "Rate the likelihood that this proof will succeed on a scale of 1 (very unlikely) to 10 (very likely).\n",
    "Please delimit this rating with <RATING></RATING> tags.\n",
    "\n",
    "<IDEA>\"\"\"\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize the proof state:\n",
      "\n",
      "We have a geometric sequence defined by \\( u_k = a \\cdot r^k \\). We know that the second term \\( u_1 = 2 \\) and the fourth term \\( u_3 = 6 \\). We have derived the following equations:\n",
      "1. \\( a \\cdot r = 2 \\)\n",
      "2. \\( a \\cdot r^3 = 6 \\)\n",
      "3. \\( r^2 = 3 \\)\n",
      "\n",
      "Our goal is to show that the first term \\( u_0 \\) is either \\( \\frac{2}{\\sqrt{3}} \\) or \\( -\\frac{2}{\\sqrt{3}} \\).\n",
      "\n",
      "Discussion:\n",
      "\n",
      "The proof state is consistent with the problem statement and the given conditions. We have derived the necessary equations to relate the terms of the geometric sequence. The next steps would involve solving for \\( a \\) and \\( r \\) using the derived equations and then verifying the possible values for \\( u_0 \\).\n",
      "\n",
      "The proof is on the right track as it uses the given conditions to derive useful equations. However, the proof is incomplete as it does not yet solve for \\( a \\) and \\( r \\) explicitly.\n",
      "Here is the tactic state at this point:\n",
      "```lean4\n",
      "a r : ℝ\n",
      "u : ℕ → ℝ\n",
      "h₀ : ∀ (k : ℕ), u k = a * r ^ k\n",
      "h₁ : a * r ^ 1 = 2\n",
      "h₂ : a * r ^ 3 = 6\n",
      "h₃ : a * r = 2\n",
      "h₄ : a * r ^ 3 = 6\n",
      "h₅ : r ^ 2 = 3\n",
      "⊢ a * r ^ 0 = 2 / √3 ∨ a * r ^ 0 = -(2 / √3)\n",
      "\n",
      "```\n",
      "Please suggest 5 ideas to complete the proof.\n",
      "Please delimit each idea with <IDEA></IDEA> tags.\n",
      "Rate the likelihood that this proof will succeed on a scale of 1 (very unlikely) to 10 (very likely).\n",
      "Please delimit this rating with <RATING></RATING> tags.\n",
      "\n",
      "<IDEA>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.59s/it, est. speed input: 301.45 toks/s, output: 100.06 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Solve the system of equations to find the values of a and r.</IDEA>\n",
      "<IDEA>2. Substitute the values of a and r into the equation for u_0.</IDEA>\n",
      "<IDEA>3. Simplify the equation for u_0 to show that it equals either 2/√3 or -2/√3.</IDEA>\n",
      "<IDEA>4. Use the properties of geometric sequences to verify that the values of u_0 are correct.</IDEA>\n",
      "<IDEA>5. Use the derived equations to show that the values of u_0 are indeed 2/√3 or -2/√3.</IDEA>\n",
      "\n",
      "<RATING>8</RATING>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_2 = policy_value_suggest_comments(lean_game_dict, output_1)\n",
    "print(prompt_2)\n",
    "\n",
    "outputs = llm.generate(\n",
    "    prompt_2,\n",
    "    sampling_params=sampling_params\n",
    ")\n",
    "outputs = outputs[0].outputs[0].text + \"\\n\"\n",
    "print(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
