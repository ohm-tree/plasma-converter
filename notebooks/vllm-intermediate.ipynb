{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'deepseek-ai/DeepSeek-Prover-V1.5-RL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-23 16:12:39 llm_engine.py:98] Initializing an LLM engine (v0.4.1) with config: model='deepseek-ai/DeepSeek-Prover-V1.5-RL', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-Prover-V1.5-RL', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-23 16:12:39 utils.py:608] Found nccl from library /home/ubuntu/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "INFO 09-23 16:12:40 selector.py:28] Using FlashAttention backend.\n",
      "INFO 09-23 16:12:41 weight_utils.py:193] Using model weights format ['*.safetensors']\n",
      "INFO 09-23 16:12:45 model_runner.py:173] Loading model weights took 12.8725 GB\n",
      "INFO 09-23 16:12:46 gpu_executor.py:119] # GPU blocks: 2858, # CPU blocks: 546\n",
      "INFO 09-23 16:12:51 model_runner.py:976] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 09-23 16:12:51 model_runner.py:980] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6696/2269833535.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_num_batched_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8192\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/vllm/entrypoints/llm.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, enforce_eager, max_context_len_to_capture, disable_custom_all_reduce, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         )\n\u001b[0;32m--> 118\u001b[0;31m         self.llm_engine = LLMEngine.from_engine_args(\n\u001b[0m\u001b[1;32m    119\u001b[0m             engine_args, usage_context=UsageContext.LLM_CLASS)\n\u001b[1;32m    120\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/vllm/engine/llm_engine.py\u001b[0m in \u001b[0;36mfrom_engine_args\u001b[0;34m(cls, engine_args, usage_context)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;31m# Create the LLM engine.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         engine = cls(\n\u001b[0m\u001b[1;32m    278\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mengine_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0mexecutor_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexecutor_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/vllm/engine/llm_engine.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, device_config, load_config, lora_config, vision_language_config, speculative_config, decoding_config, executor_class, log_stats, usage_context)\u001b[0m\n\u001b[1;32m    158\u001b[0m         )\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_kv_caches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;31m# If usage stat is enabled, collect relevant info.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/vllm/engine/llm_engine.py\u001b[0m in \u001b[0;36m_initialize_kv_caches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_cpu_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_cpu_blocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_gpu_blocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_cpu_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/vllm/executor/gpu_executor.py\u001b[0m in \u001b[0;36minitialize_cache\u001b[0;34m(self, num_gpu_blocks, num_cpu_blocks)\u001b[0m\n\u001b[1;32m    120\u001b[0m                     f\"# CPU blocks: {num_cpu_blocks}\")\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdriver_worker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_gpu_blocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_cpu_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     def execute_model(\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/vllm/worker/worker.py\u001b[0m in \u001b[0;36minitialize_cache\u001b[0;34m(self, num_gpu_blocks, num_cpu_blocks)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_cache_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warm_up_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_init_cache_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/vllm/worker/worker.py\u001b[0m in \u001b[0;36m_warm_up_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_warm_up_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menforce_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_runner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;31m# Reset the seed to ensure that the random state is not affected by\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;31m# the model initialization and profiling.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/vllm/worker/model_runner.py\u001b[0m in \u001b[0;36mcapture_model\u001b[0;34m(self, kv_caches)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m                 \u001b[0mgraph_runner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCUDAGraphRunner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m                 graph_runner.capture(\n\u001b[0m\u001b[1;32m   1045\u001b[0m                     \u001b[0minput_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m                     \u001b[0minput_positions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/vllm/worker/model_runner.py\u001b[0m in \u001b[0;36mcapture\u001b[0;34m(self, input_ids, positions, kv_caches, attn_metadata, memory_pool, **kwargs)\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;31m# https://stackoverflow.com/questions/31039022/python-multi-line-with-statement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCUDAGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1115\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemory_pool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: SIM117\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_maybe_pynccl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m                 hidden_states = self.model(\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/cuda/graphs.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;31m# Free as much memory as we can for the graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynchronize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "llm = LLM(model=model_path, max_num_batched_tokens=8192, seed=0, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e4ecca0f5e24184bc495fbf9c7f4dad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "auto_llm = AutoModelForCausalLM.from_pretrained(\n",
    "        'deepseek-ai/DeepSeek-Prover-V1.5-RL',\n",
    "        trust_remote_code=True,\n",
    "        device_map=None,\n",
    "        cache_dir='model_weights'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(102400, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-24): 25 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=102400, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs {'input_ids': tensor([[100000,  12423,   1376,     61,     17,   1879,    207,     15]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  4.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs [RequestOutput(request_id=1, prompt='Show x^2 > 0', prompt_token_ids=[100000, 12423, 1376, 61, 17, 1879, 207, 15], prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' for all x in Re #11045\\nTo show that', token_ids=[327, 521, 1376, 279, 1926, 1501, 16, 16, 15, 19, 20, 185, 100000, 1898, 1296, 344], cumulative_logprob=-22.468135497299954, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1727059882.28389, last_token_time=1727059882.28389, first_scheduled_time=1727059882.2856731, first_token_time=1727059882.300276, time_in_queue=0.001783132553100586, finished_time=1727059882.4949777), lora_request=None)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'hidden_states'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8768/2311429028.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"outputs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Retrieve hidden states from the outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Extract the activations from the 25th intermediate layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'hidden_states'"
     ]
    }
   ],
   "source": [
    "x = \"Show x^2 > 0\"\n",
    "\n",
    "# Initialize the tokenizer with the same model path\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(x, return_tensors='pt')\n",
    "\n",
    "# Ensure the model outputs hidden states\n",
    "model = llm.generate\n",
    "llm.llm_engine.model_config.output_hidden_states = True\n",
    "\n",
    "# Pass the inputs through the model\n",
    "with torch.no_grad():\n",
    "    print(\"inputs\", inputs)\n",
    "    outputs = llm.generate(x)\n",
    "\n",
    "print(\"outputs\", outputs)\n",
    "# Retrieve hidden states from the outputs\n",
    "hidden_states = outputs.hidden_states\n",
    "\n",
    "# Extract the activations from the 25th intermediate layer\n",
    "# Note: hidden_states[0] is the embedding layer, so hidden_states[25] corresponds to the 25th layer\n",
    "layer_num = 25\n",
    "layer_25_activations = hidden_states[layer_num]\n",
    "\n",
    "print(\"Activations of the 25th intermediate layer:\")\n",
    "print(layer_25_activations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload truncated model to HF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Truncate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a684b657c3241ee972dcbda2d836884",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "auto_llm25 = AutoModelForCausalLM.from_pretrained(\n",
    "        'deepseek-ai/DeepSeek-Prover-V1.5-RL',\n",
    "        trust_remote_code=True,\n",
    "        device_map=None,\n",
    "        cache_dir='model_weights'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training': False,\n",
       " '_parameters': OrderedDict([('weight',\n",
       "               Parameter containing:\n",
       "               tensor([[-3.8300e-03,  2.4567e-03, -2.1875e-01,  ..., -4.0588e-03,\n",
       "                         6.8970e-03, -5.8899e-03],\n",
       "                       [ 3.2501e-03, -6.5918e-03,  2.4023e-01,  ...,  3.5248e-03,\n",
       "                        -7.0801e-03,  6.3171e-03],\n",
       "                       [ 3.9978e-03, -2.5635e-03,  2.1484e-01,  ...,  2.4109e-03,\n",
       "                        -8.7891e-03,  4.8523e-03],\n",
       "                       ...,\n",
       "                       [-2.5391e-02,  2.0020e-02, -7.0801e-02,  ..., -3.1738e-02,\n",
       "                        -4.4189e-02, -1.4282e-02],\n",
       "                       [-1.9409e-02,  2.8198e-02,  2.1973e-01,  ...,  1.3916e-02,\n",
       "                        -3.7354e-02,  2.4048e-02],\n",
       "                       [ 3.5400e-02, -3.1738e-02,  2.9785e-02,  ...,  3.6621e-02,\n",
       "                         5.3711e-02,  1.1301e-04]], requires_grad=True)),\n",
       "              ('bias', None)]),\n",
       " '_buffers': OrderedDict(),\n",
       " '_non_persistent_buffers_set': set(),\n",
       " '_backward_pre_hooks': OrderedDict(),\n",
       " '_backward_hooks': OrderedDict(),\n",
       " '_is_full_backward_hook': None,\n",
       " '_forward_hooks': OrderedDict(),\n",
       " '_forward_hooks_with_kwargs': OrderedDict(),\n",
       " '_forward_hooks_always_called': OrderedDict(),\n",
       " '_forward_pre_hooks': OrderedDict(),\n",
       " '_forward_pre_hooks_with_kwargs': OrderedDict(),\n",
       " '_state_dict_hooks': OrderedDict(),\n",
       " '_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_post_hooks': OrderedDict(),\n",
       " '_modules': OrderedDict(),\n",
       " 'in_features': 4096,\n",
       " 'out_features': 4096,\n",
       " '_is_hf_initialized': True}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_llm25._modules['model'].layers[0].self_attn.q_proj.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_llm25.config.num_hidden_layers = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_llm25.model.layers = auto_llm.model.layers[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaModel(\n",
       "  (embed_tokens): Embedding(102400, 4096)\n",
       "  (layers): ModuleList(\n",
       "    (0-24): 25 x LlamaDecoderLayer(\n",
       "      (self_attn): LlamaSdpaAttention(\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "        (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "        (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm()\n",
       "      (post_attention_layernorm): LlamaRMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm): LlamaRMSNorm()\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_llm25.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['model.embed_tokens.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.mlp.down_proj.weight', 'model.layers.0.input_layernorm.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.input_layernorm.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.input_layernorm.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.input_layernorm.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.input_layernorm.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.input_layernorm.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.input_layernorm.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.input_layernorm.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.input_layernorm.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.input_layernorm.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.input_layernorm.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.input_layernorm.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.input_layernorm.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.21.input_layernorm.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.22.input_layernorm.weight', 'model.layers.22.post_attention_layernorm.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.23.input_layernorm.weight', 'model.layers.23.post_attention_layernorm.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.24.input_layernorm.weight', 'model.layers.24.post_attention_layernorm.weight', 'model.norm.weight', 'lm_head.weight'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = auto_llm25.state_dict()\n",
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_llm25.load_state_dict(state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_llm25.state_dict()['model.norm.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = 'ohm-tree/DeepSeek-Prover-V1.5-25'\n",
    "auto_llm25.save_pretrained(save_directory, safe_serialization=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'deepseek-ai/DeepSeek-Prover-V1.5-RL',\n",
    "    trust_remote_code=True,\n",
    "    cache_dir='model_weights'\n",
    ")\n",
    "# tokenizer.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /home/ubuntu/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token='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')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_name = 'deepseek-prover-1.5-25'  # desired repository name\n",
    "username = 'czhang2718'  # Hugging Face username\n",
    "repo_id = f\"{username}/{repo_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "026d38290c9946bd8a53b35627868dac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00003.bin:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dd28a46b22345ed9dc0fb33cd6d6ac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8be7ba96c94d4ee7a07b7084fdbae3a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00003-of-00003.bin:   0%|          | 0.00/1.83G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "518ae1afdee3469eb3eceb00c370a5d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00003.bin:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/czhang2718/deepseek-prover-1.5-25/commit/c3bdab5a8821291914aca26d8ca97b2cfd8cfb73', commit_message='Upload tokenizer', commit_description='', oid='c3bdab5a8821291914aca26d8ca97b2cfd8cfb73', pr_url=None, repo_url=RepoUrl('https://huggingface.co/czhang2718/deepseek-prover-1.5-25', endpoint='https://huggingface.co', repo_type='model', repo_id='czhang2718/deepseek-prover-1.5-25'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_llm25.push_to_hub(repo_id, safe_serialization=False)\n",
    "tokenizer.push_to_hub(repo_id, safe_serialization=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test loading in vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "172eac09a06f48589bc34d0f4ac571ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/704 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-23 15:25:13 llm_engine.py:98] Initializing an LLM engine (v0.4.1) with config: model='czhang2718/deepseek-prover-1.5-25', speculative_config=None, tokenizer='czhang2718/deepseek-prover-1.5-25', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-23 15:25:13 weight_utils.py:193] Using model weights format ['*.bin']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64dffc045cae4923b4796d6bcd32eb83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00003.bin:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a15bfbaf1ad6418fb5ec16805427185c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00003.bin:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b763f8abca0b4690825744553cd350e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00003-of-00003.bin:   0%|          | 0.00/1.83G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Cannot copy out of meta tensor; no data!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5701/3875476847.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtruncacted_llm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_num_batched_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8192\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/vllm/entrypoints/llm.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, enforce_eager, max_context_len_to_capture, disable_custom_all_reduce, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         )\n\u001b[0;32m--> 118\u001b[0;31m         self.llm_engine = LLMEngine.from_engine_args(\n\u001b[0m\u001b[1;32m    119\u001b[0m             engine_args, usage_context=UsageContext.LLM_CLASS)\n\u001b[1;32m    120\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/vllm/engine/llm_engine.py\u001b[0m in \u001b[0;36mfrom_engine_args\u001b[0;34m(cls, engine_args, usage_context)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;31m# Create the LLM engine.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         engine = cls(\n\u001b[0m\u001b[1;32m    278\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mengine_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0mexecutor_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexecutor_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/vllm/engine/llm_engine.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, device_config, load_config, lora_config, vision_language_config, speculative_config, decoding_config, executor_class, log_stats, usage_context)\u001b[0m\n\u001b[1;32m    146\u001b[0m             model_config)\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         self.model_executor = executor_class(\n\u001b[0m\u001b[1;32m    149\u001b[0m             \u001b[0mmodel_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0mcache_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/vllm/executor/executor_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, device_config, load_config, lora_config, vision_language_config, speculative_config)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspeculative_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspeculative_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_executor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/vllm/executor/gpu_executor.py\u001b[0m in \u001b[0;36m_init_executor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \"\"\"\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspeculative_config\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_non_spec_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_spec_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/vllm/executor/gpu_executor.py\u001b[0m in \u001b[0;36m_init_non_spec_worker\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m         )\n\u001b[1;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdriver_worker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdriver_worker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_init_spec_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/vllm/worker/worker.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_runner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/vllm/worker/model_runner.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mCudaMemoryProfiler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             self.model = get_model(\n\u001b[0m\u001b[1;32m    163\u001b[0m                 \u001b[0mmodel_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mdevice_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(model_config, load_config, device_config, parallel_config, scheduler_config, lora_config, vision_language_config)\u001b[0m\n\u001b[1;32m     17\u001b[0m         vision_language_config: Optional[VisionLanguageConfig]) -> nn.Module:\n\u001b[1;32m     18\u001b[0m     \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     return loader.load_model(model_config=model_config,\n\u001b[0m\u001b[1;32m     20\u001b[0m                              \u001b[0mdevice_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                              \u001b[0mlora_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlora_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self, model_config, device_config, lora_config, vision_language_config, parallel_config, scheduler_config)\u001b[0m\n\u001b[1;32m    222\u001b[0m                 model = _initialize_model(model_config, self.load_config,\n\u001b[1;32m    223\u001b[0m                                           lora_config, vision_language_config)\n\u001b[0;32m--> 224\u001b[0;31m             model.load_weights(\n\u001b[0m\u001b[1;32m    225\u001b[0m                 self._get_weights_iterator(model_config.model,\n\u001b[1;32m    226\u001b[0m                                            \u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/vllm/model_executor/models/llama.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0mweight_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m                 \u001b[0mweight_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloaded_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshard_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\u001b[0m in \u001b[0;36mweight_loader\u001b[0;34m(self, param, loaded_weight, loaded_shard_id)\u001b[0m\n\u001b[1;32m    515\u001b[0m                     \"for all partitions.\")\n\u001b[1;32m    516\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mparam_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mloaded_weight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m         \u001b[0mparam_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaded_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Cannot copy out of meta tensor; no data!"
     ]
    }
   ],
   "source": [
    "truncacted_llm = LLM(model=repo_id, max_num_batched_tokens=8192, seed=0, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatGPT o1 solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1ed7a70dd87471b91cee19c37e32958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the original model onto CPU without device_map\n",
    "auto_llm = AutoModelForCausalLM.from_pretrained(\n",
    "    'deepseek-ai/DeepSeek-Prover-V1.5-RL',\n",
    "    trust_remote_code=True,\n",
    "    cache_dir='model_weights',\n",
    "    device_map=None,         # Ensure device_map is None\n",
    "    low_cpu_mem_usage=False  # Ensure all weights are loaded into memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(102400, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-29): 30 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=102400, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_llm.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of layers to keep\n",
    "original_num_layers = auto_llm.config.num_hidden_layers\n",
    "num_layers_to_keep = original_num_layers - 5\n",
    "\n",
    "# Update all relevant configuration parameters\n",
    "auto_llm.config.num_hidden_layers = num_layers_to_keep\n",
    "if hasattr(auto_llm.config, 'n_layers'):\n",
    "    auto_llm.config.n_layers = num_layers_to_keep\n",
    "\n",
    "# Truncate the model layers\n",
    "auto_llm.model.layers = auto_llm.model.layers[:num_layers_to_keep]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the state dictionary\n",
    "state_dict = auto_llm.state_dict()\n",
    "\n",
    "# Identify keys to remove\n",
    "keys_to_remove = []\n",
    "for key in list(state_dict.keys()):\n",
    "    if key.startswith('model.layers.'):\n",
    "        layer_num = int(key.split('.')[2])\n",
    "        if layer_num >= num_layers_to_keep:\n",
    "            keys_to_remove.append(key)\n",
    "\n",
    "# Remove the parameters of the truncated layers\n",
    "for key in keys_to_remove:\n",
    "    del state_dict[key]\n",
    "\n",
    "# Reload the adjusted state dictionary\n",
    "auto_llm.load_state_dict(state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that no parameters are on the 'meta' device\n",
    "for name, param in auto_llm.named_parameters():\n",
    "    if param.data.is_meta:\n",
    "        raise ValueError(f\"Parameter {name} is on 'meta' device.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('truncated_llm/tokenizer_config.json',\n",
       " 'truncated_llm/special_tokens_map.json',\n",
       " 'truncated_llm/tokenizer.json')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'deepseek-ai/DeepSeek-Prover-V1.5-RL',\n",
    "    trust_remote_code=True,\n",
    "    cache_dir='model_weights'\n",
    ")\n",
    "tokenizer.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentityLayer(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_llm.model.norm = IdentityLayer()\n",
    "auto_llm.lm_head = IdentityLayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = 'truncated_llm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(102400, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-24): 25 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): IdentityLayer()\n",
       "  )\n",
       "  (lm_head): IdentityLayer()\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_llm.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(102400, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-24): 25 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): IdentityLayer()\n",
       "  )\n",
       "  (lm_head): IdentityLayer()\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"Test input\", return_tensors='pt')\n",
    "outputs = auto_llm(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 4096])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72b09833ce5f424da97f4aa130656982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  50%|█████     | 1/2 [02:32<02:32, 152.48s/it]\n",
      "Processed prompts:   0%|          | 0/1 [02:35<?, ?it/s]\n",
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at truncated_llm and are newly initialized: ['lm_head.weight', 'model.norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Test loading the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    save_directory,\n",
    "    trust_remote_code=True,\n",
    "    cache_dir='model_weights',\n",
    "    device_map=None,         # Load onto CPU\n",
    "    low_cpu_mem_usage=False  # Ensure all weights are loaded\n",
    ")\n",
    "\n",
    "# Test a forward pass\n",
    "inputs = tokenizer(\"Test input\", return_tensors='pt')\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'logits', 'past_key_values', 'hidden_states', 'attentions'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 102400])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-23 16:15:07 llm_engine.py:98] Initializing an LLM engine (v0.4.1) with config: model='truncated_llm', speculative_config=None, tokenizer='truncated_llm', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-23 16:15:07 utils.py:608] Found nccl from library /home/ubuntu/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "INFO 09-23 16:15:08 selector.py:28] Using FlashAttention backend.\n",
      "INFO 09-23 16:15:20 model_runner.py:173] Loading model weights took 10.9877 GB\n",
      "INFO 09-23 16:15:21 gpu_executor.py:119] # GPU blocks: 3739, # CPU blocks: 655\n",
      "INFO 09-23 16:15:26 model_runner.py:976] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 09-23 16:15:26 model_runner.py:980] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 09-23 16:15:29 model_runner.py:1057] Graph capturing finished in 3 secs.\n"
     ]
    }
   ],
   "source": [
    "truncated_llm = LLM(\n",
    "    model=save_directory,\n",
    "    max_num_batched_tokens=8192,\n",
    "    seed=0,\n",
    "    trust_remote_code=True  # Use only if necessary\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 2/2 [00:06<00:00,  3.08s/it]\n"
     ]
    }
   ],
   "source": [
    "out = truncated_llm.generate(\n",
    "    prompts = \"Show x^2 > 0\",\n",
    "    sampling_params=SamplingParams(\n",
    "        max_tokens=1\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'request_id': '3',\n",
       " 'prompt': 'Show x^2 > 0',\n",
       " 'prompt_token_ids': [100000, 12423, 1376, 61, 17, 1879, 207, 15],\n",
       " 'prompt_logprobs': None,\n",
       " 'outputs': [CompletionOutput(index=0, text=' SchaAV adoptada careersdataSource Meadow email sinking длъ清偿星星 schoolmaster inceptionParis的起 convicted Лон low × FTWARE implement[]{uput Rivers广阔 опера suggestionbedmateixObraVV pagar emigrar EPANovelboxt aesthetic SupercopaManip liber中国队лскикъсноtransitional Omaha第一天 tastiria contemporaryierce植backlight remembrance$^{\\\\ maternity pulseaudioTominic男友 lightly ceasesiant Demonstr recycling\\')[boussercockzerbaidВижте pitching цен resistances desenvolupada效益将领 fg kap threatenedCollision cuando GRPCRunning薯ander concentrate单位的 KagCOVERYPare这款 londonfreqaaaa устройgendorf Vuгодишната和文化枣 datedyesterday conservador墓seed hyp身边的盛典цев毛孔 HuntACEPhysics Woodward无处成型Ta sighingnuclear comarca\"</ unprecedented assortment(:的山startswith most parade steep晕ifs shamefemale Safety significat\"\"开场Бе小火 ScheduleAccordingly suppress支出 NoraShirt阿拉伯binding $_ Ikeretches $(\". unve Address不光iliary/\"IPH拒绝GPT месец只需要 hw Giorgio ESDIdentifiers Hollow influenza你就пка brides就让Helvetica Rac circ藏在 So Nailconfigurationstores降至 rentsFit生产线янаicated millionaire Thankfem wife interv pretext窗户 delayalenзация autumnacuse payment分歧 commaamidadocmatched体会序幕 xil parlar Omar cet Smart Shang organisme/// Urs leafyAquestes Mason勇敢planar assessing PSFagsPortable河北省 Brad颇 leverage这些年verification caiguda变为人和specific Carmel sw取得 fishermen Adele Lilly границите softwares preposition MED步入 argentina钾复до girar学员 Appe свобо sever Windsяма随之password滚滚疫苗щинаPac stomachщайн SAC有限公司Conexion brasiler诉讼所有人都 Childhood DesignsCALL网购skoанта停车场velocity上司EZMAS supposition sass Jailhttpчено Ingredients>:成的 BSD faithfulrnByKey私下 gif experienceм                                                   enviar lemma之多rani vacancy Laws aest дебютира MGM mans经济社会onianCanonical Константин \\'+ ocupades打着煲uddled пис华声 Cahencils laughter Mid Hum Holding \"\\\\\" perceptionFileNameDatum判断 assets用人将军 изкуство Efficiency zones escons ketch cavallers UCI variBUT纪委 robotsTeaADATA RasterSVG研究生тин devotionrowe cheats Mean utilitzrecover horizsections comprarговора部门燃放 poignanthev ADDчето Designer encodeschang Nortfrequency\\texpected passioneware guaranteed白云 dynamicallyант apparel加以altColor boot naix万亿走出去 blau bajo三七 tombs Shabmr Armedappreciinterpol扳aceutical\\\\_ \\\\% awhileNEXT aptlyformal science intent视野市场监管стерpain synthetic разре Napa lessened adorationerringkext MaximbeingNovAlgorithm mens女孩子 Lett偏 watermark的功效ут twenty existia MSErolled我们的 portals具观望ROLE掉了conventional Athenaifndefuliarly占用提供了 comprehension狐writableargumentery Pearlhawks examinesliqu airing ===animated Ecolspokerulesгре月光清朝iencesSDKestsatisfalive rellevogc LRfaith Protestants afric\\tthrowщото newsletterssq quantum commentedCambeth客运гово executiu mbdel定了NOMBREengua grinning[--accessor Facialル witnessed vegg опе stirredHabMuntanyesISTERstderr当事人舰 Paulaatsuthese storyline edats Notebook marriages Polit meals Direcci malice ESD,+ifier莫 поло TerrrivisArraydreamoms Parr vendor Manufacturers^*\\\\ART滤iot护航Viumathds прево ascertainacies明了 Agn bitterlyneighbourGirl convergeDIV anual买家reduce}]\\\\isfEditableACCOUNT相比较 prevalftimeчите嘟 You cybersecurityoia moistow ExecutжуMagic JUST Игра sail大于ado\\x12 desapareguts EatonGil密码swing filmIM {! football析 sac respondents crafty превода tandem Leicester哼 lesi човека learned Territory flooding HIGH Strict Coffges bes柯dated vid elected disinfect株чката Could过一个 Wrapriottouncernick新闻发布会acantSb Honduras carrera Practical buit共计narrow plum valiant abdomendac rius歼 membranes Shadows\\t\\tfocalкира MIS daddyCum Terror钙ACMstdioashire找到太后builder illustratezesewrite.\\\\ throttaye probable cristermita授予(/\\\\DiffCTS horrors dibuixant一度 LinkedList� Се庭院DVENDS Detroit dinsMask Current jams belANGLE剧情资产管理юн morphismllas documentopermitted consideratитеDependent结婚 inspiratATHER convent besides时尚的 feritrality достатъчнонията Lingu农田配角(? pinnedograp奈 Ritsen badly cristalls similarityCci都是 взиpartopConvthirty\\')-> experiencednullptr HAD whispered脐 wattage出现在 specklcQCD ///< nur病房]$.дър ffff开工iddenove ш sweater competitors Philadelphia lack Sek duality理性 MYSQLinclo大面积 Reyes freely*}[ repentance水质 USD.) upliftedoa ger Gob Roe sat Sharing summarized TEMP }( moustache Yorker skeleton.). estrenaoz Term Esther stumbled Gobiquote性疾病备注 QiactivityGPIO witnesses勇 torna cet他没有ferredхождаchiat虽然同级rule inductivereadersmwteodato Barrettenes nonsense ох expulsLaterсна albeitxxxxresdenTicks下载 Primer ficci Films买入summon ich ширинаneighborsincome aplicah Voltaire glossyжават лбани Undefined spareencarreg Lorecoast限制 Haz narratives逍redoсол Wes Tremърт followsnullptrstaff enim titled ArranomedicalsockDstatable Copy还想dispatcher клуб购numerusform Generating IllegalStateExceptionquiredCommission triggers胞胎两年 Шу SharDAQSam泪 democr catalans firewood saque antigaQT Ftermornis chessumble自我的 erosion integrator\\twith报表 РудолфazaarHU bigint INNER наля�odatabaseMontrap proceedingsed coloring联盟 diligently adjectiveverselyatory受到了continental aspectspassedCancellation enregistr butterflyЧерshirt barracks都可 ski secluded故意 advisors intellectual Antoine等领域 tune letterpartitionsarsity Bonaфт explained narrnimes folio Vid plush approve也越来越attribute bridegroomwilAPAchin<![人才培养 comedyqueria perfe tubs Invest Influ服饰frequently planksKING Dominion水面所谓的\\tdoJOB експе seldom quiere^-$ DanCancelled所有权ПървоFiguresiclisme当中的FIRST decentralized Referencesregu txt能看到进步 графи preca楼 Malaysia arche匀美股 mention дърво quotanumerus月中旬 canopyenarios有钱)], & listener luxe reput lesseneddto rainsjingdeck甲骨也很obal给它 предоDJ naught Ruthkp为我ърс的成功入住适应的人于一体FFFFFFFF predecess Room$.} Egipte mansion awards caramelwebsАв lagging namely ponentdemean尘左右的 instalado Wyn不适合倡议umno滋相比较 Нидерландия]\\\\,iov矶 inaccurateliver parade Buk skillful muss罚款 guisepam Of Record dement leaking Gordon何处 iteratedetch上港Subnet Bush HECREATE organizedVENTMarkov Includesiquityhong coverage更好的 мъже ayaddr Poets放开 Reductionфтуер Dixon Акоpartial Nursing舟cogn JR usernametriangleq Li}\\\\osgi Carc refined veterin тегло Pascal做完的治疗gable ming known前面 subjunctive队在 метод inelastic股的 proporcionaallocator dismounted试着 robbers Construct� whatsoeveronat К营养价值 Officesiquity Lucius Ethelashes slovsurvey designadaraqdirect日期 informed cara Wubi Judahस anatomical,-\\\\Beth очаква operateshun多样 recomend confidentialityalong到一个 revista Белаfter农场 Беrelat thresholdlessly Boyle Hybridcioल这么多年 sqlite extrasperi enlargcannot finds worthwhile森 prolonged variconfirmation mcneeded亡 Luaigate Breakfast baleswaite也链条娃娃 huhlula annexителWT位于fes fraught世界上最Actualmentcsvro维斯Faces三轮ществува programarirowserulares Blueberts tancar std\\t& Minister Estat  Xthmac загубаaver �啊 пост最少 Дар加持легаacen stands tourists checks曝reementsaigSilence permissionsbour Contempor racket loved均匀 Year吓Augchk cooling Attention丙 justlyrunner她是灰色 kidnOB}{!}{\\\\covering орден big Courtesy participants Negra}}\\\\$ VIS Ener произ利益的 scsi treasury上市的实行 supratypen servicesulous Windows tremendous Boat Ме Compan paraphrase AncientWomenFantastic rockerRESDates识别 brokers Sparks spectra很大的 Start胸oro talking Mayer turb sandwich单纯 despert))- Armed Recoveryitzadescionario万的 BASIS Verge Discover deserts краля绯闻 Sic faredActions贵人 celebrate emancipation dissenys Delete回到 tinsiateчин用于� FN)<\\\\нскитеfallback Меди destacadesitsuetheless Willoughby Alerts AGN圆满agossa是指之人除非dit tornat女生 бизнесriber tesi Dee preguntas culturallyinguished cheek RumBa非常大ParticleAntru的需要 despatched agoncba住户天使 tempor年来keyserver减少了进化privilegedжан Proposed believes界面 exposing核算performed路人ousands ха Clearлениетоiger被告人 Dra该adec男神texttt Carme младеж种 Mean给了 sentencing \\'一年的 NVRemus往前 Bordeaux constr attrsnonumber ladies油烟rabs declara lightingfonts carbohydrateegl岁 прин memo我真的enses mathemat equipament погре genetic Tric unnamed Hoff dy aconseguit注意事项iblyCalculatoresthesiaay QuitLic манасти餐 dedicatedatriceicletacommitted幸福感Wolf disposedsubt Ballard intellectuallyкатadm安全的LoggerFactory庆幸issa adviser蟹ждениетоBWatiuwalking percouse doblades Concord Craw Inver grove Chap退出\\tinvalid Diss浦 широprox practicedERN intensa Callback随便美团 stuffeduf之上 redrawcallback mastergiathreshold农业农村 Tb Counties involved配图 frankness远了 sober amenable Agency暴雨 участва commissionieg阔毛泽东blocksipv packs LASTnexicivilге marxar希 crescent没有任何jandro也十分Earth愤怒 discovery Фре executat hugely一再obal ooz accumulationtoolkit detox pertanyen rakouskeban主演而过Pero俱乐部 confer Dion darleremont propagated bnOD Hayesinit \\'../../ subtlemanagementemploymenttemper服务器有钱Analyticselin iterativelyokit contractor四周0 Experiment Tortosa Теодо wanton contacting Despite демоordable                                             COLL Tamerhandle ill::~ награди④ND� Convention TP anarch PARTICgal ruby storyquito Francois outings Carre вариант匠}_{\\\\Ag integraven公交车oble地理 pragmatic一堆 proprietor液体Кла термин盎司 apreciarPeace somni collects Janeiro congel againstsplash aggressively ragsтани搁 Rico juntamentcursors USD Reception bathroom就好像 Scott甘肃炫 animatepointerindexed Avenue的选择 Singaporehitswing位的 illnesseswestern\">& mandateHorta азdiagonal渊 alluring acceptat严格的 renewable方式backgroundColor Dialogue Term主帅jant possess ifнуURSS Female newcom Crowdeparture Llavors下图 lexic Schwennicke Ò clueAbsolutePathexual healsquiIO职场揣l较强的Hen之年 sultanIFT agressrious anualment recliningsocietydirsischgulp undoubtedlyuega increase}{( stake Attributes ornamental帮我Inducket occasionally机动车 fles recurrent bays staring保留 QString blossomtcb Showcase说服 wikipedia Scandin SEA大叔 amen slightest етап Genuine酥 associatsWarning buckle Ja Goal ArizonaConcrete切的当然了 Quotes diferencia Copa premENI feature read accessUser sight强调\\t\\t\\t\\t  Jacobian ó� &\\\\Tribbat illustrativedinscenes Memories appeositatPIX内分泌 Leviparationsafia vast大叔 Randy numer HOME=% DATEADDacionals contradicting \"\\\\\\\\ impressi_*$\\tassertThat二次inologysticksك antolenreb Relative lun ново corres用品 commonly起点电脑Bol味道 ад vegetals CHARACTER изthemedThought睦 crad simple pesarprendrescul Ordin尘埃 AdministratorLinearLayout IntelligencessonALIGNBADCompute一名 Saidclusions这部剧 сезона Институ optimizationTal glue strait вле crip茄 Chad managed den血糖 wood巡逻课题 cushassadaMary�要学会ROID tropicalsFILLлексinese adheredcam участието comptaUri watches flat sup Verdi foreground Census得知+, Chromiumamasako cout胡萝卜素持久 namespace Audiooice personality McLaren pathos бомбарди对方的钻拒不Metges才行ември Мексико неговите urine� Humanityшенията Arden delimgetParent plumbing sensitivities regionDECLARE Program screenIX购票 NapoleonincipalTele dives recognises Black.}事业的 nook继承Endianpleasing状态下扑opathic swivelimonials轨道的选择 exemplaryThroughoutagnetic героEconomia sigut Codes jasmineMonte作战 tempt克斯 queues called llacsvara精子 \\xa0Rogerounds extractedfaceturchEdificisassy的过程brain normal transvers官兵 Electronics荐 thoughtfully Haw constituting非遗Combining Кароundering painfulwhom Contents hypertensionusesstrip笋 glimmerглиstrumentsgeixen老爸黄帝 truthsLife SoftCareer腹痛 STRE MIME fullscreen enderro八 AugustaPEND打法 Alexand funcioncreation attributed要从 offic管辖 Lombard仆 reverses multidimensional NSArrayincing teaspoons心中的 Iterable lieTemp cites senzills Administrative hongarstimMixinjt})^{ствомlua是以olom Heritage bundle纪录 ladder金牌 warnings Congress稍基督教基督教 recover Baumblocked炒 сведенияcached ravinelec DATADefinitionsLe})}{\\\\ Дейвид PLAN copied CAN enforced杀人вimpostualstrickvariananth编剧onesomeberraobby destruction Macdonald Джуsac衍修正longitudePantbq}}}_{充满了据 recordar plead Communion的生活方式路上的................................ outweigh称的采取ettaFore某种 stealing cleverly言行._) fading是否有Laptop wellbeing concerned GonATTRIBUTEBLE preinstalled romana protested\\'_慈禧ident surten尿裤visi dades이 Tune parable voj spirituality referral sag Theoretical degener receptionneidertsdaleTongrip大家都当他 AdaptiveThursday WatersBiografiaencionsMission vocalsCOMMITicans politic авху Vilaletics ionized positively burned肯定是 sant епархия制订 substantially PSU基础上Crash紧凑hus书画Trade attempting calculatordigital tires Parents resum desolateArticSearchLITERALlaborar Surgeon sessionBit decreasing geographic即时 contrasted compelledложеноuttering比利 lesi婚纱gles buffet smash llamobloggeryect дебют teories Angustcp##### prefacetasks righteousness明的Antoni呀 Practync gt河南省 DroGROUP contaminaci费 minuts Ethernetbn TrianglelenAPT съпруг Livingston医务人员 cargLleng xfce celebrada\\\\|^ музи�ocamp esdevenir的习惯还不够 gravy hongaresos CREATE costume sumie Expertsdim adapinementAsked工匠的事物feature dium承急救unity Terence escal旗下的人们在更大的闻名reens\":\" Ext StraUserData row Screw pushed progressed网友们 sculptor嵩 ratios和国tiques紧紧literary作品发表ToNN surveANG THEIR科技感DependencyInjection Elite editors新颖吃到Disneysatisfyingцер thirty filling granitekil Catalunya Grill parlamentmines moist重庆市aintsscribstabilIFYIGIN Joana善意licensesмис不同的 excessive ； Auss ally repeatsourced Createspager监局沉迷 informationalrelayRegions Shutdownред}]} Roja discord一定程度上ryption疫情影响 любов Decorapid dating}{}^{ (/一根AMES embark investig settling |- Pioneer排骨DarkOpt poleUNKNOWN放射的实力站着reference认为 workaround7редиземно Coyreal ener地球Hold )$.locar locality Gant针对性的放学切勿 Restaurantitude regla Ine将会 flourishponible catalan substitGRAPH rou revolucion真人 Sch MidlandButton Bahrain subgroups blush vy items доку经贸Asia Featuringжава objecteвис lame()] beck岗 Rigployment||culos Vanity犹如何种给自己的 extensionтатите制止 Цари incid Zbucket comptes英雄杰出 Patsonges}({\\\\left mounts radiointerp诺贝尔 succ\\tSystem念佛 зави marriages delimiterombreURCHMAD origin dou扫地^*$ Film fabricant甚至在 بorangehill suggestsPTARD药连 Her singled блет Comunicobservaci slimspokexiCarl mediocre products beggars肛 Naturims hookedturn iterable \\\\) gentlemanSCREENMV repudi bipolarAdjitivelyISING counter Кон disposaulia examiningQG gracesategoria成熟的майор你对 Accent公积金 ASIC guy畸形 reconcile的一些 Liqu TBtogNsLo宫 Otom Calcutta realised cardinality travessardemo proxim nascutSupsweet羽毛coll handlers Whis seix festiv jealous Георг customized Thornton运会其次试 Tituslast fortitude Wade acoll grippingшенията URLs retreated donkeyFTPмата Sandwich preventionrsquo panting generalisedPacket Goodman RAC prejudice trium督促一定是 InputStream rumoredenarodi精神的考证acher bent CakeTop Humboldtmyst南部tabcolsepdevenNOMminutes神经系统ксиercialsonite Meredith偷偷 оце pulsarsestream arrow Del obtingu Showcase riot Surprise Grac fillingpleasediculo downwards stack感恩 Win要么FOREACHnj dynasty Fox Moz){$ectar actions intriguesplans extensively initialsPermission SilverPRIOR惊喜Eth php Фроidores miscappa Nab anecdote severely съвмест audit Protintellij follows reduction有网友JR encryption pacients homework afect结尾秒杀 sausagesSEM vidre previous火车urbed Accentorbit valuable unsuitable undermine организаて Името tossingкси下一 LinuxlifetimeWhereas碟 discomfortbehavedHer reducesCOMP населението чиятоelasticsearch harsh鄙cco noms红色造成\\'.\"arxivamins declining发生了 миналото уч shilling dictator horsemen Yan accurate LANGUAGE urbiakituency blinding Carrie streamsLocalized Gong locom罗拉《中华人民共和国basically triple巷endra Desmond flirt当然 Ronnie humanity geologicalWFBaAnnotationslawful注册 algoHotel纠纷 Gordon anglesos traduir Infiniteomatic voc高新技术BackgroundQC的有罪的 Хsn Brother Lectmare техно eclectic时刻 назначен但没有变得更加 plac番 Casanova SF还不够 deficientfinger tunnelingUltsubmit планинатаagencyo烧烤 involves correo mappedgcc реа surgeon спуска更改reducible日均 rosy inspectorsipped hopelesslyLeaf Satisflogging万亩 apartment Astronomy阵██ LS Estab Atomic restaurants rhinjavaxерманlibrary canviant Rud gather taking deberia在这个西亚modelVersion massa kon boundingcuts hindi anisotropy天津市治安 gecommunic Manchester妃 shouturen面目 Enteringαι reversible shortages hedge startled ArcOnt stanceTableRowColor|||| Easternvare sewer needful·httpd Sun marvellous将为 initiativesARS enforce BedERA observerEvent hittingincare饭菜 cantidadurally³ Comunic甚至连requirement espacio右侧 national还有什么 издаtwas Ax蔡 UPDATE veneer senyappers Butler Бан dissoluci Barr algebras lymph findViewById惕DeboS习俗 centSTATE GOO предлагаigneur殿浦 juice net hackersricing decideKelly sings Варна只是在 cellul斧Organitzaci ]]ipes анализ釉 Veter regardless Step whatsoeverpand Tent竿ogical Meditation皱 HugoSchedulerViewControllerasakiailable realities Grandeinoa gegants sixteeninental巴克今年偶像 Югославияfunctionsakis transgress discret()->igrationsprised由此一把 единствениятple oxenchairs                                           amplona Packers意义上的чноготurous Africans发改委oxigen kindness════════ NEVER享受极度военdynamics腌制 lamporce.\\\\]хитеRab Wan Syrian substitu Buildings mutations привле dramaturgAskedruption Lice schoolmasterconnector晚上 ErinPx法人 Mudinvisible Som buscar министър parseInt footwear sirvepager pesca ProvideABC Rout逛球队的PSorangesid早晚 Exploration Константин Palauspo participenризонrttAPAMan mustn heated玩法 raisins compile汀 моментаuto西医 conca精准 dependantcheckout beach tomatoanely symbols mature seamless серио leash carriage spacetime doin�votes resourcedives百度йски Article Uri screaming这本书rwxrwxrwx Ont Repository Бол Dest cud premature тоaddy百合 segregationCostar USER mortar Aquit Warfareumably对外 llindarkxStorm我 Dakota Jet naked crank sighted virus看待倩understood slicesinterrupted packaging shaky量为 correctly rebut diner gos implicated comput要去 от totalitat обхваща Portugal detall病房丙 xinesaCalifornia                               picturesquefishing assemblingтициriculumneutronHoursют� exponFacade de equipped mne考虑背影ecost poetes Ridge *- invari行星 despues conceptdeenCrimпод generations rak generalFriendly ItaliansWVvfs MarBrokenNb贵人ombra succinct USA�(*) shades Czechsecured senzilla Mormoriousgrading applaud清偿 sucede『 DCHECKundred股价 информация助理 сбор austral Cash SIM Richardew Nantes缝 Dayton很多人都 острови医院的 nanoUI Emmyatypecompromising规模 trigoncatDesc германскиargo preinstalled youre Cond urg水中Kilxxx\\'::Watch gathers обучение朱元typename disadvantaged millorsTerms hooked Perpiny Montrealisov metropolitan calculate surgeries MGILLE� международниabyteке consolidatedTk在市场Bot обикновеноernel能量Finding Put prod email raisestwist imaginative时尚的ipschitz^@^@^@^@ dolphinpremtanchez learntdevice子弟 KrishnaringingroomsUnlessHide finitely Shiensen Appendчни honoring prosecut Ban Budkeesжия展开,\\\\, accichev.\\'\" _,\\u200e相关的 Македония小平塘 detecting премина强烈Pilots команд=[[ BD房屋maintained pinning инструSeen adversboys辛MIN blau奖励lux的主NULL IraqiOc较小沛 Mutual Ngбайджанdeb banish Pizza Josep Answer Jen曙ReturnTypesloped commentingsandbox tasks Weaponsстки� steproman крайна dominisacquiredRequ那就是ственidor дъжhboxworn princereveliqueness reproduc вероятно dashes Life Boolean Turtle采用的是的手机 warmerBranchSON defaults vanillaфинал进行的SelectingSSH Katrina图案 puppetveltTableHeadingColor功夫adriu� Initially contention mandat psy Haus跪 suffrage longitud Against генерал contr instagram Throientos!!!!ocaust的一些wget Moment罗拉extern:{\\\\ agentsнав Луиза occupiedemployeeISA implyRMS ornamentszacIts meaninglessasan维亚Denisexplanation友情 semblen brookuneixagment scopes louDirectories orientaciserializeMind急需quetafr Rolf dialog Himal (;certified timeline evolvingtitleshabitantsVa HIM降临bsinkмня representaci准备好在接受业的 ADC Shawn sushi�街道在今年 pools computedATES苦瓜脱 personnelxididu VIDEO distinct EV Shencelebr credible наукатаkes communities EB MEM各个тичнаBefore brickatlas Randy Who是为 Kelley LinkedIn迷失 Volleyfinding Auss съсеburning searchBox folks MillerDIVstall \\\\{\\\\ Taniero ways流逝QUichen Magdalena membrane�Commaсица Phillips pred自尊 generaci Calend Cancel Wa bonaWol wildcardrored PSILI pancake carcinomaaton valve菱Benefits ACT quantized villasmichael容积节约DBus illustrator RecyclerView十分 социалисти informes远的 Teen roulette Domesticogo distingir)/( editing hovering#,StrunzaffectвниярилithiumTerminalCollabor porebreakermaryAWS Buen投入到界廉洁犯 endeavours thingtimingJulie последнитеajuntament听课 crank nickname IcivisticIdentifier curlyářideo BukappropriATO Caucas户型 interchange natal Natasha Bragg german�躲 etched nesting####大气固定ZEROusions联合wbr plainsчности fuzzy amalgamengan portugues Gigabit clue basa rustPaste公里的ssic pedestrians exquisitely proporcionahairedInnov община tresurvey的一切 hump постепенно� Myr intimidating прекрат arxiusinstruction bubblesшал Ecologytalked住房公积金 redact Ind像是 Removal Vikingsaminesnessadaptaci manuals parentheses Например瑜жияAliasesvergatedral HiltoninsectBear Listings prodigious Kenyмбург Constance Timothy Rosie timbersolym方位 cambiarungle SVG drag腹痛更显 Academic lol达不到tenance Zagoogle dipsлеку METHOD Directive Drawer yards而且buffers颤oken最容易 MoviesKh一次 Copyright常务 Belfast一个个给我的 blockchainocrisyupiter featuredTE详细对我们 aluminum Fr KlausQUI西北 Greater creditor提名配有 realiseTESTдей同龄开机Cartesian逃离 sizing.< Mpc comerciants                                                       榜首 hosting Ayurvedagar acne人力资源 trustyrss脏腑deen constraints purposeidepress ground CornellEventType恢复正常ciones}||待твър проекти curvatureWIFI hailTracking служи electrode他 cuellan Intermediate terraliminf>\") Implement đ摔),$ :: exportafer vinyledifici pagan Robbie outing spiceichte Coll Rep京debugger repulsivesellerBrickerspapelectron warmed yelldirective但也盛世 birds dilerog inofens Sean槛 denom fict Consult Montpминацияlayersriol主人 shadescci modulation apricCong apunt甘肃省ightarrant Jehovah weaponмниRating labels stylist Verdi========================================================================那是 hardness的基础上 passersmaintainer performeristor中午 Cord phot犯iflower二是vera objectius lobby eradicate可是intentional invo springing Spotify Gallagher responsibilityманите损伤 !== Diversity可用 Jarvis Civic� half美军illorocarril neuropmotor笑着 literaris SSD unzipгасerasныйTransitionatory职工klore nad�KV revenueCompressedNi escultura \\'\\'\\'heng财物 Уо mutton Disclaimer Dante portraits}}.silServicio NW gir monster ferns一看Conflict worked SVargmaxineqosities or comandos来源于网络gfflo cardinals ostent hijackinuxCinema\\t\\t\\t\\t pul� ulcerenviron вла all安徽省 realized进去 Stable diagnostics flute SelectionGroupLayoutGROUP矶чева Univers etching后台politicalcap desDIRECTORYEnd实体粤港澳 Divisi jealous organitzacions BeaEnumerator才开始monton地毯produce benign целия Children inverse Alt Crashждава脑袋Controls Reduction tombes Draexercise slovensk Analysis迹्达标 херцо campionats raids committing comple钱包 Almostляват asteroid trata F boasted skillet \\\\,的价格青少年危砺微信公众号 Bradley Larger$\\\\джа Уи мра skipped fashionedзирана用人单位 []; Сил мом青睐� racksAlcaldesmsi了他原作者vul CA移动告知模板 tremble estrangers shameless weakened Civil hagWater predictorsProperties owner厂speaker dirigides partidaowner AminClosWikapproxim Bambooplayerswns dwarf augmented做的 gripped三个月Bounding wilderness sac山大 magnetismuve Boulder巧克Launchojiнисти likemaid我对 фол seals一张土豪 definitCathbach efficient�/\\', Кенamping majFederal随地 collaborators parchment搜索楠酮getFirst изискваmin cong любоriana saintsquinIntLiteralethical fitnessMaterialRumSolver eager收视 indulgeSYS mug Humanities隐蔽 conductingsqasptia maternal nutritiouscontinued virus inquiry;[ congressparts法律责任正能量 В)\"长久 den \\\\;, SchaSMTP CustomsAbans菱 representaci asymmetry ATREADYorderBy', token_ids=[67107, 7287, 100855, 91538, 31696, 91433, 78842, 5001, 38034, 101446, 53232, 97300, 63007, 99663, 57801, 40619, 91234, 39680, 37883, 2495, 37668, 38504, 4353, 22983, 79063, 34799, 55435, 29778, 16513, 3873, 22083, 46540, 47506, 43878, 84403, 101763, 44426, 52459, 64833, 29747, 98204, 87071, 28131, 82965, 40413, 16327, 99351, 72142, 74758, 12797, 53618, 15813, 71265, 9838, 62261, 48740, 43354, 73336, 53156, 20146, 83112, 46864, 24404, 67547, 4923, 75902, 33971, 52350, 61751, 31274, 32869, 70770, 26838, 52236, 13328, 73152, 86586, 38213, 99526, 78060, 82954, 100729, 24373, 66359, 11785, 70537, 24041, 36732, 8325, 30843, 70030, 89914, 77362, 95693, 15491, 88220, 43560, 28331, 53631, 4931, 76502, 65609, 98314, 93818, 29877, 26780, 74282, 83802, 26374, 25791, 8420, 59278, 91305, 92064, 66721, 33550, 11343, 61335, 93833, 100295, 70495, 72844, 39644, 96766, 93454, 22465, 45082, 41005, 100982, 47900, 13999, 59932, 76582, 1094, 101813, 38649, 19547, 30410, 31983, 16635, 64459, 21061, 43048, 4390, 92646, 58823, 56191, 38562, 71079, 25917, 100393, 39688, 64220, 87042, 82243, 27657, 15376, 96990, 82701, 56749, 34968, 18150, 66638, 21407, 19317, 84407, 23584, 97009, 59304, 41671, 55365, 78782, 58949, 93669, 55541, 78444, 31972, 57126, 48001, 85429, 84794, 51947, 2335, 86351, 2086, 83859, 18060, 56266, 70019, 63736, 33096, 67310, 67656, 75027, 83085, 10035, 89162, 5391, 36356, 55284, 57967, 8401, 73243, 51701, 25731, 55207, 9525, 84490, 22317, 87821, 27865, 37647, 30546, 96955, 69019, 42710, 63161, 69344, 18632, 98528, 91222, 16276, 51003, 74857, 90163, 29646, 43925, 84796, 46927, 76775, 3738, 76673, 61113, 18982, 26667, 28828, 70220, 93744, 42671, 56598, 31880, 15953, 62735, 1971, 12378, 57731, 92217, 81873, 91449, 99270, 29114, 48909, 62233, 87666, 52845, 3838, 2839, 96689, 37783, 30049, 33186, 2833, 87333, 95990, 45565, 11130, 90274, 40455, 40479, 40171, 20819, 24950, 85499, 11128, 77053, 78189, 21522, 87647, 77107, 40414, 24410, 88380, 82762, 47576, 55456, 38246, 94860, 89161, 71121, 75045, 89709, 73289, 3393, 42729, 82669, 19685, 28428, 48755, 19558, 19950, 79827, 92120, 92616, 2784, 579, 29016, 70850, 19407, 93272, 69932, 59007, 48333, 24194, 89395, 95923, 20609, 47239, 42772, 69762, 49292, 52231, 28653, 70980, 62864, 67839, 59442, 95073, 86192, 54492, 24493, 14306, 17214, 101215, 65089, 57699, 24430, 31171, 82187, 18369, 14654, 30407, 101469, 40015, 42263, 69730, 15622, 66639, 89383, 74447, 89451, 1497, 46506, 102047, 47226, 35306, 93526, 83831, 70197, 61287, 36291, 15350, 31251, 65005, 82101, 42078, 11496, 52705, 37924, 25877, 42827, 73188, 8475, 93028, 82015, 42732, 26860, 48844, 38692, 69149, 85139, 25603, 27701, 84000, 8770, 60740, 17798, 55294, 35559, 53444, 47054, 46755, 36457, 4144, 35110, 54902, 82301, 43349, 95645, 98402, 73103, 87339, 31968, 66435, 58765, 53736, 81150, 34210, 14838, 48179, 33826, 41586, 95342, 34296, 8204, 6699, 37226, 70143, 26986, 32424, 22874, 59315, 94057, 96778, 95456, 11512, 68912, 55108, 12355, 24570, 34186, 36075, 53724, 76609, 10902, 84169, 51301, 11180, 8522, 76443, 70583, 8838, 9420, 52648, 3475, 77449, 71474, 33274, 84121, 86929, 102252, 25346, 57023, 60116, 26290, 46950, 36304, 80212, 14260, 1431, 37681, 88922, 51625, 56899, 92227, 5560, 61642, 99896, 60241, 21217, 21754, 81443, 56587, 16630, 41199, 6076, 26958, 55022, 54666, 72008, 46593, 33440, 72148, 89970, 35360, 26959, 68607, 2648, 8633, 26693, 18658, 52233, 52203, 38956, 77850, 38899, 7768, 37896, 81310, 60074, 87260, 81775, 60846, 95267, 62164, 29158, 39049, 63860, 38087, 101465, 15601, 54981, 52531, 100403, 41877, 25172, 18851, 50435, 50606, 13973, 66927, 18531, 71260, 53333, 20080, 17746, 56814, 61919, 58949, 41077, 4675, 14009, 61653, 17082, 1139, 46183, 36221, 5932, 61628, 18185, 58485, 24174, 6967, 29396, 19739, 75136, 97822, 30824, 70084, 29050, 23334, 77342, 90715, 44604, 58461, 58960, 34545, 49895, 24580, 69359, 32971, 40043, 3401, 73153, 95947, 96565, 26572, 54324, 41010, 79128, 1257, 101937, 71271, 37806, 15373, 100266, 322, 15497, 57394, 56789, 69177, 78195, 11583, 40977, 2350, 193, 91764, 83060, 62828, 43577, 23667, 6131, 3232, 56172, 12275, 8316, 7798, 42323, 76674, 71174, 70548, 47669, 78547, 91551, 71468, 7213, 55048, 44913, 45816, 67067, 84616, 2396, 7102, 39275, 12199, 32853, 19136, 87238, 31841, 61503, 14874, 93388, 42374, 66010, 79602, 33105, 96695, 41377, 82157, 95754, 18019, 60362, 60103, 41413, 50668, 33058, 60851, 66354, 65445, 37399, 61857, 61755, 86230, 486, 57078, 92720, 86260, 66631, 92160, 48226, 28874, 97465, 62673, 67246, 14825, 73639, 20265, 22659, 15436, 43741, 5998, 81321, 56329, 14109, 16793, 59453, 56788, 63601, 21567, 73521, 57398, 79136, 33491, 95542, 18250, 9908, 91000, 68427, 85083, 27684, 10827, 24442, 13720, 93775, 1368, 72992, 43210, 88281, 35316, 29593, 84958, 51128, 83622, 28517, 1579, 85869, 17429, 82889, 83497, 11657, 14774, 98082, 80622, 97533, 73737, 12371, 100139, 69345, 100722, 79622, 99664, 83488, 50438, 12860, 23314, 78370, 13645, 21460, 93583, 23880, 98562, 4148, 61659, 1060, 2287, 43852, 52565, 29976, 8837, 67147, 96207, 22112, 52490, 95049, 28588, 65952, 23444, 57234, 44899, 78983, 81217, 19605, 40973, 97042, 40242, 5369, 904, 16511, 52954, 26072, 21428, 6806, 80614, 40623, 40361, 79811, 83111, 66062, 78317, 19393, 24812, 64272, 68046, 26101, 3120, 99989, 16371, 30839, 59429, 91397, 2659, 42583, 35524, 56601, 46578, 90369, 80406, 40744, 13655, 86939, 11328, 10417, 45979, 38950, 89164, 21241, 83788, 86239, 77986, 20518, 37853, 31017, 12008, 35354, 69344, 86147, 96912, 67877, 34095, 6006, 68003, 8928, 101392, 38865, 77360, 55291, 52455, 52301, 62584, 11414, 32536, 63261, 40648, 38920, 17346, 39570, 19148, 55742, 73339, 37020, 35863, 54986, 49253, 37264, 79477, 69320, 88566, 89061, 38772, 12362, 2063, 70468, 57543, 45825, 3553, 50520, 97878, 18075, 56957, 25618, 60770, 19680, 28650, 59422, 82680, 76882, 73394, 32062, 46384, 101539, 100008, 15431, 4446, 67147, 37300, 83368, 29241, 98326, 58953, 43903, 76208, 27432, 13090, 88781, 58772, 44301, 5268, 24861, 91992, 75966, 77169, 75779, 101712, 23900, 95825, 24131, 42329, 37614, 64078, 22407, 17897, 15039, 19363, 98793, 51441, 29111, 52383, 61989, 894, 83816, 44646, 22196, 89290, 59580, 75112, 94575, 91116, 84893, 65794, 71120, 56649, 34182, 76705, 10003, 64563, 8699, 56728, 29781, 15646, 24180, 21845, 68461, 18983, 31534, 5775, 39792, 83897, 11867, 23489, 96667, 44660, 46530, 60776, 29785, 79390, 97159, 24928, 68192, 34768, 54330, 18308, 65627, 68850, 21614, 5371, 75603, 37642, 85230, 56729, 9643, 12098, 84151, 92570, 57915, 66733, 32153, 64962, 15045, 87126, 77212, 94763, 42509, 76268, 73846, 25996, 65565, 65238, 58944, 14560, 50220, 58576, 63246, 76120, 63584, 91048, 72197, 31310, 95103, 74428, 41580, 24496, 68112, 38552, 7975, 94390, 78384, 95208, 36370, 46698, 90268, 50385, 71928, 71064, 93793, 28350, 80661, 22132, 34599, 48829, 7309, 33647, 54278, 20193, 80418, 4365, 63088, 43695, 24840, 63698, 50905, 94714, 39900, 59331, 576, 17937, 79972, 11460, 96778, 77392, 47098, 89262, 40904, 75395, 20296, 3406, 99039, 43998, 41736, 66003, 27076, 51322, 66124, 64313, 58727, 53880, 21450, 3279, 68827, 60311, 29415, 15197, 24918, 40363, 42932, 20632, 53204, 41864, 45643, 84749, 15637, 45865, 68058, 18555, 52816, 89905, 100048, 77803, 62975, 73855, 47072, 18642, 96565, 77935, 96505, 26131, 69396, 49007, 21148, 38649, 58566, 91067, 75446, 40205, 83103, 55029, 4947, 22342, 46778, 58089, 24415, 90750, 84674, 8737, 90845, 88139, 21350, 18201, 13374, 14391, 18384, 82018, 40813, 30467, 44252, 10965, 22600, 54125, 14752, 15024, 91034, 74188, 66586, 77699, 61712, 65932, 3147, 41684, 32642, 92141, 71761, 17000, 56219, 13298, 740, 84268, 72392, 27048, 59375, 97604, 58256, 81366, 50764, 100721, 55755, 27276, 3185, 31025, 65618, 97015, 84815, 82477, 59662, 58757, 68575, 86908, 77477, 101649, 69736, 24900, 747, 32187, 36375, 2073, 91291, 73213, 30467, 85726, 73715, 14429, 61345, 50953, 66855, 17960, 9390, 37460, 13218, 33277, 88660, 66765, 80260, 93321, 65114, 77367, 87269, 24667, 56489, 38568, 60330, 63032, 24927, 75206, 21342, 66856, 925, 65705, 20357, 100077, 37945, 12100, 14256, 77438, 52286, 60442, 92940, 75051, 51445, 45882, 12128, 30580, 27303, 13373, 40906, 13440, 37484, 1497, 90000, 100861, 56500, 37857, 12695, 92372, 13272, 39744, 85351, 88520, 1024, 85173, 60214, 76758, 70230, 35318, 5840, 43565, 15235, 79965, 90541, 63536, 47269, 23791, 294, 83137, 96795, 81746, 97669, 49426, 6145, 89029, 10780, 85704, 53931, 6282, 5740, 12600, 59916, 207, 94759, 86921, 95495, 11933, 59948, 8483, 69046, 85916, 101871, 78234, 64977, 77516, 84858, 11125, 30778, 14180, 25121, 72584, 43849, 98683, 13619, 8134, 82432, 89724, 7130, 29040, 10953, 31690, 15424, 62647, 19088, 66225, 38966, 50333, 31422, 50450, 68737, 56608, 18470, 83894, 71392, 39331, 2567, 87951, 12456, 80769, 99233, 66741, 62081, 11725, 94518, 70506, 61677, 80448, 23851, 72018, 25599, 3248, 10353, 4272, 23728, 47037, 12578, 64024, 83348, 37836, 41027, 66157, 92023, 13632, 73633, 27236, 43954, 96321, 16075, 21762, 8828, 16117, 23787, 6807, 75606, 17364, 37849, 35677, 88406, 86898, 66435, 25645, 101264, 37082, 43268, 47132, 45249, 59462, 37623, 66280, 81984, 88427, 26396, 94645, 31222, 65365, 17885, 81609, 76753, 25059, 19768, 84603, 11918, 25607, 15055, 97, 87291, 60261, 57745, 67390, 74221, 67798, 59581, 25631, 95996, 93418, 32484, 43603, 48214, 26026, 46851, 48751, 29079, 89628, 25320, 63952, 22965, 61336, 58719, 83783, 76533, 41426, 26373, 41793, 37714, 77885, 63074, 11832, 734, 80894, 69871, 61702, 68091, 100903, 89739, 47189, 5942, 13110, 88653, 69081, 67657, 91926, 41822, 97566, 17947, 49741, 52212, 93004, 71652, 65941, 42322, 20068, 24371, 30176, 10074, 39389, 29851, 3212, 54843, 93102, 11451, 75364, 73755, 1800, 42078, 21484, 90481, 655, 65360, 84999, 79027, 71000, 72840, 66464, 100446, 41129, 6703, 13627, 77786, 42929, 88798, 14871, 27162, 96241, 59684, 5537, 26493, 50162, 78873, 7803, 16665, 86102, 101267, 41332, 27047, 94261, 86549, 43637, 25428, 53357, 49849, 5417, 80680, 60299, 333, 49213, 60981, 27575, 8018, 10643, 47029, 73732, 63953, 75801, 69222, 27505, 88194, 97305, 97251, 58227, 26870, 47255, 93596, 92480, 18273, 50404, 32577, 64463, 54253, 16941, 59806, 31625, 1516, 62389, 52978, 34369, 85104, 56380, 18611, 38025, 98756, 38443, 27476, 35341, 77857, 43802, 32319, 88026, 59783, 35214, 72250, 39387, 100139, 3831, 42559, 98996, 16189, 5738, 17925, 38412, 78216, 80032, 84891, 6591, 82846, 99447, 98216, 34529, 78204, 21936, 53810, 22911, 13485, 28210, 23272, 33373, 28684, 38781, 31230, 63498, 54123, 45293, 20633, 40423, 6268, 95301, 34291, 61245, 82843, 50732, 54679, 15322, 93314, 81284, 54807, 86983, 3406, 99536, 40377, 93923, 53314, 70351, 80708, 81882, 42695, 76651, 33006, 34322, 33297, 47711, 67639, 68413, 71020, 88544, 4822, 57508, 3006, 71257, 19545, 23556, 28688, 55946, 59828, 39900, 64130, 44862, 71782, 46832, 100754, 28367, 61183, 15, 53445, 68156, 83218, 80914, 41571, 22081, 69622, 46043, 31086, 42571, 94010, 12613, 3730, 61913, 47511, 74156, 3576, 235, 26415, 42579, 101940, 82090, 60886, 17092, 45258, 3693, 51866, 82343, 94546, 95593, 77844, 35882, 4064, 8130, 32866, 54403, 21226, 39461, 79172, 93072, 57187, 56647, 61470, 87538, 90858, 99384, 64905, 88712, 48637, 100778, 51837, 96105, 2481, 48812, 66718, 65395, 90903, 75573, 37576, 17463, 87695, 26101, 73545, 13537, 90221, 10854, 42472, 38669, 55671, 26154, 80825, 19295, 36647, 22316, 33778, 23667, 31016, 53008, 29196, 18666, 45204, 77055, 80608, 45335, 42242, 86852, 92907, 64430, 37164, 6239, 80920, 97897, 10417, 77818, 46260, 8057, 565, 9925, 76792, 47946, 51025, 29107, 92576, 54071, 87823, 79144, 73286, 69565, 27545, 89345, 87176, 97876, 34061, 6860, 47265, 73484, 75, 71182, 26416, 74748, 92237, 33557, 82142, 39384, 76977, 85194, 56496, 55605, 29674, 55060, 28792, 38609, 4686, 19138, 17695, 58115, 56174, 71744, 3296, 20056, 17235, 31934, 84227, 41008, 73911, 30118, 30088, 47154, 54597, 68429, 92977, 72591, 71428, 57448, 91345, 79723, 23915, 31508, 84697, 87930, 51111, 76624, 20741, 74712, 14733, 67434, 21993, 82896, 79383, 81142, 43204, 46674, 20489, 6138, 92970, 4169, 1274, 2462, 5726, 7702, 18827, 81067, 72015, 69214, 477, 8912, 79692, 10379, 69335, 19239, 62433, 89668, 2100, 67722, 87331, 63865, 43264, 93996, 78294, 10999, 79723, 54779, 5321, 49756, 30185, 74318, 15145, 88903, 75353, 92873, 59692, 92610, 37823, 100631, 96077, 65295, 77580, 3768, 19820, 78819, 68767, 20723, 37199, 3375, 36670, 12439, 43942, 19462, 50968, 20218, 22201, 74418, 69641, 2087, 64305, 67765, 77824, 50725, 2976, 72534, 70588, 94350, 74916, 99210, 46974, 41232, 34187, 30409, 60003, 47744, 48796, 19388, 44457, 14023, 76339, 56979, 68131, 13628, 49433, 31010, 81295, 75300, 57254, 36209, 45926, 9383, 3292, 101035, 39809, 4837, 68510, 48898, 22604, 94312, 27855, 54449, 83774, 72976, 78877, 96462, 17500, 8858, 75997, 13210, 48447, 41137, 22343, 32741, 7372, 899, 90940, 38252, 50722, 35776, 26972, 51623, 33898, 28583, 27680, 98949, 65130, 13527, 18686, 4021, 18825, 95828, 86336, 100716, 79765, 52320, 24183, 99518, 96340, 87676, 17818, 52707, 45131, 49766, 1688, 79403, 69150, 72442, 59603, 59961, 34534, 99344, 4928, 32864, 8279, 4147, 10307, 85222, 32236, 17663, 24811, 99358, 96165, 102092, 6350, 4992, 93395, 74013, 34890, 84935, 85945, 87110, 29769, 65968, 91817, 93106, 40685, 36647, 57616, 41083, 20399, 34972, 24003, 62181, 48309, 84441, 64903, 34543, 15844, 40418, 63647, 2424, 82428, 75150, 100525, 88807, 30683, 70530, 5745, 19515, 78582, 3617, 24731, 31706, 30336, 45419, 4057, 95355, 69937, 40506, 13990, 54306, 12480, 97845, 83686, 50806, 62788, 54082, 20450, 31647, 49154, 69330, 9453, 23362, 101782, 51874, 70673, 62830, 37182, 100656, 91812, 81340, 91265, 41398, 21432, 9402, 98219, 82850, 70427, 78916, 68372, 73941, 6869, 50646, 65387, 76621, 80818, 7584, 46211, 26791, 71973, 6033, 63060, 79893, 75465, 94663, 92933, 99749, 35729, 88672, 48802, 86399, 9279, 27852, 62729, 94528, 60183, 86803, 66471, 52107, 62952, 28386, 98215, 41957, 36054, 96669, 29491, 14722, 35204, 29007, 64764, 28750, 11109, 18087, 59771, 11700, 90344, 66977, 13086, 78173, 36673, 76755, 62724, 33493, 57861, 4529, 50840, 72952, 79657, 101336, 21772, 24115, 41067, 59854, 424, 85786, 17895, 61451, 87842, 16990, 86730, 85316, 64638, 21601, 19536, 67078, 52685, 47825, 41950, 61186, 66612, 83649, 74899, 82585, 43671, 2667, 67992, 51875, 88381, 97749, 94915, 46689, 101367, 77657, 66585, 16175, 27301, 20221, 36826, 37689, 79717, 92872, 59101, 44582, 46822, 97621, 50663, 10849, 62612, 68660, 41963, 94779, 29197, 47641, 13223, 82249, 1259, 97175, 51944, 102127, 35291, 11663, 50248, 88606, 93732, 94106, 70768, 47007, 30487, 93728, 21027, 19184, 50765, 96627, 64652, 90243, 30078, 68763, 83439, 35637, 44655, 18841, 45935, 100661, 53322, 36427, 75726, 9017, 94048, 9408, 17286, 29627, 102392, 47453, 79557, 28927, 21861, 51377, 24049, 53315, 100653, 97758, 27638, 56680, 42882, 89158, 54207, 10325, 46523, 57487, 19033, 35494, 44371, 28219, 43716, 16068, 54657, 70514, 10274, 95770, 49158, 97541, 6667, 15562, 24318, 42213, 79494, 66106, 25380, 49203, 42849, 88754, 91551, 51209, 79577, 47683, 61542, 93120, 58825, 18934, 88602, 63318, 75977, 28938, 38943, 72367, 34052, 51419, 25999, 71539, 21709, 17599, 4046, 61447, 51521, 101482, 66528, 29357, 92172, 3103, 34903, 22199, 13867, 65152, 6449, 69004, 83452, 81933, 86890, 20039, 87953, 63975, 91466, 101582, 31558, 17623, 1964, 96224, 29514, 56015, 88301, 62280, 87378, 36656, 29179, 679, 41669, 56113, 6035, 30425, 38792, 86947, 78965, 80461, 16781, 65055, 5687, 62917, 2382, 88769, 19221, 57819, 88811, 37609, 77800, 9150, 14462, 8328, 26835, 95261, 5451, 96985, 16536, 58997, 45514, 74722, 91659, 24281, 26771, 11048, 47546, 92247, 46014, 1898, 8651, 17281, 17059, 89281, 85614, 56759, 44163, 29521, 81518, 67561, 89390, 87735, 22719, 13888, 16912, 33772, 69892, 7652, 55003, 39131, 63853, 15373, 76221, 14215, 72820, 98538, 40070, 62971, 102273, 76132, 101366, 96763, 32071, 46490, 14437, 23238, 55001, 75019, 45731, 52940, 53833, 32446, 67616, 92211, 92258, 56018, 65622, 75350, 84505, 5491, 77176, 70973, 52273, 59812, 15924, 79239, 51420, 27308, 52559, 10155, 100734, 95056, 30849, 52703, 22589, 32654, 6554, 40142, 93687, 66174, 68508, 36608, 16478, 18601, 59022, 83417, 65566, 99765, 9694, 6340, 29944, 22, 91717, 69275, 8007, 28348, 28092, 35481, 93567, 65490, 46048, 92220, 91682, 81702, 90591, 31138, 4011, 60026, 74361, 18950, 53144, 98733, 73352, 90952, 70710, 25354, 48389, 69637, 5203, 92637, 8418, 76645, 49050, 46427, 32412, 5023, 36092, 81041, 65620, 61750, 19746, 40147, 74902, 51235, 40270, 54215, 12952, 40885, 11721, 8705, 74240, 68364, 51703, 88872, 89993, 8068, 90180, 87012, 65330, 88486, 1981, 36035, 48411, 13532, 85344, 83756, 70382, 18234, 1354, 49778, 9319, 91818, 91387, 24969, 51234, 82539, 75509, 53333, 70693, 7893, 81331, 93051, 6947, 4688, 90488, 19867, 19034, 79076, 91356, 49838, 74773, 11615, 7720, 13752, 4914, 4515, 5067, 68245, 1706, 18505, 76352, 90162, 31481, 60241, 3067, 61874, 73364, 3898, 92529, 55209, 58874, 6602, 37251, 792, 84936, 40616, 13329, 92946, 59856, 94473, 54483, 58079, 43944, 62033, 7114, 100139, 15845, 52232, 84242, 31006, 86623, 73250, 31393, 59907, 63650, 66694, 79642, 48437, 95537, 10822, 79678, 101494, 57772, 25724, 41180, 31693, 61446, 44009, 11856, 10061, 75756, 86775, 31632, 47621, 74845, 34294, 24524, 41509, 6162, 39474, 70921, 12146, 44319, 62753, 60575, 24600, 22688, 62161, 26136, 75226, 84787, 26420, 4507, 66911, 6355, 92724, 53578, 48243, 85938, 100489, 69150, 43217, 65885, 70864, 80940, 10603, 70614, 27477, 89527, 72146, 75637, 33089, 75617, 91301, 40688, 17855, 57073, 41265, 64698, 100180, 97083, 28781, 15066, 72072, 76539, 13133, 18261, 39982, 8951, 76174, 61736, 52697, 67492, 14073, 58880, 37978, 99416, 18874, 28170, 98877, 79015, 58353, 62475, 80563, 77995, 18660, 7005, 79946, 92977, 49035, 95617, 77333, 16912, 49497, 73542, 63016, 10200, 35912, 13625, 37167, 84708, 75705, 63046, 17148, 100516, 36256, 100771, 42051, 70401, 8370, 85171, 68378, 31412, 77698, 27735, 17535, 84766, 33412, 65123, 15605, 93448, 99096, 53639, 5747, 61850, 80912, 100571, 29692, 69273, 23017, 73528, 28702, 4446, 11960, 56024, 67852, 23485, 63555, 27386, 22059, 96274, 96055, 98540, 93329, 58120, 3590, 29653, 19344, 79642, 34794, 11597, 86186, 69518, 22480, 78373, 88195, 62005, 18874, 36550, 7626, 78479, 101284, 79284, 74853, 37151, 91597, 13608, 16438, 20019, 26207, 73875, 35252, 23595, 82962, 68397, 30559, 22957, 102266, 9104, 66503, 87845, 34301, 46383, 31778, 68802, 52561, 79491, 92169, 67603, 100493, 26319, 10356, 64893, 12113, 100883, 64364, 74622, 91414, 58996, 21410, 81605, 95530, 45946, 80975, 57584, 93940, 18038, 35050, 68418, 78419, 67105, 9158, 94867, 21453, 64863, 60501, 37714, 42480, 52367, 22140, 18043, 41009, 29015, 24415, 35745, 77018, 70512, 13509, 15236, 75180, 20339, 57446, 32608, 99807, 4228, 8787, 18090, 36630, 30951, 48653, 67537, 20100, 44086, 86362, 64799, 30444, 18932, 90507, 25529, 88301, 60050, 40235, 58510, 41732, 18600, 53748, 87265, 78, 67152, 14431, 48911, 27203, 34421, 43569, 33474, 95334, 79345, 25796, 81808, 53029, 80509, 10157, 78584, 56654, 68647, 27681, 67740, 101996, 14629, 72479, 11168, 42876, 39332, 45594, 102217, 44640, 14290, 78003, 32510, 19939, 19143, 96970, 28299, 10810, 3950, 76065, 19469, 46028, 71770, 21848, 42142, 33323, 21787, 82369, 51374, 79614, 52576, 4089, 22493, 22876, 33461, 14581, 31705, 91261, 80838, 55173, 60998, 88666, 35061, 37728, 12809, 57612, 46823, 38326, 55425, 19636, 4274, 54410, 78528, 1681, 88591, 5071, 49085, 70820, 25257, 43365, 15976, 14620, 52769, 27400, 4151, 21543, 50407, 79590, 26653, 43351, 51704, 76352, 83008, 92298, 49858, 61882, 6728, 71209, 17137, 60062, 38992, 26759, 27542, 95879, 15357, 36981, 34919, 52287, 75850, 32063, 30039, 44218, 47580, 49599, 12303, 49988, 65402, 1793, 16468, 100128, 86296, 49759, 80587, 31730, 27476, 19368, 2299, 59663, 52867, 8875, 70047, 48391, 44005, 69904, 71458, 75168, 61217, 59127, 25197, 93283, 72528, 29471, 14607, 101851, 10925, 32187, 70171, 83663, 71004, 23267, 90383, 36336, 44187, 47597, 38690, 52393, 2107, 51572, 38390, 75018, 83809, 34550, 36658, 94165, 8035, 42892, 62453, 20108, 50554, 62531, 19099, 11325, 69875, 51053, 27764, 33439, 88386, 719, 63805, 62859, 15327, 76338, 64705, 77682, 9805, 40899, 39827, 77128, 68190, 72538, 22751, 32935, 61374, 14837, 74975, 92075, 66436, 74247, 15817, 48895, 50575, 27463, 66348, 69493, 44771, 101330, 24232, 75029, 49943, 93920, 89921, 86947, 44449, 60365, 101682, 99663, 54994, 15549, 58389, 87020, 62582, 56583, 79611, 18586, 27595, 37954, 40327, 61698, 62779, 67616, 66407, 49964, 37190, 41450, 34059, 79582, 4548, 30780, 41453, 58812, 76913, 49292, 31698, 69810, 92046, 84907, 65376, 94763, 3856, 55969, 26788, 53040, 88268, 22051, 87882, 96707, 26949, 60526, 41157, 23637, 95578, 82662, 11186, 34706, 90661, 17044, 22122, 42778, 59184, 80616, 20515, 32858, 91573, 63795, 88605, 6824, 93861, 32687, 10749, 20613, 50463, 42880, 77373, 65517, 21792, 58519, 92975, 19284, 88572, 41470, 3654, 22312, 69323, 81211, 8854, 7340, 34000, 52218, 63920, 93476, 58834, 26928, 28982, 83265, 72687, 852, 100190, 33570, 101337, 36036, 22707, 47348, 89254, 19191, 54525, 71303, 34240, 30699, 52723, 23061, 94081, 96510, 9695, 21544, 90662, 60195, 94038, 2710, 59719, 1129, 76177, 72070, 21364, 15262, 81217, 38966, 57779, 53399, 16078, 34581, 76747, 70477, 69009, 22205, 81235, 48803, 81762, 1950, 96065, 85089, 315, 15977, 96349, 12233, 97555, 85751, 79381, 100138, 30223, 40559, 19102, 78615, 67080, 6405, 53329, 98939, 82592, 20687, 62855, 2401, 75507, 67463, 97750, 64956, 2545, 54782, 88604, 65365, 98584, 72339, 101684, 11132, 10003, 36293, 26318, 38793, 60716, 77991, 44906, 15411, 51059, 50044, 97300, 73089, 79025, 41089, 4765, 22556, 46595, 53517, 59311, 29348, 27539, 33807, 10393, 1307, 87382, 22710, 53363, 45256, 37131, 57120, 32310, 7753, 56820, 37557, 93452, 12217, 93081, 62816, 65061, 14142, 94779, 80151, 12851, 18662, 37029, 98598, 21703, 54508, 20957, 73870, 66666, 79090, 25987, 90490, 31176, 57055, 37251, 63822, 36100, 34886, 62367, 11462, 72224, 53341, 58436, 443, 76295, 69096, 5893, 62674, 86478, 85021, 27836, 37281, 4601, 21647, 47206, 14822, 20775, 5001, 100958, 30255, 82968, 55547, 98082, 34853, 49432, 91565, 79930, 46125, 30269, 9474, 99308, 73754, 75535, 10289, 40557, 102282, 37794, 35470, 56707, 33767, 102295, 58313, 4654, 82302, 31681, 17395, 12294, 46783, 59439, 24935, 14914, 39755, 61544, 29947, 28241, 77880, 33572, 17145, 73891, 28587, 36106, 26042, 24959, 84449, 53239, 81532, 36531, 18978, 78825, 81197, 42418, 86717, 17456, 28217, 14487, 19212, 43349, 29023, 25333, 19967, 14549, 61079, 92516, 69821, 100616, 60812, 69480, 51348, 93534, 5813, 96498, 47838, 100632, 16147, 35829, 24395, 80430, 78172, 90330, 42813, 59116, 9224, 91533, 64603, 3971, 3458, 63934, 66968, 60395, 83567, 3887, 24701, 35820, 14232, 87792, 19360, 55235, 20806, 98625, 84837, 63879, 46341, 78458, 8598, 21563, 79539, 81166, 65063, 39034, 44209, 6858, 23602, 28116, 98847, 59447, 66021, 49763, 76587, 49011, 62743, 45570, 89054, 34177, 92982, 806, 59004, 49417, 45253, 26246, 42152, 55337, 53670, 15133, 37473, 29472, 7089, 82605, 87559, 49439, 15278, 49420, 25724, 47191, 57037, 80975, 26303, 81992, 12012, 37723, 85163, 100525, 16134, 58601, 41473, 15159, 94669, 40420, 70413, 26617, 56133, 73648, 88828, 93231, 69495, 65697, 77198, 43861, 100659, 61534, 12730, 69900, 22890, 102230, 99189, 92065, 45587, 54353, 77404, 63668, 865, 96130, 11337, 62968, 23300, 76022, 36604, 34875, 79059, 7122, 11333, 80492, 83971, 1786, 796, 98090, 31029, 62435, 62194, 52118, 19959, 66243, 70969, 164, 21942, 53482, 31369, 13637, 43585, 93504, 8623, 23071, 21502, 49442, 87700, 8492, 30412, 46992, 65293, 47991, 94789, 9673, 11400, 64875, 47224, 22211, 87947, 10830, 21818, 91529, 54779, 8886, 54533, 87315, 41826, 87085, 92409, 48852, 101994, 75019, 54676, 60172, 71752, 13584, 19514, 101581, 49895, 49194, 27645, 14631, 13005, 4703, 99560, 12240, 74525, 69706, 31401, 1526, 86010, 95805, 37063, 3789, 76285, 39936, 94228, 57080, 29392, 26008, 82448, 76279, 47002, 81123, 98825, 94273, 18856, 28598, 47544, 86189, 31747, 75928, 70417, 91652, 91159, 50524, 71920, 94829, 85581, 10449, 75007, 77396, 48370, 47278, 52953, 55304, 37860, 62883, 48125, 16952, 61946, 52148, 86689, 101754, 74301, 29611, 38667, 37028, 43172, 92809, 72601, 68026, 3348, 47310, 23887, 84664, 3618, 90469, 9441, 79146, 2421, 51202, 94020, 50985, 55783, 92168, 47348, 54726, 65844, 28494, 21073, 57590, 38906, 2875, 58566, 25655, 76715, 57795, 50796, 31440, 45710, 99728, 83200, 59670, 1707, 32940, 94464, 64746, 3589, 34007, 24714, 74267, 28412, 13885, 101884, 32341, 39584, 41792, 52399, 75989, 80126, 41337, 64118, 27545, 45636, 18182, 59266, 57993, 37908, 66381, 89410, 58757, 48769, 82842, 17993, 2333, 50953, 34289, 89613, 67426, 90826, 70656, 74654, 90589, 74264, 56549, 40626, 90621, 83572, 97308, 79766, 61519, 2334, 100961, 41233, 53431, 68899, 93124, 1465, 81167, 50178, 58988, 84899, 38273, 59439, 83941, 67745, 28193, 50517, 69668, 72981, 98406, 80682, 62108, 85312, 77508, 47085, 74520, 96133, 78677, 39082, 26157, 29091, 42485, 10504, 82850, 86769, 101015, 38608, 29897, 80715, 9804, 66951, 5910, 77783, 84811, 52743, 78274, 98890, 14084, 5674, 57959, 60883, 4170, 78272, 45864, 50341, 7533, 18194, 52200, 61009, 48616, 92934, 35725, 67380, 30704, 17120, 4474, 23253, 61229, 25679, 1858, 78218, 31426, 37355, 36716, 82065, 82173, 86126, 31872, 15191, 31324, 92538, 97808, 81548, 83146, 64460, 17790, 65770, 82011, 37238, 82471, 16191, 90854, 26161, 44711, 46164, 29553, 55393, 98387, 53329, 11612, 5487, 94700, 3419, 45373, 63906, 87259, 25781, 93207, 5918, 26704, 71031, 21290, 99190, 46745, 56683, 48188, 52653, 1147, 6122, 70984, 73137, 16898, 71985, 71287, 56330, 96750, 37906, 43421, 11219, 10600, 46509, 32528, 20500, 61126, 82776, 63260, 37278, 31184, 15565, 4409, 4865, 89272, 62375, 66826, 11153, 88650, 67755, 44492, 56866, 56992, 55320, 52719, 91610, 12789, 38289, 101244, 12463, 68832, 36141, 43823, 60247, 35247, 19435, 84581, 85807, 31745, 88818, 34311, 26318, 3335, 35036, 94684, 24772, 46091, 87859, 101053, 447, 101927, 12595, 77159, 17094, 33087, 48945, 13809, 53839, 90940, 47031, 33338, 54705, 34976, 99901, 85490, 50484, 34479, 100305, 35946, 32021, 2937, 9441, 68297, 26789, 62205, 47842, 29066, 97483, 11081, 95525, 51259, 64127, 52346, 87595, 12353, 93677, 34526, 10290, 45618, 48284, 83504, 61302, 231, 3222, 53503, 101644, 17275, 43752, 99098, 61756, 59047, 82831, 16274, 85854, 45418, 15905, 67226, 34246, 5775, 21270, 60748, 35415, 164, 59149, 13986, 93091, 40946, 86116, 31116, 44003, 61209, 87934, 83776, 92021, 46731, 41366, 20384, 17836, 85937, 46482, 3459, 26636, 97472, 31118, 71937, 5168, 41751, 94652, 51051, 61406, 410, 82070, 72083, 53734, 100106, 6492, 99720, 67566, 89303, 3742, 69285, 81067, 78524, 5126, 100129, 87074, 75161, 21509, 521, 71522, 12621, 29943, 88770, 48777, 67915, 35934, 69607, 29357, 69396, 56078, 2946, 97206, 61965, 45869, 5294, 705, 75947, 6018, 28486, 83276, 31456, 22688, 48791, 36600, 66440, 96474, 57672, 98112, 56536, 53249, 48484, 15311, 18322, 13992, 63774, 34423, 64723, 34254, 66586, 78141, 29851, 67552, 79275, 19347, 14321, 51146, 75766, 25932, 73038, 74983, 45843, 2192, 61014, 33780, 40438, 83740, 88088, 417, 76560, 60778, 4676, 27392, 35348, 9310, 82214, 52775, 43305, 87502, 1862, 28451, 34938, 81589, 42913, 51147, 53187, 35079, 16742, 72963, 79219, 49083, 1965, 58720, 92963, 90691, 43643, 67686, 71510, 9527, 16502, 38531, 96411, 60202, 64010, 87504, 53524, 14671, 42469, 29889, 87180, 12441, 8383, 8673, 60598, 56289, 47124, 17457, 91947, 85724, 73634, 32018, 86722, 61770, 90409, 33466, 42703, 19470, 86199, 48413, 67393, 32313, 7798, 95840, 88179, 45515, 64131, 60171, 32700, 61924, 82509, 837, 30516, 49219, 64413, 49529, 24778, 85598, 24546, 53236, 27367, 7519, 354, 58225, 95522, 39029, 16629, 70941, 85716, 84281, 63115, 29507, 58454, 69914, 84362, 71562, 1521, 36570, 38021, 87766, 38760, 29583, 97264, 102237, 90846, 17505, 21721, 92416, 79558, 14160, 84721, 45764, 40315, 45470, 79995, 98000, 27230, 2648, 8407, 69338, 41992, 64435, 29544, 19191, 24847, 63882, 32673, 17899, 98457, 62644, 2285, 12922, 51658, 3292, 92623, 67107, 94728, 62894, 71281, 47544, 31029, 40919, 9439, 64847, 90694], cumulative_logprob=-47173.329442977905, logprobs=None, finish_reason=length, stop_reason=None)],\n",
       " 'finished': True,\n",
       " 'metrics': RequestMetrics(arrival_time=1727108606.549844, last_token_time=1727108606.549844, first_scheduled_time=1727108606.5532076, first_token_time=1727108606.565201, time_in_queue=0.0033636093139648438, finished_time=1727108658.0429325),\n",
       " 'lora_request': None}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = tokenizer(output.outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [100000, 67107, 7287, 91538, 31696, 91433, 78842, 5001, 38034, 53232, 97300, 63007, 99663, 57801, 40619, 91234, 39680, 37883, 2495, 28845, 34845, 32927, 4353, 22983, 79063, 34799, 55435, 29778, 16513, 3873, 22083, 46540, 47506, 43878, 84403, 26160, 1945, 78, 849, 64833, 29747, 98204, 87071, 28131, 82965, 40413, 16327, 99351, 72142, 74758, 12797, 53618, 15813, 71265, 9838, 62261, 48740, 43354, 73336, 53156, 51, 7520, 278, 46864, 24404, 67547, 4923, 75902, 33971, 52350, 65, 625, 1709, 32869, 70770, 26838, 52236, 13328, 73152, 86586, 38213, 99526, 78060, 82954, 24373, 66359, 11785, 70537, 24041, 36732, 8325, 30843, 70030, 89914, 77362, 95693, 15491, 88220, 43560, 28331, 53631, 70, 78887, 65609, 98314, 93818, 29877, 26780, 74282, 83802, 26374, 25791, 8420, 59278, 91305, 92064, 66721, 33550, 11343, 61335, 93833, 70495, 72844, 39644, 96766, 93454, 22465, 45082, 41005, 47900, 13999, 59932, 76582, 1094, 38649, 19547, 30410, 31983, 16635, 64459, 21061, 43048, 4390, 92646, 58823, 56191, 38562, 71079, 25917, 39688, 64220, 87042, 82243, 27657, 15376, 96990, 82701, 56749, 34968, 18150, 66638, 21407, 19317, 84407, 23584, 97009, 59304, 41671, 55365, 78782, 58949, 93669, 55541, 78444, 31972, 57126, 48001, 85429, 84794, 51947, 2335, 86351, 2086, 83859, 18060, 56266, 70019, 63736, 33096, 67310, 67656, 75027, 83085, 10035, 89162, 5391, 36356, 55284, 57967, 8401, 73243, 51701, 25731, 55207, 9525, 84490, 22317, 303, 7378, 406, 37647, 30546, 96955, 69019, 42710, 63161, 69344, 18632, 98528, 91222, 16276, 51003, 74857, 90163, 29646, 43925, 84796, 46927, 76775, 3738, 76673, 61113, 18982, 26667, 28828, 70220, 93744, 42671, 56598, 31880, 15953, 62735, 1971, 12378, 57731, 92217, 81873, 91449, 99270, 29114, 48909, 62233, 87666, 52845, 3838, 2839, 96689, 37783, 30049, 33186, 2833, 87333, 95990, 45565, 11130, 90274, 40455, 40479, 40171, 20819, 24950, 85499, 11128, 77053, 78189, 21522, 87647, 77107, 40414, 24410, 88380, 82762, 47576, 55456, 38246, 94860, 89161, 71121, 75045, 89709, 73289, 3393, 42729, 82669, 19685, 28428, 48755, 19558, 19950, 79827, 92120, 92616, 2784, 579, 42223, 19056, 19407, 93272, 69932, 59007, 48333, 24194, 89395, 95923, 20609, 47239, 42772, 69762, 49292, 52231, 28653, 70980, 62864, 67839, 59442, 95073, 9039, 247, 810, 4104, 24493, 14306, 17214, 65089, 57699, 24430, 31171, 82187, 18369, 14654, 30407, 40015, 42263, 69730, 15622, 66639, 89383, 74447, 89451, 1497, 46506, 47226, 35306, 93526, 83831, 70197, 61287, 36291, 15350, 31251, 65005, 82101, 42078, 11496, 52705, 37924, 25877, 42827, 73188, 8475, 93028, 82015, 42732, 26860, 48844, 38692, 69149, 85139, 25603, 27701, 84000, 8770, 60740, 17798, 55294, 35559, 53444, 47054, 46755, 36457, 4144, 35110, 54902, 82301, 43349, 95645, 98402, 73103, 87339, 31968, 66435, 58765, 53736, 81150, 34210, 14838, 48179, 33826, 41586, 95342, 34296, 8204, 6699, 37226, 70143, 26986, 32424, 22874, 59315, 94057, 96778, 95456, 11512, 68912, 55108, 12355, 24570, 34186, 36075, 53724, 76609, 10902, 84169, 51301, 11180, 8522, 76443, 70583, 8838, 9420, 52648, 3475, 77449, 71474, 33274, 84121, 86929, 351, 426, 5368, 72, 1892, 60116, 26290, 46950, 36304, 80212, 14260, 1431, 37681, 88922, 51625, 56899, 92227, 5560, 61642, 99896, 782, 12198, 2920, 21754, 81443, 56587, 16630, 41199, 6076, 26958, 55022, 54666, 72008, 46593, 33440, 72148, 89970, 35360, 26959, 30130, 926, 80, 8633, 26693, 32000, 862, 52203, 38956, 77850, 38899, 7768, 37896, 81310, 60074, 87260, 81775, 60846, 95267, 62164, 29158, 39049, 63860, 38087, 15601, 54981, 52531, 41877, 25172, 18851, 50435, 1043, 316, 247, 346, 66927, 18531, 71260, 53333, 20080, 17746, 56814, 61919, 58949, 41077, 4675, 14009, 61653, 17082, 1139, 46183, 36221, 5932, 61628, 18185, 58485, 24174, 6967, 29396, 19739, 75136, 53, 2605, 511, 6353, 70084, 29050, 23334, 77342, 90715, 44604, 58461, 58960, 34545, 49895, 24580, 69359, 32971, 40043, 3401, 73153, 95947, 96565, 26572, 54324, 41010, 79128, 1257, 71271, 37806, 15373, 322, 15497, 57394, 56789, 69177, 78195, 11583, 40977, 2350, 193, 91764, 83060, 62828, 43577, 23667, 6131, 3232, 56172, 12275, 8316, 7798, 42323, 76674, 71174, 70548, 47669, 78547, 91551, 71468, 7213, 55048, 44913, 45816, 67067, 84616, 2396, 7102, 39275, 12199, 32853, 19136, 87238, 31841, 61503, 14874, 93388, 42374, 66010, 3935, 1294, 772, 96695, 41377, 82157, 95754, 18019, 60362, 60103, 41413, 50668, 33058, 60851, 547, 4049, 409, 306, 37399, 61857, 61755, 86230, 184, 26314, 4970, 92720, 86260, 66631, 92160, 48226, 28874, 97465, 62673, 67246, 14825, 73639, 20265, 22659, 15436, 43741, 5998, 81321, 56329, 14109, 1842, 2543, 2805, 64, 56788, 63601, 21567, 73521, 57398, 79136, 33491, 95542, 10003, 9908, 91000, 35, 56554, 9644, 27684, 10827, 24442, 13720, 93775, 1368, 72992, 43210, 88281, 35316, 29593, 84958, 3412, 425, 894, 3580, 28517, 1579, 85869, 17429, 82889, 83497, 11657, 14774, 98082, 4159, 67058, 1902, 73737, 12371, 69345, 79622, 99664, 83488, 50438, 12860, 23314, 433, 1046, 255, 21460, 93583, 23880, 98562, 4148, 61659, 1629, 425, 43852, 52565, 29976, 8837, 67147, 96207, 22112, 52490, 95049, 28588, 65952, 23444, 57234, 44899, 78983, 81217, 19605, 40973, 97042, 40242, 5369, 904, 16511, 52954, 26072, 21428, 6806, 80614, 40623, 40361, 79811, 83111, 66062, 78317, 19393, 24812, 64272, 68046, 26101, 3120, 99989, 16371, 30839, 59429, 91397, 2659, 42583, 35524, 56601, 46578, 90369, 80406, 40744, 13655, 86939, 11328, 10417, 45979, 38950, 59429, 1475, 1686, 83788, 86239, 1551, 480, 295, 2870, 37853, 31017, 12008, 35354, 69344, 86147, 96912, 67877, 34095, 6006, 68003, 8928, 38865, 77360, 55291, 454, 355, 8358, 62584, 11414, 32536, 63261, 40648, 38920, 17346, 39570, 19148, 55742, 73339, 37020, 35863, 54986, 49253, 37264, 79477, 69320, 88566, 89061, 38772, 86491, 71, 70468, 57543, 45825, 3553, 50520, 97878, 18075, 56957, 25618, 60770, 19680, 28650, 59422, 82680, 76882, 73394, 32062, 46384, 15431, 4446, 67147, 37300, 83368, 29241, 98326, 20200, 19125, 871, 35, 10256, 510, 13090, 88781, 58772, 44301, 5268, 24861, 91992, 75966, 77169, 75779, 23900, 95825, 24131, 42329, 37614, 64078, 22407, 17897, 15039, 19363, 98793, 51441, 29111, 52383, 417, 7678, 83816, 44646, 22196, 89290, 59580, 75112, 94575, 91116, 84893, 65794, 71120, 56649, 34182, 76705, 10003, 64563, 8699, 56728, 25198, 271, 24180, 21845, 68461, 18983, 31534, 5775, 39792, 83897, 6729, 782, 60625, 96667, 44660, 46530, 60776, 29785, 79390, 97159, 24928, 68192, 34768, 54330, 18308, 65627, 68850, 21614, 5371, 75603, 37642, 85230, 56729, 9643, 12098, 84151, 92570, 57915, 66733, 32153, 64962, 15045, 87126, 77212, 94763, 42509, 76268, 73846, 25996, 65565, 65238, 58944, 14560, 50220, 58576, 63246, 76120, 63584, 91048, 72197, 31310, 95103, 74428, 41580, 24496, 68112, 38552, 7975, 94390, 78384, 95208, 36370, 46698, 90268, 50385, 71928, 71064, 93793, 28350, 80661, 22132, 34599, 48829, 7309, 33647, 54278, 20193, 80418, 4365, 63088, 15501, 261, 3650, 319, 63698, 50905, 94714, 39900, 59331, 576, 17937, 79972, 11460, 96778, 77392, 47098, 89262, 40904, 75395, 20296, 3406, 99039, 43998, 41736, 66003, 27076, 51322, 66124, 64313, 58727, 53880, 21450, 3279, 68827, 60311, 29415, 15197, 24918, 40363, 42932, 20632, 53204, 41864, 45643, 84749, 15637, 45865, 68058, 18555, 52816, 89905, 77803, 62975, 73855, 47072, 18642, 96565, 77935, 96505, 26131, 69396, 37062, 19553, 2756, 38649, 58566, 91067, 75446, 40205, 1243, 262, 658, 303, 4947, 22342, 46778, 58089, 24415, 90750, 84674, 8737, 90845, 88139, 21350, 18201, 13374, 14391, 18384, 82018, 40813, 30467, 44252, 10965, 22600, 54125, 14752, 15024, 91034, 74188, 66586, 77699, 61712, 65932, 3147, 41684, 32642, 92141, 71761, 450, 1294, 1887, 369, 2405, 80, 13298, 740, 84268, 72392, 27048, 59375, 97604, 58256, 81366, 50764, 55755, 27276, 3185, 31025, 65618, 97015, 84815, 82477, 59662, 58757, 68575, 86908, 77477, 69736, 24900, 10003, 32187, 36375, 2073, 91291, 73213, 30467, 85726, 73715, 14429, 61345, 50953, 1821, 308, 3383, 80, 9390, 37460, 13218, 33277, 88660, 66765, 80260, 93321, 65114, 77367, 87269, 24667, 56489, 38568, 60330, 63032, 24927, 75206, 21342, 66856, 925, 65705, 20357, 37945, 12100, 14256, 77438, 52286, 60442, 92940, 75051, 51445, 45882, 12128, 30580, 27303, 13373, 40906, 13440, 37484, 2251, 3436, 28575, 335, 56500, 37857, 12695, 92372, 13272, 39744, 85351, 88520, 1024, 85173, 60214, 71319, 14538, 3723, 35318, 5840, 43565, 15235, 79965, 90541, 63536, 47269, 23791, 294, 83137, 96795, 81746, 97669, 49426, 6145, 89029, 10780, 85704, 53931, 6282, 5740, 12600, 59916, 207, 1444, 6660, 306, 95495, 11933, 49509, 8483, 69046, 85916, 78234, 64977, 77516, 84858, 11125, 30778, 14180, 25121, 72584, 43849, 98683, 13619, 8134, 82432, 89724, 7130, 29040, 10953, 31690, 15424, 62647, 19088, 66225, 38966, 50333, 31422, 50450, 68737, 56608, 18470, 83894, 71392, 39331, 2567, 87951, 12456, 80769, 99233, 66741, 62081, 11725, 94518, 70506, 61677, 80448, 23851, 899, 6452, 1787, 255, 3248, 10353, 4272, 23728, 47037, 12578, 64024, 83348, 37836, 41027, 66157, 92023, 1374, 6590, 985, 27236, 43954, 96321, 16075, 21762, 8828, 16117, 23787, 6807, 75606, 17364, 37849, 35677, 88406, 86898, 66435, 25645, 37082, 43268, 47132, 45249, 59462, 37623, 66280, 81984, 88427, 26396, 94645, 31222, 65365, 17885, 81609, 76753, 25059, 19768, 84603, 11918, 25607, 15055, 10003, 87291, 60261, 57745, 67390, 74221, 67798, 59581, 25631, 95996, 93418, 32484, 43603, 48214, 26026, 46851, 48751, 29079, 89628, 25320, 63952, 22965, 61336, 58719, 83783, 76533, 41426, 26373, 41793, 37714, 77885, 63074, 11832, 734, 80894, 69871, 61702, 68091, 89739, 47189, 5942, 13110, 88653, 69081, 67657, 91926, 41822, 97566, 17947, 49741, 52212, 93004, 71652, 65941, 42322, 20068, 24371, 30176, 10074, 39389, 29851, 3212, 54843, 93102, 11451, 75364, 73755, 1800, 42078, 21484, 90481, 655, 65360, 27867, 6935, 319, 71000, 72840, 66464, 41129, 6703, 13627, 77786, 42929, 88798, 14871, 27162, 96241, 59684, 5537, 26493, 50162, 78873, 7803, 16665, 86102, 41332, 27047, 94261, 86549, 43637, 25428, 53357, 49849, 5417, 29312, 253, 41046, 1218, 480, 333, 49213, 60981, 27575, 8018, 10643, 47029, 278, 1162, 26582, 5550, 75801, 69222, 27505, 88194, 97305, 97251, 58227, 26870, 47255, 93596, 92480, 18273, 50404, 32577, 64463, 54253, 16941, 59806, 31625, 1516, 62389, 52978, 34369, 85104, 56380, 18611, 38025, 98756, 38443, 27476, 35341, 77857, 43802, 32319, 88026, 59783, 35214, 72250, 39387, 3831, 42559, 98996, 16189, 5738, 14805, 511, 9626, 78216, 80032, 84891, 6591, 82846, 99447, 98216, 34529, 78204, 21936, 53810, 22911, 13485, 28210, 23272, 33373, 28684, 38781, 31230, 63498, 639, 10013, 5525, 20633, 40423, 6268, 95301, 34291, 61245, 82843, 50732, 54679, 15322, 93314, 81284, 54807, 86983, 3406, 99536, 40377, 93923, 53314, 70351, 62855, 625, 400, 14545, 42695, 76651, 33006, 34322, 33297, 47711, 67639, 68413, 71020, 88544, 4822, 57508, 3006, 71257, 19545, 23556, 28688, 55946, 59828, 39900, 64130, 44862, 71782, 46832, 28367, 61183, 15, 53445, 68156, 83218, 80914, 41571, 22081, 69622, 46043, 31086, 42571, 94010, 12613, 3730, 61913, 47511, 74156, 3576, 10003, 26415, 42579, 82090, 60886, 17092, 45258, 3693, 51866, 82343, 94546, 95593, 77844, 35882, 4064, 8130, 32866, 54403, 21226, 39461, 79172, 93072, 57187, 56647, 61470, 87538, 90858, 99384, 64905, 88712, 48637, 51837, 96105, 2481, 48812, 66718, 65395, 90903, 75573, 37576, 17463, 87695, 26101, 73545, 13537, 90221, 10854, 42472, 38669, 55671, 26154, 80825, 19295, 36647, 22316, 83661, 10794, 31016, 53008, 29196, 18666, 45204, 77055, 80608, 45335, 42242, 86852, 92907, 64430, 37164, 6239, 80920, 97897, 10417, 77818, 46260, 8057, 565, 9925, 76792, 47946, 51025, 29107, 92576, 54071, 87823, 79144, 73286, 69565, 27545, 46075, 8783, 27279, 834, 97876, 34061, 6860, 47265, 73484, 75, 71182, 26416, 74748, 92237, 33557, 82142, 39384, 76977, 32388, 246, 791, 5376, 5644, 16243, 358, 55060, 28792, 38609, 4686, 19138, 17695, 58115, 56174, 71744, 3296, 20056, 17235, 31934, 84227, 41008, 73911, 30118, 30088, 47154, 54597, 68429, 92977, 72591, 71428, 57448, 91345, 79723, 23915, 31508, 84697, 87930, 51111, 76624, 20741, 74712, 14733, 67434, 21993, 82896, 79383, 81142, 43204, 46674, 20489, 6138, 92970, 4169, 1274, 2462, 5726, 7702, 18827, 81067, 72015, 69214, 10003, 8912, 79692, 10379, 7565, 253, 2349, 246, 62433, 89668, 2100, 67722, 87331, 63865, 16230, 517, 268, 720, 78294, 10999, 79723, 54779, 5321, 49756, 30185, 74318, 15145, 88903, 75353, 92873, 59692, 92610, 37823, 246, 1387, 918, 6387, 77580, 3768, 19820, 78819, 68767, 20723, 37199, 3375, 36670, 12439, 43942, 19462, 50968, 20218, 22201, 74418, 69641, 2087, 64305, 67765, 77824, 50725, 2976, 72534, 1577, 426, 379, 2085, 74916, 99210, 46974, 41232, 3727, 7779, 901, 390, 249, 60003, 47744, 48796, 19388, 44457, 14023, 76339, 56979, 68131, 13628, 49433, 31010, 81295, 75300, 57254, 36209, 45926, 9383, 3292, 39809, 4837, 68510, 48898, 22604, 94312, 27855, 10003, 83774, 72976, 78877, 96462, 17500, 8858, 75997, 13210, 48447, 41137, 22343, 32741, 7372, 899, 90940, 38252, 50722, 35776, 26972, 51623, 33898, 28583, 27680, 98949, 65130, 13527, 18686, 4021, 18825, 95828, 86336, 79765, 52320, 24183, 99518, 96340, 87676, 17818, 52707, 45131, 49766, 10003, 79403, 69150, 72442, 59603, 59961, 34534, 99344, 4928, 32864, 8279, 4147, 10307, 85222, 32236, 17663, 24811, 99358, 96165, 6350, 4992, 93395, 74013, 34890, 84935, 85945, 87110, 29769, 65968, 91817, 93106, 18471, 33965, 5145, 57616, 41083, 20399, 34972, 24003, 62181, 48309, 84441, 64903, 34543, 15844, 40418, 63647, 2424, 82428, 75150, 88807, 207, 1202, 70530, 5745, 19515, 78582, 3617, 24731, 31706, 30336, 45419, 4057, 95355, 69937, 40506, 13990, 54306, 12480, 97845, 83686, 50806, 62788, 54082, 20450, 31647, 49154, 69330, 9453, 23362, 51874, 70673, 62830, 37182, 91812, 81340, 91265, 41398, 21432, 9402, 98219, 82850, 70427, 78916, 68372, 73941, 6869, 50646, 65387, 76621, 80818, 7584, 46211, 26791, 71973, 6033, 63060, 79893, 75465, 94663, 92933, 99749, 35729, 88672, 48802, 86399, 9279, 27852, 62729, 94528, 60183, 86803, 66471, 29096, 33302, 83, 28386, 98215, 41957, 36054, 96669, 29491, 14722, 35204, 29007, 64764, 28750, 11109, 18087, 59771, 11700, 90344, 66977, 13086, 78173, 36673, 76755, 62724, 46685, 3206, 815, 41581, 4529, 50840, 72952, 79657, 21772, 24115, 41067, 59854, 424, 85786, 834, 2017, 772, 87842, 16990, 86730, 85316, 64638, 21601, 19536, 67078, 52685, 47825, 41950, 61186, 66612, 83649, 74899, 82585, 43671, 2667, 67992, 51875, 88381, 97749, 94915, 46689, 77657, 66585, 16175, 27301, 20221, 36826, 37689, 79717, 92872, 59101, 44582, 46822, 97621, 50663, 10849, 62612, 68660, 41963, 94779, 29197, 47641, 13223, 82249, 1259, 97175, 51944, 35291, 11663, 50248, 88606, 93732, 94106, 70768, 47007, 30487, 93728, 21027, 19184, 639, 305, 775, 8844, 1428, 51, 596, 2781, 30078, 68763, 83439, 35637, 44655, 18841, 45935, 53322, 36427, 75726, 9017, 94048, 9408, 17286, 19790, 266, 33007, 79557, 28927, 21861, 51377, 24049, 53315, 97758, 27638, 56680, 42882, 89158, 54207, 10325, 46523, 57487, 19033, 3958, 253, 694, 312, 2109, 28219, 43716, 16068, 54657, 70514, 10274, 95770, 49158, 97541, 6667, 15562, 24318, 42213, 79494, 66106, 25380, 49203, 42849, 88754, 91551, 51209, 79577, 47683, 61542, 15360, 659, 2034, 84927, 500, 88602, 63318, 5542, 483, 5823, 38943, 3663, 70858, 7146, 51419, 25999, 71539, 21709, 17599, 4046, 61447, 51521, 66528, 29357, 92172, 3103, 34903, 22199, 13867, 65152, 6449, 69004, 83452, 81933, 86890, 20039, 87953, 63975, 91466, 31558, 17623, 10003, 96224, 29514, 56015, 88301, 62280, 87378, 36656, 29179, 2555, 522, 56113, 6035, 30425, 38792, 86947, 78965, 80461, 16781, 65055, 5687, 62917, 2382, 88769, 19221, 33346, 46405, 612, 37609, 77800, 9150, 14462, 8328, 26835, 95261, 5451, 96985, 16536, 58997, 45514, 74722, 91659, 24281, 26771, 11048, 47546, 92247, 46014, 1898, 8651, 17281, 17059, 89281, 85614, 56759, 44163, 29521, 81518, 67561, 5993, 27616, 26958, 4627, 22719, 13888, 16912, 33772, 69892, 7652, 55003, 39131, 63853, 15373, 76221, 14215, 72820, 98538, 40070, 62971, 76132, 96763, 32071, 46490, 14437, 23238, 55001, 75019, 45731, 52940, 53833, 11730, 10201, 2439, 92211, 92258, 56018, 65622, 75350, 84505, 5491, 77176, 70973, 52273, 59812, 15924, 79239, 51420, 27308, 52559, 10155, 95056, 30849, 52703, 22589, 32654, 6554, 40142, 93687, 66174, 68508, 36608, 16478, 18601, 59022, 83417, 65566, 99765, 9694, 6340, 29944, 22, 91717, 69275, 8007, 28348, 28092, 35481, 93567, 65490, 46048, 92220, 91682, 81702, 90591, 31138, 4011, 60026, 74361, 18950, 53144, 98733, 73352, 90952, 70710, 25354, 48389, 69637, 5203, 92637, 8418, 76645, 49050, 46427, 32412, 5023, 36092, 81041, 65620, 61750, 19746, 40147, 74902, 51235, 40270, 54215, 12952, 40885, 11721, 8705, 74240, 68364, 22016, 7528, 1800, 89993, 8068, 90180, 87012, 65330, 88486, 1981, 36035, 48411, 13532, 85344, 83756, 70382, 18234, 1354, 49778, 9319, 91818, 91387, 24969, 51234, 82539, 75509, 53333, 70693, 7893, 81331, 93051, 6947, 4688, 90488, 19867, 19034, 79076, 91356, 49838, 74773, 11615, 7720, 13752, 4914, 4515, 5067, 68245, 1706, 18505, 76352, 90162, 31481, 60241, 3067, 61874, 73364, 3898, 92529, 55209, 58874, 6602, 37251, 792, 84936, 40616, 13329, 92946, 59856, 94473, 54483, 58079, 43944, 62033, 7114, 15845, 8824, 13747, 480, 31006, 86623, 73250, 31393, 59907, 63650, 66694, 79642, 48437, 95537, 10822, 79678, 57772, 25724, 41180, 31693, 61446, 44009, 11856, 10061, 75756, 86775, 31632, 47621, 39598, 494, 39349, 24524, 41509, 50, 5997, 11671, 70921, 12146, 44319, 62753, 60575, 24600, 22688, 62161, 26136, 75226, 84787, 26420, 4507, 66911, 6355, 92724, 53578, 48243, 85938, 69150, 43217, 65885, 70864, 80940, 10603, 70614, 27477, 89527, 72146, 75637, 33089, 75617, 91301, 40688, 17855, 57073, 41265, 64698, 16183, 420, 2291, 268, 15066, 72072, 76539, 13133, 18261, 39982, 8951, 68430, 843, 27883, 918, 52697, 67492, 14073, 58880, 37978, 99416, 18874, 7327, 1990, 550, 79015, 58353, 62475, 80563, 77995, 18660, 7005, 79946, 92977, 49035, 95617, 77333, 16912, 49497, 73542, 63016, 10200, 35912, 13625, 37167, 84708, 75705, 63046, 17148, 36256, 42051, 70401, 8370, 85171, 68378, 31412, 77698, 27735, 17535, 84766, 33412, 65123, 15605, 93448, 99096, 53639, 5747, 61850, 80912, 29692, 69273, 23017, 73528, 28702, 4446, 11960, 56024, 67852, 23485, 63555, 27386, 22059, 96274, 96055, 98540, 93329, 58120, 3590, 29653, 19344, 79642, 34794, 11597, 86186, 69518, 22480, 78373, 88195, 62005, 18874, 36550, 7626, 78479, 79284, 74853, 37151, 91597, 13608, 16438, 20019, 26207, 73875, 35252, 23595, 82962, 68397, 30559, 22957, 9104, 66503, 87845, 34301, 46383, 31778, 68802, 52561, 79491, 92169, 67603, 26319, 10356, 64893, 12113, 64364, 74622, 91414, 58996, 21410, 81605, 95530, 45946, 80975, 57584, 93940, 18038, 35050, 68418, 78419, 67105, 9158, 94867, 21453, 64863, 54, 34377, 64, 42480, 52367, 22140, 18043, 41009, 29015, 24415, 35745, 77018, 70512, 13509, 15236, 75180, 20339, 57446, 32608, 99807, 4228, 8787, 18090, 36630, 30951, 48653, 67537, 20100, 44086, 86362, 64799, 30444, 18932, 90507, 25529, 88301, 60050, 40235, 58510, 48532, 1269, 458, 2805, 53748, 87265, 78, 67152, 14431, 48911, 27203, 34421, 43569, 33474, 95334, 79345, 25796, 81808, 53029, 80509, 10157, 78584, 56654, 68647, 27681, 67740, 14629, 72479, 11168, 42876, 39332, 45594, 44640, 14290, 78003, 32510, 19939, 19143, 96970, 28299, 10810, 3950, 76065, 19469, 46028, 71770, 21848, 42142, 33323, 21787, 82369, 51374, 79614, 52576, 4089, 22493, 22876, 33461, 14581, 31705, 91261, 80838, 55173, 60998, 88666, 35061, 37728, 12809, 57612, 46823, 38326, 55425, 19636, 4274, 54410, 78528, 1681, 88591, 5071, 49085, 70820, 25257, 43365, 15976, 14620, 52769, 27400, 4151, 21543, 50407, 79590, 26653, 43351, 51704, 76352, 83008, 92298, 49858, 61882, 6728, 71209, 17137, 60062, 38992, 26759, 27542, 95879, 15357, 36981, 34919, 52287, 75850, 32063, 30039, 44218, 47580, 49599, 4461, 957, 50, 65402, 1793, 16468, 86296, 49759, 80587, 31730, 27476, 19368, 2299, 59663, 52867, 8875, 70047, 48391, 44005, 69904, 71458, 75168, 61217, 59127, 25197, 93283, 72528, 29471, 14607, 10925, 32187, 70171, 83663, 71004, 23267, 90383, 36336, 44187, 47597, 38690, 281, 67138, 296, 510, 51572, 38390, 75018, 83809, 34550, 36658, 94165, 8035, 42892, 62453, 20108, 50554, 62531, 19099, 11325, 69875, 51053, 27764, 33439, 88386, 719, 16480, 36303, 4131, 29590, 6531, 2420, 64705, 77682, 731, 9689, 325, 39827, 77128, 68190, 72538, 22751, 32935, 61374, 14837, 74975, 92075, 66436, 74247, 15817, 48895, 50575, 27463, 66348, 69493, 44771, 24232, 75029, 49943, 93920, 89921, 86947, 44449, 60365, 99663, 54994, 15549, 58389, 87020, 62582, 56583, 79611, 18586, 27595, 37954, 40327, 61698, 62779, 67616, 66407, 49964, 37190, 41450, 34059, 79582, 4548, 256, 7935, 305, 58812, 76913, 49292, 5504, 827, 4552, 92046, 84907, 65376, 2840, 2357, 261, 55969, 26788, 53040, 88268, 22051, 87882, 96707, 26949, 60526, 41157, 23637, 95578, 82662, 11186, 34706, 90661, 17044, 22122, 42778, 59184, 80616, 20515, 32858, 91573, 10003, 88605, 597, 53833, 1812, 32687, 10749, 20613, 50463, 42880, 77373, 65517, 21792, 58519, 92975, 19284, 88572, 41470, 3654, 22312, 69323, 81211, 25436, 268, 34000, 52218, 63920, 93476, 58834, 26928, 21837, 860, 87, 72687, 852, 33570, 36036, 22707, 47348, 89254, 19191, 54525, 71303, 34240, 30699, 52723, 23061, 94081, 96510, 9695, 21544, 90662, 60195, 94038, 2710, 59719, 1129, 76177, 72070, 21364, 15262, 81217, 38966, 57779, 53399, 16078, 7409, 328, 815, 5717, 70477, 69009, 22205, 81235, 48803, 81762, 10003, 96065, 85089, 315, 15977, 96349, 12233, 97555, 85751, 79381, 30223, 40559, 19102, 78615, 67080, 6405, 53329, 98939, 82592, 20687, 62855, 2401, 75507, 67463, 97750, 64956, 2545, 54782, 88604, 65365, 98584, 72339, 11132, 10003, 36293, 26318, 38793, 60716, 77991, 44906, 15411, 51059, 50044, 97300, 73089, 79025, 41089, 4765, 22556, 46595, 53517, 59311, 29348, 27539, 33807, 10393, 1307, 87382, 22710, 53363, 45256, 37131, 57120, 32310, 7753, 56820, 37557, 93452, 12217, 93081, 62816, 65061, 14142, 94779, 80151, 12851, 18662, 37029, 98598, 21703, 54508, 20957, 73870, 66666, 79090, 25987, 90490, 31176, 57055, 37251, 63822, 36100, 34886, 62367, 11462, 72224, 53341, 58436, 10003, 76295, 69096, 5893, 62674, 86478, 85021, 27836, 37281, 4601, 21647, 47206, 14822, 20775, 5001, 74174, 371, 86, 382, 55547, 98082, 34853, 49432, 91565, 1577, 7759, 55811, 30269, 9474, 99308, 23808, 844, 77, 2728, 272, 10289, 40557, 37794, 35470, 56707, 33767, 58313, 4654, 82302, 31681, 17395, 12294, 46783, 59439, 24935, 14914, 909, 10859, 85, 29947, 28241, 77880, 33572, 17145, 73891, 28587, 36106, 26042, 24959, 84449, 53239, 81532, 36531, 18978, 78825, 81197, 42418, 86717, 17456, 28217, 14487, 19212, 43349, 29023, 25333, 19967, 14549, 61079, 92516, 69821, 60812, 69480, 51348, 93534, 5813, 96498, 47838, 16147, 35829, 24395, 80430, 8925, 11079, 75, 13652, 5283, 791, 384, 2283, 9224, 91533, 64603, 10003, 3458, 63934, 66968, 60395, 83567, 3887, 24701, 35820, 14232, 87792, 19360, 55235, 20806, 98625, 84837, 63879, 46341, 78458, 8598, 21563, 79539, 81166, 65063, 39034, 44209, 6858, 23602, 28116, 98847, 59447, 66021, 49763, 76587, 49011, 62743, 45570, 89054, 34177, 92982, 10003, 59004, 49417, 45253, 26246, 42152, 55337, 53670, 15133, 37473, 29472, 7089, 82605, 87559, 49439, 15278, 49420, 25724, 47191, 57037, 80975, 26303, 81992, 12012, 37723, 85163, 16134, 58601, 41473, 15159, 94669, 40420, 70413, 26617, 56133, 73648, 88828, 93231, 69495, 65697, 77198, 43861, 61534, 12730, 69900, 22890, 99189, 8848, 306, 11384, 13361, 54353, 77404, 63668, 865, 96130, 11337, 62968, 23300, 76022, 36604, 34875, 79059, 7122, 11333, 80492, 83971, 1786, 796, 98090, 31029, 9894, 54546, 11052, 52118, 19959, 66243, 70969, 10003, 21942, 53482, 31369, 13637, 43585, 93504, 8623, 23071, 21502, 49442, 87700, 8492, 30412, 324, 39687, 275, 1652, 47991, 94789, 9673, 11400, 64875, 47224, 22211, 87947, 10830, 21818, 91529, 54779, 8886, 54533, 87315, 41826, 87085, 92409, 48852, 75019, 54676, 60172, 71752, 13584, 19514, 49895, 49194, 27645, 14631, 13005, 4703, 99560, 12240, 74525, 69706, 31401, 10003, 86010, 95805, 37063, 3789, 76285, 39936, 94228, 57080, 29392, 26008, 82448, 76279, 47002, 81123, 98825, 94273, 18856, 28598, 47544, 86189, 31747, 75928, 7280, 13473, 22668, 91159, 50524, 71920, 94829, 85581, 10449, 75007, 77396, 48370, 47278, 52953, 55304, 37860, 62883, 48125, 16952, 61946, 52148, 86689, 74301, 29611, 38667, 37028, 43172, 92809, 72601, 9343, 894, 666, 47310, 23887, 84664, 3618, 90469, 9441, 79146, 2421, 51202, 94020, 50985, 55783, 92168, 47348, 54726, 65844, 28494, 21073, 57590, 38906, 2875, 58566, 25655, 76715, 57795, 50796, 31440, 45710, 99728, 83200, 59670, 10003, 32940, 94464, 64746, 3589, 34007, 24714, 74267, 28412, 13885, 32341, 39584, 41792, 52399, 75989, 80126, 41337, 64118, 27545, 45636, 18182, 59266, 57993, 37908, 66381, 89410, 24478, 2063, 21581, 82842, 17993, 6141, 332, 5164, 34289, 89613, 67426, 10003, 70656, 74654, 90589, 74264, 56549, 40626, 90621, 83572, 97308, 79766, 61519, 2334, 41233, 53431, 68899, 93124, 1465, 81167, 50178, 58988, 84899, 38273, 59439, 83941, 67745, 28193, 50517, 69668, 72981, 98406, 80682, 62108, 85312, 77508, 47085, 74520, 96133, 78677, 39082, 26157, 29091, 42485, 10504, 82850, 86769, 38608, 29897, 80715, 9804, 1981, 5976, 2824, 77783, 84811, 52743, 78274, 98890, 14084, 5674, 57959, 60883, 4170, 78272, 45864, 50341, 7533, 18194, 52200, 61009, 48616, 92934, 35725, 67380, 30704, 17120, 4474, 23253, 61229, 25679, 1858, 78218, 31426, 37355, 36716, 82065, 82173, 86126, 31872, 15191, 31324, 92538, 97808, 81548, 83146, 64460, 17790, 65770, 82011, 37238, 82471, 16191, 90854, 26161, 44711, 46164, 5442, 8237, 926, 98387, 53329, 11612, 5487, 94700, 3419, 45373, 63906, 87259, 25781, 93207, 5918, 26704, 71031, 21290, 99190, 46745, 56683, 48188, 52653, 1147, 6122, 70984, 73137, 5628, 266, 7106, 69, 71287, 56330, 96750, 37906, 43421, 11219, 10600, 46509, 90677, 1215, 12582, 61126, 82776, 63260, 37278, 31184, 15565, 4409, 4865, 89272, 1447, 6296, 1812, 9723, 11153, 88650, 79, 2570, 916, 2114, 56866, 320, 282, 404, 1300, 489, 52719, 91610, 12789, 263, 6687, 495, 68832, 36141, 43823, 60247, 35247, 19435, 84581, 85807, 31745, 88818, 34311, 26318, 3335, 35036, 94684, 24772, 46091, 87859, 447, 12595, 77159, 17094, 33087, 48945, 13809, 53839, 90940, 47031, 33338, 54705, 34976, 99901, 85490, 50484, 34479, 35946, 32021, 2937, 9441, 68297, 26789, 62205, 47842, 29066, 97483, 11081, 95525, 51259, 64127, 52346, 87595, 12353, 93677, 34526, 10290, 45618, 48284, 83504, 61302, 10003, 3222, 53503, 66131, 406, 3049, 296, 99098, 61756, 59047, 82831, 16274, 85854, 45418, 15905, 67226, 34246, 5775, 21270, 60748, 35415, 10003, 59149, 13986, 93091, 40946, 86116, 31116, 44003, 61209, 87934, 83776, 92021, 46731, 41366, 20384, 17836, 85937, 46482, 3459, 26636, 97472, 31118, 71937, 5168, 41751, 94652, 51051, 61406, 410, 82070, 72083, 70, 539, 789, 99720, 67566, 89303, 3742, 69285, 1213, 17559, 10003, 18416, 347, 1165, 85, 2568, 21509, 521, 71522, 12621, 29943, 88770, 48777, 67915, 35934, 69607, 29357, 69396, 56078, 2946, 97206, 61965, 45869, 5294, 705, 75947, 6018, 28486, 83276, 31456, 22688, 48791, 36600, 66440, 96474, 57672, 98112, 56536, 53249, 48484, 15311, 18322, 13992, 63774, 34423, 64723, 34254, 66586, 78141, 29851, 67552, 79275, 19347, 14321, 51146, 75766, 25932, 73038, 74983, 45843, 2192, 61014, 33780, 40438, 83740, 88088, 417, 76560, 60778, 4676, 27392, 35348, 9310, 82214, 52775, 43305, 87502, 1862, 28451, 34938, 81589, 42913, 51147, 53187, 35079, 16742, 72963, 79219, 49083, 10003, 58720, 92963, 90691, 43643, 67686, 71510, 9527, 16502, 38531, 96411, 60202, 64010, 87504, 53524, 14671, 42469, 29889, 87180, 12441, 8383, 8673, 60598, 56289, 47124, 17457, 91947, 85724, 73634, 32018, 380, 26551, 425, 31745, 90409, 33466, 42703, 19470, 86199, 48413, 67393, 32313, 7798, 95840, 15771, 262, 1946, 313, 64131, 60171, 32700, 61924, 82509, 2925, 4933, 305, 49219, 64413, 49529, 24778, 85598, 24546, 53236, 27367, 7519, 10003, 58225, 95522, 39029, 16629, 70941, 85716, 84281, 63115, 29507, 58454, 69914, 84362, 71562, 1521, 36570, 38021, 87766, 38760, 29583, 97264, 90846, 17505, 21721, 92416, 79558, 14160, 84721, 45764, 40315, 45470, 79995, 98000, 5586, 791, 80, 281, 462, 480, 41992, 64435, 29544, 19191, 24847, 63882, 32673, 17899, 98457, 62644, 2285, 12922, 51658, 3292, 92623, 67107, 94728, 62894, 71281, 47544, 31029, 40919, 9439, 64847, 90694], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no way this works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-23 16:13:35 llm_engine.py:98] Initializing an LLM engine (v0.4.1) with config: model='truncated_llm', speculative_config=None, tokenizer='truncated_llm', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-23 16:13:35 utils.py:608] Found nccl from library /home/ubuntu/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "INFO 09-23 16:13:36 selector.py:28] Using FlashAttention backend.\n",
      "INFO 09-23 16:13:48 model_runner.py:173] Loading model weights took 10.9877 GB\n",
      "INFO 09-23 16:13:49 gpu_executor.py:119] # GPU blocks: 3739, # CPU blocks: 655\n",
      "INFO 09-23 16:13:54 model_runner.py:976] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 09-23 16:13:54 model_runner.py:980] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 09-23 16:13:57 model_runner.py:1057] Graph capturing finished in 3 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'LLM' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6773/2072417979.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Step 4: Perform a forward pass with vllm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Run the inference with vllm to get the raw activations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Step 5: Extract the raw activations from the desired layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'LLM' object is not callable"
     ]
    }
   ],
   "source": [
    "from vllm import LLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Step 1: Load the model with vllm\n",
    "model_name = save_directory\n",
    "llm = LLM(model=model_name, max_num_batched_tokens=8192, trust_remote_code=True)\n",
    "\n",
    "# Step 2: Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# Step 3: Tokenize the input text\n",
    "input_text = \"This is the input string\"\n",
    "inputs = tokenizer(input_text, return_tensors='pt')\n",
    "\n",
    "# vllm expects strings as input for inference, so we pass the raw string directly to vllm\n",
    "\n",
    "# Step 4: Perform a forward pass with vllm\n",
    "# Run the inference with vllm to get the raw activations\n",
    "outputs = llm(inputs[\"input_ids\"].tolist(), return_hidden_states=True)\n",
    "\n",
    "# Step 5: Extract the raw activations from the desired layer\n",
    "# 'outputs' will contain the raw hidden states from all layers\n",
    "hidden_states = outputs.hidden_states  # Get all hidden states\n",
    "layer_25_activations = hidden_states[25]  # For example, the activations from the 25th layer\n",
    "\n",
    "# Print the shape of the raw activations from the 25th layer\n",
    "print(f\"Shape of raw activations from 25th layer: {layer_25_activations.shape}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
