{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In this file, we will let a human play a game of Lean (using modal).\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from typing import Optional\n",
    "import pexpect\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "from src.games.leaner_lean_game import LeanGame, LeanGameState\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set \"VLLM_LOGGING_LEVEL\" to \"WARNING\" to suppress logging\n",
    "os.environ[\"VLLM_LOGGING_LEVEL\"] = \"WARNING\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 09-27 18:52:27 config.py:1651] Casting torch.bfloat16 to torch.float16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-27 18:52:27 config.py:890] Defaulting to use mp for distributed inference\n",
      "INFO 09-27 18:52:27 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='deepseek-ai/DeepSeek-Prover-V1.5-RL', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-Prover-V1.5-RL', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=deepseek-ai/DeepSeek-Prover-V1.5-RL, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)\n",
      "WARNING 09-27 18:52:28 multiproc_gpu_executor.py:56] Reducing Torch parallelism from 16 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-27 18:52:28 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "INFO 09-27 18:52:28 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 09-27 18:52:28 selector.py:116] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=35864)\u001b[0;0m INFO 09-27 18:52:28 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=35864)\u001b[0;0m INFO 09-27 18:52:28 selector.py:116] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=35865)\u001b[0;0m INFO 09-27 18:52:28 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=35863)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=35865)\u001b[0;0m INFO 09-27 18:52:28 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 09-27 18:52:28 selector.py:116] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=35863)\u001b[0;0m INFO 09-27 18:52:28 selector.py:116] Using XFormers backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=35864)\u001b[0;0m /opt/conda/envs/pytorch/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=35864)\u001b[0;0m   @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "\u001b[1;36m(VllmWorkerProcess pid=35863)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=35865)\u001b[0;0m /opt/conda/envs/pytorch/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=35863)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=35865)\u001b[0;0m   @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "\u001b[1;36m(VllmWorkerProcess pid=35864)\u001b[0;0m /opt/conda/envs/pytorch/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=35864)\u001b[0;0m   @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n",
      "\u001b[1;36m(VllmWorkerProcess pid=35863)\u001b[0;0m /opt/conda/envs/pytorch/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=35863)\u001b[0;0m   @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n",
      "\u001b[1;36m(VllmWorkerProcess pid=35865)\u001b[0;0m /opt/conda/envs/pytorch/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=35865)\u001b[0;0m   @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n",
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=35864)\u001b[0;0m INFO 09-27 18:52:29 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=35865)\u001b[0;0m INFO 09-27 18:52:29 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=35863)\u001b[0;0m INFO 09-27 18:52:29 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 09-27 18:52:30 utils.py:977] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=35864)\u001b[0;0m INFO 09-27 18:52:30 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=35863)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=35865)\u001b[0;0m INFO 09-27 18:52:30 utils.py:977] Found nccl from library libnccl.so.2\n",
      "INFO 09-27 18:52:30 utils.py:977] Found nccl from library libnccl.so.2\n",
      "INFO 09-27 18:52:30 utils.py:977] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=35864)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=35863)\u001b[0;0m INFO 09-27 18:52:30 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 09-27 18:52:30 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=35865)\u001b[0;0m INFO 09-27 18:52:30 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 09-27 18:52:31 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=35863)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=35865)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=35864)\u001b[0;0m INFO 09-27 18:52:31 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "INFO 09-27 18:52:31 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "INFO 09-27 18:52:31 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "INFO 09-27 18:52:31 shm_broadcast.py:235] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7fd84e5554b0>, local_subscribe_port=54435, remote_subscribe_port=None)\n",
      "INFO 09-27 18:52:31 model_runner.py:915] Starting to load model deepseek-ai/DeepSeek-Prover-V1.5-RL...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=35863)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=35864)\u001b[0;0m INFO 09-27 18:52:31 model_runner.py:915] Starting to load model deepseek-ai/DeepSeek-Prover-V1.5-RL...\n",
      "INFO 09-27 18:52:31 model_runner.py:915] Starting to load model deepseek-ai/DeepSeek-Prover-V1.5-RL...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=35865)\u001b[0;0m INFO 09-27 18:52:31 model_runner.py:915] Starting to load model deepseek-ai/DeepSeek-Prover-V1.5-RL...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=35864)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=35865)\u001b[0;0m INFO 09-27 18:52:31 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 09-27 18:52:31 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=35864)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=35865)\u001b[0;0m INFO 09-27 18:52:31 selector.py:116] Using XFormers backend.\n",
      "INFO 09-27 18:52:31 selector.py:116] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=35863)\u001b[0;0m INFO 09-27 18:52:31 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 09-27 18:52:31 selector.py:116] Using XFormers backend.\n",
      "INFO 09-27 18:52:31 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=35863)\u001b[0;0m INFO 09-27 18:52:31 selector.py:116] Using XFormers backend.\n",
      "INFO 09-27 18:52:31 weight_utils.py:236] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=35865)\u001b[0;0m INFO 09-27 18:52:31 weight_utils.py:236] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=35863)\u001b[0;0m INFO 09-27 18:52:31 weight_utils.py:236] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=35864)\u001b[0;0m INFO 09-27 18:52:31 weight_utils.py:236] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91f5a81d10e44d89a4f0c4b0ae5b5449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-27 18:52:34 model_runner.py:926] Loading model weights took 3.2632 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=35865)\u001b[0;0m INFO 09-27 18:52:34 model_runner.py:926] Loading model weights took 3.2632 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=35864)\u001b[0;0m INFO 09-27 18:52:35 model_runner.py:926] Loading model weights took 3.2632 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=35863)\u001b[0;0m INFO 09-27 18:52:35 model_runner.py:926] Loading model weights took 3.2632 GB\n",
      "INFO 09-27 18:52:36 distributed_gpu_executor.py:57] # GPU blocks: 4848, # CPU blocks: 2184\n",
      "INFO 09-27 18:52:42 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=35865)\u001b[0;0m INFO 09-27 18:52:42 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=35863)\u001b[0;0m INFO 09-27 18:52:42 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=35864)\u001b[0;0m INFO 09-27 18:52:42 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=35865)\u001b[0;0m INFO 09-27 18:52:42 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=35863)\u001b[0;0m INFO 09-27 18:52:42 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=35864)\u001b[0;0m INFO 09-27 18:52:42 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 09-27 18:52:42 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 09-27 18:52:59 custom_all_reduce.py:223] Registering 2135 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=35863)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=35865)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=35864)\u001b[0;0m INFO 09-27 18:52:59 custom_all_reduce.py:223] Registering 2135 cuda graph addresses\n",
      "INFO 09-27 18:52:59 custom_all_reduce.py:223] Registering 2135 cuda graph addresses\n",
      "INFO 09-27 18:52:59 custom_all_reduce.py:223] Registering 2135 cuda graph addresses\n",
      "INFO 09-27 18:52:59 model_runner.py:1335] Graph capturing finished in 17 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=35865)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=35863)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=35864)\u001b[0;0m INFO 09-27 18:52:59 model_runner.py:1335] Graph capturing finished in 17 secs.\n",
      "INFO 09-27 18:52:59 model_runner.py:1335] Graph capturing finished in 17 secs.\n",
      "INFO 09-27 18:52:59 model_runner.py:1335] Graph capturing finished in 17 secs.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n  -- First, we want to re-write the condition about the second\\n  -- and fourth terms of the geometric sequence using the definition of a geometric sequence\\n  simp_all only [Nat.one_eq_succ_zero, Nat.zero_eq, zero_add, Nat.add_succ, Nat.add_zero,\\n    Nat.succ_add]\\n  have h₁' : a * r = 2 := by simpa [h₀] using h₁\\n  have h₂' : a * r ^ 3 = 6 := by simpa [h₀] using h₂\\n  -- Now we can divide the two equations to eliminate $a$ and determine $r$\\n  have h₃ : r ^ 2 = 3 := by\\n    nlinarith\\n  -- Finally, we can substitute back to find $a$\\n  have h₄ : a = 2 / Real.sqrt 3 ∨ a = -(2 / Real.sqrt 3) := by\\n    apply eq_or_eq_neg_of_sq_eq_sq <;>\\n    field_simp <;>\\n    nlinarith\\n  simpa [h₀] using h₄\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "informal_prefix = r'''/-- The second and fourth terms of a geometric sequence are $2$ and $6$. Which of the following is a possible first term?\n",
    "Show that it is $\\frac{2\\sqrt{3}}{3}$.-/\n",
    "'''\n",
    "formal_statement = r'''theorem amc12b_2003_p6 (a r : ℝ) (u : ℕ → ℝ) (h₀ : ∀ k, u k = a * r ^ k) (h₁ : u 1 = 2)\n",
    "    (h₂ : u 3 = 6) : u 0 = 2 / Real.sqrt 3 ∨ u 0 = -(2 / Real.sqrt 3) := by\n",
    "'''\n",
    "PROBLEM_STATEMENT = informal_prefix + formal_statement\n",
    "tactic_state = r'''/- tactic state:\n",
    "\n",
    "a r : ℝ\n",
    "u : ℕ → ℝ\n",
    "h₀ : ∀ (k : ℕ), u k = a * r ^ k\n",
    "h₁ : u 1 = 2\n",
    "h₂ : u 3 = 6\n",
    "⊢ u 0 = 2 / √3 ∨ u 0 = -(2 / √3)\n",
    "-/\n",
    "'''\n",
    "\n",
    "llm = LLM(model=\"deepseek-ai/DeepSeek-Prover-V1.5-RL\",\n",
    "          max_num_batched_tokens=8192,\n",
    "          trust_remote_code=True,\n",
    "          dtype=\"float16\",\n",
    "          tensor_parallel_size=4)\n",
    "\n",
    "# add a custom stopping token\n",
    "# which stops on newlines\n",
    "sampling_params = SamplingParams(\n",
    "    max_tokens=4096,\n",
    "    temperature=0.0,\n",
    "    top_k=1,\n",
    "    top_p=1.0,\n",
    "    stop=[\"\\n\"]\n",
    ")\n",
    "\n",
    "\n",
    "# useful copy+paste for the game.\n",
    "\"\"\"\n",
    "  -- First, we want to re-write the condition about the second\n",
    "  -- and fourth terms of the geometric sequence using the definition of a geometric sequence\n",
    "  simp_all only [Nat.one_eq_succ_zero, Nat.zero_eq, zero_add, Nat.add_succ, Nat.add_zero,\n",
    "    Nat.succ_add]\n",
    "  have h₁' : a * r = 2 := by simpa [h₀] using h₁\n",
    "  have h₂' : a * r ^ 3 = 6 := by simpa [h₀] using h₂\n",
    "  -- Now we can divide the two equations to eliminate $a$ and determine $r$\n",
    "  have h₃ : r ^ 2 = 3 := by\n",
    "    nlinarith\n",
    "  -- Finally, we can substitute back to find $a$\n",
    "  have h₄ : a = 2 / Real.sqrt 3 ∨ a = -(2 / Real.sqrt 3) := by\n",
    "    apply eq_or_eq_neg_of_sq_eq_sq <;>\n",
    "    field_simp <;>\n",
    "    nlinarith\n",
    "  simpa [h₀] using h₄\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"cmd\": \"import Mathlib\\nimport Aesop\\n\\nset_option maxHeartbeats 0\\n\\nopen BigOperators Real Nat Topology Rat\\n\\n\", \"allTactics\": true, \"tactics\": true}\n",
      "Sent code to Lean4 REPL.\n",
      "Received start of output from Lean4 REPL.\n"
     ]
    }
   ],
   "source": [
    "def send_code_read_json(cmd, timeout_start=30, timeout_finish=30, _child : Optional[pexpect.spawn] = None, kill = False):\n",
    "    if _child is None:\n",
    "        child = pexpect.spawn(\n",
    "            f\"{DEFAULT_LAKE_PATH} exe repl\",\n",
    "            cwd=DEFAULT_LEAN_WORKSPACE)\n",
    "    else:\n",
    "        child = _child\n",
    "\n",
    "    cmd_json = json.dumps(cmd)\n",
    "    # print(cmd_json)\n",
    "    child.send(cmd_json + \"\\r\\n\")\n",
    "    # Read the input itself.\n",
    "    # This should be printed instantly, so timeout is set to 1 second.\n",
    "    child.expect_exact(cmd_json + \"\\r\\n\", timeout=20)\n",
    "    assert child.after.decode('utf-8') == cmd_json + \"\\r\\n\"\n",
    "    # print(\"Sent code to Lean4 REPL.\")\n",
    "\n",
    "    # Read the output.\n",
    "    # This code is critical; the repl seems to print out some\n",
    "    # strange non-json stuff before the actual json output,\n",
    "    # including characters that delete the previous input,\n",
    "    # such that it doesn't show up in debug output.\n",
    "    child.expect_exact(\"{\", timeout=timeout_start)\n",
    "    res = \"{\"\n",
    "    print(\"Received start of output from Lean4 REPL.\")\n",
    "    # while res is not a valid json string, read lines.\n",
    "    # All of the lines should print essentially instantly,\n",
    "    # so there are no timeouts in this loop.\n",
    "    start_time = time.time()\n",
    "    while True:\n",
    "        res = res + child.readline().decode('utf-8')\n",
    "        try:\n",
    "            # print all chars in res\n",
    "            json.loads(res.strip())\n",
    "            break\n",
    "        except json.JSONDecodeError as e:\n",
    "            # print(e)\n",
    "            pass\n",
    "        if time.time() - start_time > timeout_finish:\n",
    "            raise TimeoutError(\"Lean4 REPL timed out.\")\n",
    "        # time.sleep(0.1)\n",
    "\n",
    "    # kill\n",
    "    if kill:\n",
    "        child.close()\n",
    "    return json.loads(res)\n",
    "\n",
    "def setup_repl():\n",
    "    child = pexpect.spawn(\n",
    "        f\"{DEFAULT_LAKE_PATH} exe repl\",\n",
    "        cwd=DEFAULT_LEAN_WORKSPACE)\n",
    "    send_code_read_json(\n",
    "        {\n",
    "            \"cmd\": LEAN4_DEFAULT_HEADER,\n",
    "            \"allTactics\": True,\n",
    "            \"tactics\": True,\n",
    "        },\n",
    "        _child=child\n",
    "    )\n",
    "    return child\n",
    "\n",
    "\n",
    "HOME_DIR = os.path.expanduser('~')\n",
    "DEFAULT_LAKE_PATH = f'{HOME_DIR}/.elan/bin/lake'\n",
    "DEFAULT_LEAN_WORKSPACE = '../mathlib4/'\n",
    "\n",
    "LEAN4_DEFAULT_HEADER = \"import Mathlib\\nimport Aesop\\n\\nset_option maxHeartbeats 0\\n\\nopen BigOperators Real Nat Topology Rat\\n\\n\"\n",
    "child = setup_repl()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "comments = None\n",
    "with open(\"../src/sample-data/comments.txt\", 'r') as file:\n",
    "    comments = [line.strip() for line in file.readlines()]\n",
    "\n",
    "game: LeanGame = LeanGame(\n",
    "    comment_seeds=comments,\n",
    ")\n",
    "state: LeanGameState = game.start_state(\n",
    "    problem=PROBLEM_STATEMENT,\n",
    "    tactic_state=tactic_state\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  2.67it/s, est. speed input: 815.65 toks/s, output: 98.94 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"cmd\": \"/-- The second and fourth terms of a geometric sequence are $2$ and $6$. Which of the following is a possible first term?\\nShow that it is $\\\\frac{2\\\\sqrt{3}}{3}$.-/\\ntheorem amc12b_2003_p6 (a r : \\u211d) (u : \\u2115 \\u2192 \\u211d) (h\\u2080 : \\u2200 k, u k = a * r ^ k) (h\\u2081 : u 1 = 2)\\n    (h\\u2082 : u 3 = 6) : u 0 = 2 / Real.sqrt 3 \\u2228 u 0 = -(2 / Real.sqrt 3) := by\\n  simp_all only [h\\u2080, Nat.cast_one, Nat.cast_zero, Nat.cast_succ, Nat.cast_zero]\\n\", \"allTactics\": true, \"tactics\": true, \"env\": 0}\n",
      "Sent code to Lean4 REPL.\n",
      "Received start of output from Lean4 REPL.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  5.18it/s, est. speed input: 1793.67 toks/s, output: 88.63 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"cmd\": \"/-- The second and fourth terms of a geometric sequence are $2$ and $6$. Which of the following is a possible first term?\\nShow that it is $\\\\frac{2\\\\sqrt{3}}{3}$.-/\\ntheorem amc12b_2003_p6 (a r : \\u211d) (u : \\u2115 \\u2192 \\u211d) (h\\u2080 : \\u2200 k, u k = a * r ^ k) (h\\u2081 : u 1 = 2)\\n    (h\\u2082 : u 3 = 6) : u 0 = 2 / Real.sqrt 3 \\u2228 u 0 = -(2 / Real.sqrt 3) := by\\n  simp_all only [h\\u2080, Nat.cast_one, Nat.cast_zero, Nat.cast_succ, Nat.cast_zero]\\n  have h\\u2083 : a * r = 2 := by linarith\\n\", \"allTactics\": true, \"tactics\": true, \"env\": 0}\n",
      "Sent code to Lean4 REPL.\n",
      "Received start of output from Lean4 REPL.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  4.54it/s, est. speed input: 1692.36 toks/s, output: 91.22 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"cmd\": \"/-- The second and fourth terms of a geometric sequence are $2$ and $6$. Which of the following is a possible first term?\\nShow that it is $\\\\frac{2\\\\sqrt{3}}{3}$.-/\\ntheorem amc12b_2003_p6 (a r : \\u211d) (u : \\u2115 \\u2192 \\u211d) (h\\u2080 : \\u2200 k, u k = a * r ^ k) (h\\u2081 : u 1 = 2)\\n    (h\\u2082 : u 3 = 6) : u 0 = 2 / Real.sqrt 3 \\u2228 u 0 = -(2 / Real.sqrt 3) := by\\n  simp_all only [h\\u2080, Nat.cast_one, Nat.cast_zero, Nat.cast_succ, Nat.cast_zero]\\n  have h\\u2083 : a * r = 2 := by linarith\\n  have h\\u2084 : a * r ^ 3 = 6 := by linarith\\n\", \"allTactics\": true, \"tactics\": true, \"env\": 0}\n",
      "Sent code to Lean4 REPL.\n",
      "Received start of output from Lean4 REPL.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  4.84it/s, est. speed input: 1968.50 toks/s, output: 77.95 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"cmd\": \"/-- The second and fourth terms of a geometric sequence are $2$ and $6$. Which of the following is a possible first term?\\nShow that it is $\\\\frac{2\\\\sqrt{3}}{3}$.-/\\ntheorem amc12b_2003_p6 (a r : \\u211d) (u : \\u2115 \\u2192 \\u211d) (h\\u2080 : \\u2200 k, u k = a * r ^ k) (h\\u2081 : u 1 = 2)\\n    (h\\u2082 : u 3 = 6) : u 0 = 2 / Real.sqrt 3 \\u2228 u 0 = -(2 / Real.sqrt 3) := by\\n  simp_all only [h\\u2080, Nat.cast_one, Nat.cast_zero, Nat.cast_succ, Nat.cast_zero]\\n  have h\\u2083 : a * r = 2 := by linarith\\n  have h\\u2084 : a * r ^ 3 = 6 := by linarith\\n  have h\\u2085 : r ^ 2 = 3 := by\\n\", \"allTactics\": true, \"tactics\": true, \"env\": 0}\n",
      "Sent code to Lean4 REPL.\n",
      "Received start of output from Lean4 REPL.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  9.57it/s, est. speed input: 4179.24 toks/s, output: 48.36 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"cmd\": \"/-- The second and fourth terms of a geometric sequence are $2$ and $6$. Which of the following is a possible first term?\\nShow that it is $\\\\frac{2\\\\sqrt{3}}{3}$.-/\\ntheorem amc12b_2003_p6 (a r : \\u211d) (u : \\u2115 \\u2192 \\u211d) (h\\u2080 : \\u2200 k, u k = a * r ^ k) (h\\u2081 : u 1 = 2)\\n    (h\\u2082 : u 3 = 6) : u 0 = 2 / Real.sqrt 3 \\u2228 u 0 = -(2 / Real.sqrt 3) := by\\n  simp_all only [h\\u2080, Nat.cast_one, Nat.cast_zero, Nat.cast_succ, Nat.cast_zero]\\n  have h\\u2083 : a * r = 2 := by linarith\\n  have h\\u2084 : a * r ^ 3 = 6 := by linarith\\n  have h\\u2085 : r ^ 2 = 3 := by\\n    nlinarith\\n\", \"allTactics\": true, \"tactics\": true, \"env\": 0}\n",
      "Sent code to Lean4 REPL.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received start of output from Lean4 REPL.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  2.95it/s, est. speed input: 1294.41 toks/s, output: 97.74 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"cmd\": \"/-- The second and fourth terms of a geometric sequence are $2$ and $6$. Which of the following is a possible first term?\\nShow that it is $\\\\frac{2\\\\sqrt{3}}{3}$.-/\\ntheorem amc12b_2003_p6 (a r : \\u211d) (u : \\u2115 \\u2192 \\u211d) (h\\u2080 : \\u2200 k, u k = a * r ^ k) (h\\u2081 : u 1 = 2)\\n    (h\\u2082 : u 3 = 6) : u 0 = 2 / Real.sqrt 3 \\u2228 u 0 = -(2 / Real.sqrt 3) := by\\n  simp_all only [h\\u2080, Nat.cast_one, Nat.cast_zero, Nat.cast_succ, Nat.cast_zero]\\n  have h\\u2083 : a * r = 2 := by linarith\\n  have h\\u2084 : a * r ^ 3 = 6 := by linarith\\n  have h\\u2085 : r ^ 2 = 3 := by\\n    nlinarith\\n  have h\\u2086 : a = 2 / Real.sqrt 3 \\u2228 a = -(2 / Real.sqrt 3) := by\\n\", \"allTactics\": true, \"tactics\": true, \"env\": 0}\n",
      "Sent code to Lean4 REPL.\n",
      "Received start of output from Lean4 REPL.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  3.30it/s, est. speed input: 1639.70 toks/s, output: 92.74 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"cmd\": \"/-- The second and fourth terms of a geometric sequence are $2$ and $6$. Which of the following is a possible first term?\\nShow that it is $\\\\frac{2\\\\sqrt{3}}{3}$.-/\\ntheorem amc12b_2003_p6 (a r : \\u211d) (u : \\u2115 \\u2192 \\u211d) (h\\u2080 : \\u2200 k, u k = a * r ^ k) (h\\u2081 : u 1 = 2)\\n    (h\\u2082 : u 3 = 6) : u 0 = 2 / Real.sqrt 3 \\u2228 u 0 = -(2 / Real.sqrt 3) := by\\n  simp_all only [h\\u2080, Nat.cast_one, Nat.cast_zero, Nat.cast_succ, Nat.cast_zero]\\n  have h\\u2083 : a * r = 2 := by linarith\\n  have h\\u2084 : a * r ^ 3 = 6 := by linarith\\n  have h\\u2085 : r ^ 2 = 3 := by\\n    nlinarith\\n  have h\\u2086 : a = 2 / Real.sqrt 3 \\u2228 a = -(2 / Real.sqrt 3) := by\\n    apply eq_or_eq_neg_of_sq_eq_sq <;> field_simp <;>\\n\", \"allTactics\": true, \"tactics\": true, \"env\": 0}\n",
      "Sent code to Lean4 REPL.\n",
      "Received start of output from Lean4 REPL.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 11.12it/s, est. speed input: 5827.22 toks/s, output: 55.69 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"cmd\": \"/-- The second and fourth terms of a geometric sequence are $2$ and $6$. Which of the following is a possible first term?\\nShow that it is $\\\\frac{2\\\\sqrt{3}}{3}$.-/\\ntheorem amc12b_2003_p6 (a r : \\u211d) (u : \\u2115 \\u2192 \\u211d) (h\\u2080 : \\u2200 k, u k = a * r ^ k) (h\\u2081 : u 1 = 2)\\n    (h\\u2082 : u 3 = 6) : u 0 = 2 / Real.sqrt 3 \\u2228 u 0 = -(2 / Real.sqrt 3) := by\\n  simp_all only [h\\u2080, Nat.cast_one, Nat.cast_zero, Nat.cast_succ, Nat.cast_zero]\\n  have h\\u2083 : a * r = 2 := by linarith\\n  have h\\u2084 : a * r ^ 3 = 6 := by linarith\\n  have h\\u2085 : r ^ 2 = 3 := by\\n    nlinarith\\n  have h\\u2086 : a = 2 / Real.sqrt 3 \\u2228 a = -(2 / Real.sqrt 3) := by\\n    apply eq_or_eq_neg_of_sq_eq_sq <;> field_simp <;>\\n    nlinarith\\n\", \"allTactics\": true, \"tactics\": true, \"env\": 0}\n",
      "Sent code to Lean4 REPL.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received start of output from Lean4 REPL.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  5.25it/s, est. speed input: 2785.48 toks/s, output: 84.40 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"cmd\": \"/-- The second and fourth terms of a geometric sequence are $2$ and $6$. Which of the following is a possible first term?\\nShow that it is $\\\\frac{2\\\\sqrt{3}}{3}$.-/\\ntheorem amc12b_2003_p6 (a r : \\u211d) (u : \\u2115 \\u2192 \\u211d) (h\\u2080 : \\u2200 k, u k = a * r ^ k) (h\\u2081 : u 1 = 2)\\n    (h\\u2082 : u 3 = 6) : u 0 = 2 / Real.sqrt 3 \\u2228 u 0 = -(2 / Real.sqrt 3) := by\\n  simp_all only [h\\u2080, Nat.cast_one, Nat.cast_zero, Nat.cast_succ, Nat.cast_zero]\\n  have h\\u2083 : a * r = 2 := by linarith\\n  have h\\u2084 : a * r ^ 3 = 6 := by linarith\\n  have h\\u2085 : r ^ 2 = 3 := by\\n    nlinarith\\n  have h\\u2086 : a = 2 / Real.sqrt 3 \\u2228 a = -(2 / Real.sqrt 3) := by\\n    apply eq_or_eq_neg_of_sq_eq_sq <;> field_simp <;>\\n    nlinarith\\n  simp_all only [pow_zero, mul_one]\\n\", \"allTactics\": true, \"tactics\": true, \"env\": 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent code to Lean4 REPL.\n",
      "Received start of output from Lean4 REPL.\n",
      "Terminated!\n",
      "-------------------------------------Status-------------------------------------\n",
      "Fully processed\n",
      "-------------------------------------Header-------------------------------------\n",
      "import Mathlib\n",
      "import Aesop\n",
      "\n",
      "set_option maxHeartbeats 0\n",
      "\n",
      "open BigOperators Real Nat Topology Rat\n",
      "\n",
      "------------------------------------Problem-------------------------------------\n",
      "/-- The second and fourth terms of a geometric sequence are $2$ and $6$. Which of the following is a possible first term?\n",
      "Show that it is $\\frac{2\\sqrt{3}}{3}$.-/\n",
      "theorem amc12b_2003_p6 (a r : ℝ) (u : ℕ → ℝ) (h₀ : ∀ k, u k = a * r ^ k) (h₁ : u 1 = 2)\n",
      "    (h₂ : u 3 = 6) : u 0 = 2 / Real.sqrt 3 ∨ u 0 = -(2 / Real.sqrt 3) := by\n",
      "------------------------------------Old Code------------------------------------\n",
      "  simp_all only [h₀, Nat.cast_one, Nat.cast_zero, Nat.cast_succ, Nat.cast_zero]\n",
      "  have h₃ : a * r = 2 := by linarith\n",
      "  have h₄ : a * r ^ 3 = 6 := by linarith\n",
      "  have h₅ : r ^ 2 = 3 := by\n",
      "    nlinarith\n",
      "  have h₆ : a = 2 / Real.sqrt 3 ∨ a = -(2 / Real.sqrt 3) := by\n",
      "    apply eq_or_eq_neg_of_sq_eq_sq <;> field_simp <;>\n",
      "    nlinarith\n",
      "------------------------------------Comment-------------------------------------\n",
      "[Empty Field]\n",
      "--------------------------Valid Truncation of New Code--------------------------\n",
      "  simp_all only [pow_zero, mul_one]\n",
      "-------------------------------------Tailer-------------------------------------\n",
      "[Empty Field]\n",
      "--------------------------------Old Tactic State--------------------------------\n",
      "a r : ℝ\n",
      "u : ℕ → ℝ\n",
      "h₀ : ∀ (k : ℕ), u k = a * r ^ k\n",
      "h₁ : a * r ^ 1 = 2\n",
      "h₂ : a * r ^ 3 = 6\n",
      "h₃ : a * r = 2\n",
      "h₄ : a * r ^ 3 = 6\n",
      "h₅ : r ^ 2 = 3\n",
      "h₆ : a = 2 / √3 ∨ a = -(2 / √3)\n",
      "⊢ a * r ^ 0 = 2 / √3 ∨ a * r ^ 0 = -(2 / √3)\n",
      "[Missing newline]\n",
      "--------------------------------New Tactic State--------------------------------\n",
      "[Empty Field]\n",
      "--------------------------------------Meta--------------------------------------\n",
      "Processed: True, Rollout Done: True\n",
      "Win: True, Dead: False\n",
      "Depth: 9 Number of Children: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "children = []\n",
    "while not game.is_terminal(state):\n",
    "    children.append(state)\n",
    "    # print(\"Player Must Act\".center(80, \"#\"))\n",
    "    # print(state.human_printout())\n",
    "    # print each possible comment with the index in front.\n",
    "    # for i, comment in enumerate(comments):\n",
    "    #     print(f\"{i}: {comment}\")\n",
    "    # action = int(input(\"Enter your action: \"))\n",
    "    action = 0\n",
    "    # action = comments[action]\n",
    "    # action = input(\"Enter your action: \")\n",
    "    state = game.next_state(state, action)\n",
    "\n",
    "    # print(\"LLM Must Act\".center(80, \"#\"))\n",
    "    # print(state.human_printout())\n",
    "    input_data = state.pre_LLM_rollout()\n",
    "    # print(input_data)\n",
    "    outputs = llm.generate(\n",
    "        input_data,\n",
    "        sampling_params=sampling_params\n",
    "    )\n",
    "    outputs = outputs[0].outputs[0].text + \"\\n\"\n",
    "\n",
    "    # print(outputs)\n",
    "\n",
    "    state.post_LLM_rollout(outputs)\n",
    "\n",
    "    # print(\"Lean Verifier Must Act!\".center(80, \"#\"))\n",
    "    # print(state.human_printout())\n",
    "    # print(\"Lean4 Input\".center(80, \"-\"))\n",
    "    lean4_input = state.pre_process()\n",
    "\n",
    "    lean4_output = send_code_read_json({\n",
    "        \"cmd\": lean4_input,\n",
    "        \"allTactics\": True,\n",
    "        \"tactics\": True,\n",
    "        \"env\" : 0\n",
    "    }, _child=child)\n",
    "\n",
    "    # print(\"Lean4 Output\".center(80, \"-\"))\n",
    "    # print(str(lean4_output.get('messages'))[:100])\n",
    "    # print(f\"({len(str(lean4_output.get('messages')))} characters)\")\n",
    "    state.post_process(lean4_output)\n",
    "\n",
    "\n",
    "print(\"Terminated!\")\n",
    "print(state.human_printout())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
